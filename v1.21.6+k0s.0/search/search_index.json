{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview # k0s is an all-inclusive Kubernetes distribution, configured with all of the features needed to build a Kubernetes cluster simply by copying and running an executable file on each target host. Key Features # Available as a single static binary Offers a self-hosted, isolated control plane Supports a variety of storage backends, including etcd, SQLite, MySQL (or any compatible), and PostgreSQL. Offers an Elastic control plane Vanilla upstream Kubernetes Supports custom container runtimes (containerd is the default) Supports custom Container Network Interface (CNI) plugins (calico is the default) Supports x86_64 and arm64 Join the Community # We welcome your help in building k0s! If you are interested, we invite you to check out the k0s Contributing Guide and our Code of Conduct . Demo # Downloading k0s # Download k0s for linux amd64 and arm64 architectures. Getting Started # Quick Start Guide for creating a full Kubernetes cluster with a single node.","title":"Overview"},{"location":"#overview","text":"k0s is an all-inclusive Kubernetes distribution, configured with all of the features needed to build a Kubernetes cluster simply by copying and running an executable file on each target host.","title":"Overview"},{"location":"#key-features","text":"Available as a single static binary Offers a self-hosted, isolated control plane Supports a variety of storage backends, including etcd, SQLite, MySQL (or any compatible), and PostgreSQL. Offers an Elastic control plane Vanilla upstream Kubernetes Supports custom container runtimes (containerd is the default) Supports custom Container Network Interface (CNI) plugins (calico is the default) Supports x86_64 and arm64","title":"Key Features"},{"location":"#join-the-community","text":"We welcome your help in building k0s! If you are interested, we invite you to check out the k0s Contributing Guide and our Code of Conduct .","title":"Join the Community"},{"location":"#demo","text":"","title":"Demo"},{"location":"#downloading-k0s","text":"Download k0s for linux amd64 and arm64 architectures.","title":"Downloading k0s"},{"location":"#getting-started","text":"Quick Start Guide for creating a full Kubernetes cluster with a single node.","title":"Getting Started"},{"location":"CODE_OF_CONDUCT/","text":"k0s Community Code Of Conduct # Please refer to our contributor code of conduct .","title":"k0s Community Code Of Conduct"},{"location":"CODE_OF_CONDUCT/#k0s-community-code-of-conduct","text":"Please refer to our contributor code of conduct .","title":"k0s Community Code Of Conduct"},{"location":"FAQ/","text":"Frequently asked questions # How is k0s pronounced? # kay-zero-ess How do I run a single node cluster? # The cluster can be started with: k0s controller --single See also the Getting Started tutorial. How do I connect to the cluster? # You find the config in ${DATADIR}/pki/admin.conf (default: /var/lib/k0s/pki/admin.conf ). Copy this file, and change the localhost entry to the public ip of the controller. Use the modified config to connect with kubectl: export KUBECONFIG = /path/to/admin.conf kubectl ... Why doesn't kubectl get nodes list the k0s controllers? # As a default, the control plane does not run kubelet at all, and will not accept any workloads, so the controller will not show up on the node list in kubectl. If you want your controller to accept workloads and run pods, you do so with: k0s controller --enable-worker (recommended only as test/dev/POC environments). Is k0sproject really open source? # Yes, k0sproject is 100% open source. The source code is under Apache 2 and the documentation is under the Creative Commons License. Mirantis, Inc. is the main contributor and sponsor for this OSS project: building all the binaries from upstream, performing necessary security scans and calculating checksums so that it's easy and safe to use. The use of these ready-made binaries are subject to Mirantis EULA and the binaries include only open source software.","title":"FAQ"},{"location":"FAQ/#frequently-asked-questions","text":"","title":"Frequently asked questions"},{"location":"FAQ/#how-is-k0s-pronounced","text":"kay-zero-ess","title":"How is k0s pronounced?"},{"location":"FAQ/#how-do-i-run-a-single-node-cluster","text":"The cluster can be started with: k0s controller --single See also the Getting Started tutorial.","title":"How do I run a single node cluster?"},{"location":"FAQ/#how-do-i-connect-to-the-cluster","text":"You find the config in ${DATADIR}/pki/admin.conf (default: /var/lib/k0s/pki/admin.conf ). Copy this file, and change the localhost entry to the public ip of the controller. Use the modified config to connect with kubectl: export KUBECONFIG = /path/to/admin.conf kubectl ...","title":"How do I connect to the cluster?"},{"location":"FAQ/#why-doesnt-kubectl-get-nodes-list-the-k0s-controllers","text":"As a default, the control plane does not run kubelet at all, and will not accept any workloads, so the controller will not show up on the node list in kubectl. If you want your controller to accept workloads and run pods, you do so with: k0s controller --enable-worker (recommended only as test/dev/POC environments).","title":"Why doesn't kubectl get nodes list the k0s controllers?"},{"location":"FAQ/#is-k0sproject-really-open-source","text":"Yes, k0sproject is 100% open source. The source code is under Apache 2 and the documentation is under the Creative Commons License. Mirantis, Inc. is the main contributor and sponsor for this OSS project: building all the binaries from upstream, performing necessary security scans and calculating checksums so that it's easy and safe to use. The use of these ready-made binaries are subject to Mirantis EULA and the binaries include only open source software.","title":"Is k0sproject really open source?"},{"location":"airgap-install/","text":"Airgap install # You can install k0s in an environment with restricted Internet access. Airgap installation requires an image bundle, which contains all the needed container images. There are two options to get the image bundle: Use a ready-made image bundle, which is created for each k0s release. It can be downloaded from the releases page . Create your own image bundle. In this case, you can easily customize the bundle to also include container images, which are not used by default in k0s. Prerequisites # In order to create your own image bundle, you need A working cluster with at least one controller, to be used to build the image bundle. For more information, refer to the Quick Start Guide . The containerd CLI management tool ctr , installed on the worker machine (refer to the ContainerD getting-started guide). 1. Create your own image bundle (optional) # k0s/containerd uses OCI (Open Container Initiative) bundles for airgap installation. OCI bundles must be uncompressed. As OCI bundles are built specifically for each architecture, create an OCI bundle that uses the same processor architecture (x86-64, ARM64, ARMv7) as on the target system. k0s offers two methods for creating OCI bundles, one using Docker and the other using a previously set up k0s worker. Be aware, though, that you cannot use the Docker method for the ARM architectures due to kube-proxy image multiarch manifest problem . Docker # Pull the images. k0s airgap list-images | xargs -I {} docker pull {} Create a bundle. docker image save $( k0s airgap list-images | xargs ) -o bundle_file Previously set up k0s worker # As containerd pulls all the images during the k0s worker normal bootstrap, you can use it to build the OCI bundle with images. Use the following commands on a machine with an installed k0s worker: ctr --namespace k8s.io \\ --address /run/k0s/containerd.sock \\ images export bundle_file $( k0s airgap list-images | xargs ) 2a. Sync the bundle file with the airgapped machine (locally) # Copy the bundle_file you created in the previous step or downloaded from the releases page to the target machine into the images directory in the k0s data directory. Copy the bundle only to the worker nodes. Controller nodes don't use it. # mkdir -p /var/lib/k0s/images # cp bundle_file /var/lib/k0s/images/bundle_file 2b. Sync the bundle file with the airgapped machines (remotely with k0sctl) # As an alternative to the previous step, you can use k0sctl to upload the bundle file to the worker nodes. k0sctl can also be used to upload k0s binary file to all nodes. Take a look at this example (k0sctl.yaml) with one controller and one worker node to upload the bundle file and k0s binary: apiVersion : k0sctl.k0sproject.io/v1beta1 kind : Cluster metadata : name : k0s-cluster spec : hosts : - ssh : address : <ip-address-controller> user : ubuntu keyPath : /path/.ssh/id_rsa role : controller uploadBinary : true k0sBinaryPath : /path/to/k0s_binary/k0s - ssh : address : <ip-address-worker> user : ubuntu keyPath : /path/.ssh/id_rsa role : worker uploadBinary : true k0sBinaryPath : /path/to/k0s_binary/k0s files : - name : bundle-file src : /path/to/bundle-file/airgap-bundle-amd64.tar dstDir : /var/lib/k0s/images/ perm : 0755 k0s : version : 1.21.6+k0s.0 3. Ensure pull policy in the k0s.yaml (optional) # Use the following k0s.yaml to ensure that containerd does not pull images for k0s components from the Internet at any time. apiVersion : k0s.k0sproject.io/v1beta1 kind : Cluster metadata : name : k0s spec : images : default_pull_policy : Never 4. Set up the controller and worker nodes # Refer to the Manual Install for information on setting up the controller and worker nodes locally. Alternatively, you can use k0sctl . Note : During the worker start up k0s imports all bundles from the $K0S_DATA_DIR/images before starting kubelet .","title":"Airgap Install"},{"location":"airgap-install/#airgap-install","text":"You can install k0s in an environment with restricted Internet access. Airgap installation requires an image bundle, which contains all the needed container images. There are two options to get the image bundle: Use a ready-made image bundle, which is created for each k0s release. It can be downloaded from the releases page . Create your own image bundle. In this case, you can easily customize the bundle to also include container images, which are not used by default in k0s.","title":"Airgap install"},{"location":"airgap-install/#prerequisites","text":"In order to create your own image bundle, you need A working cluster with at least one controller, to be used to build the image bundle. For more information, refer to the Quick Start Guide . The containerd CLI management tool ctr , installed on the worker machine (refer to the ContainerD getting-started guide).","title":"Prerequisites"},{"location":"airgap-install/#1-create-your-own-image-bundle-optional","text":"k0s/containerd uses OCI (Open Container Initiative) bundles for airgap installation. OCI bundles must be uncompressed. As OCI bundles are built specifically for each architecture, create an OCI bundle that uses the same processor architecture (x86-64, ARM64, ARMv7) as on the target system. k0s offers two methods for creating OCI bundles, one using Docker and the other using a previously set up k0s worker. Be aware, though, that you cannot use the Docker method for the ARM architectures due to kube-proxy image multiarch manifest problem .","title":"1. Create your own image bundle (optional)"},{"location":"airgap-install/#docker","text":"Pull the images. k0s airgap list-images | xargs -I {} docker pull {} Create a bundle. docker image save $( k0s airgap list-images | xargs ) -o bundle_file","title":"Docker"},{"location":"airgap-install/#previously-set-up-k0s-worker","text":"As containerd pulls all the images during the k0s worker normal bootstrap, you can use it to build the OCI bundle with images. Use the following commands on a machine with an installed k0s worker: ctr --namespace k8s.io \\ --address /run/k0s/containerd.sock \\ images export bundle_file $( k0s airgap list-images | xargs )","title":"Previously set up k0s worker"},{"location":"airgap-install/#2a-sync-the-bundle-file-with-the-airgapped-machine-locally","text":"Copy the bundle_file you created in the previous step or downloaded from the releases page to the target machine into the images directory in the k0s data directory. Copy the bundle only to the worker nodes. Controller nodes don't use it. # mkdir -p /var/lib/k0s/images # cp bundle_file /var/lib/k0s/images/bundle_file","title":"2a. Sync the bundle file with the airgapped machine (locally)"},{"location":"airgap-install/#2b-sync-the-bundle-file-with-the-airgapped-machines-remotely-with-k0sctl","text":"As an alternative to the previous step, you can use k0sctl to upload the bundle file to the worker nodes. k0sctl can also be used to upload k0s binary file to all nodes. Take a look at this example (k0sctl.yaml) with one controller and one worker node to upload the bundle file and k0s binary: apiVersion : k0sctl.k0sproject.io/v1beta1 kind : Cluster metadata : name : k0s-cluster spec : hosts : - ssh : address : <ip-address-controller> user : ubuntu keyPath : /path/.ssh/id_rsa role : controller uploadBinary : true k0sBinaryPath : /path/to/k0s_binary/k0s - ssh : address : <ip-address-worker> user : ubuntu keyPath : /path/.ssh/id_rsa role : worker uploadBinary : true k0sBinaryPath : /path/to/k0s_binary/k0s files : - name : bundle-file src : /path/to/bundle-file/airgap-bundle-amd64.tar dstDir : /var/lib/k0s/images/ perm : 0755 k0s : version : 1.21.6+k0s.0","title":"2b. Sync the bundle file with the airgapped machines (remotely with k0sctl)"},{"location":"airgap-install/#3-ensure-pull-policy-in-the-k0syaml-optional","text":"Use the following k0s.yaml to ensure that containerd does not pull images for k0s components from the Internet at any time. apiVersion : k0s.k0sproject.io/v1beta1 kind : Cluster metadata : name : k0s spec : images : default_pull_policy : Never","title":"3. Ensure pull policy in the k0s.yaml (optional)"},{"location":"airgap-install/#4-set-up-the-controller-and-worker-nodes","text":"Refer to the Manual Install for information on setting up the controller and worker nodes locally. Alternatively, you can use k0sctl . Note : During the worker start up k0s imports all bundles from the $K0S_DATA_DIR/images before starting kubelet .","title":"4. Set up the controller and worker nodes"},{"location":"architecture/","text":"Architecture # Note: As k0s is a new and dynamic project, the product architecture may occasionally outpace the documentation. The high level concepts and patterns, however, should always apply. Packaging # The k0s package is a single, self-extracting binary that embeds Kubernetes binaries, the benefits of which include: Statically compiled No OS-level dependencies Requires no RPMs, dependencies, snaps, or any other OS-specific packaging Provides a single package for all operating systems Allows full version control for each dependency Control plane # As a single binary, k0s acts as the process supervisor for all other control plane components. As such, there is no container engine or kubelet running on controllers by default, which thus means that a cluster user cannot schedule workloads onto controller nodes. Using k0s you can create, manage, and configure each of the components, running each as a \"naked\" process. Thus, there is no container engine running on the controller node. Storage # Kubernetes control plane typically supports only etcd as the datastore. k0s, however, supports many other datastore options in addition to etcd, which it achieves by including kine . Kine allows the use of a wide variety of backend data stores, such as MySQL, PostgreSQL, SQLite, and dqlite (refer to the spec.storage documentation ). In the case of k0s managed etcd, k0s manages the full lifecycle of the etcd cluster. For example, by joining a new controller node with k0s controller \"long-join-token\" k0s atomatically adjusts the etcd cluster membership info to allow the new member to join the cluster. Note : k0s cannot shrink the etcd cluster. As such, to shut down the k0s controller on a node that node must first be manually removed from the etcd cluster. Worker node # As with the control plane, with k0s you can create and manage the core worker components as naked processes on the worker node. By default, k0s workers use containerd as a high-level runtime and runc as a low-level runtime. Custom runtimes are also supported, refer to Using custom CRI runtime .","title":"Architecture"},{"location":"architecture/#architecture","text":"Note: As k0s is a new and dynamic project, the product architecture may occasionally outpace the documentation. The high level concepts and patterns, however, should always apply.","title":"Architecture"},{"location":"architecture/#packaging","text":"The k0s package is a single, self-extracting binary that embeds Kubernetes binaries, the benefits of which include: Statically compiled No OS-level dependencies Requires no RPMs, dependencies, snaps, or any other OS-specific packaging Provides a single package for all operating systems Allows full version control for each dependency","title":"Packaging"},{"location":"architecture/#control-plane","text":"As a single binary, k0s acts as the process supervisor for all other control plane components. As such, there is no container engine or kubelet running on controllers by default, which thus means that a cluster user cannot schedule workloads onto controller nodes. Using k0s you can create, manage, and configure each of the components, running each as a \"naked\" process. Thus, there is no container engine running on the controller node.","title":"Control plane"},{"location":"architecture/#storage","text":"Kubernetes control plane typically supports only etcd as the datastore. k0s, however, supports many other datastore options in addition to etcd, which it achieves by including kine . Kine allows the use of a wide variety of backend data stores, such as MySQL, PostgreSQL, SQLite, and dqlite (refer to the spec.storage documentation ). In the case of k0s managed etcd, k0s manages the full lifecycle of the etcd cluster. For example, by joining a new controller node with k0s controller \"long-join-token\" k0s atomatically adjusts the etcd cluster membership info to allow the new member to join the cluster. Note : k0s cannot shrink the etcd cluster. As such, to shut down the k0s controller on a node that node must first be manually removed from the etcd cluster.","title":"Storage"},{"location":"architecture/#worker-node","text":"As with the control plane, with k0s you can create and manage the core worker components as naked processes on the worker node. By default, k0s workers use containerd as a high-level runtime and runc as a low-level runtime. Custom runtimes are also supported, refer to Using custom CRI runtime .","title":"Worker node"},{"location":"backup/","text":"Backup/Restore overview # k0s has integrated support for backing up cluster state and configuration. The k0s backup utility is aiming to back up and restore k0s managed parts of the cluster. The backups created by k0s backup command have following pieces of your cluster: certificates (the content of the <data-dir>/pki directory) etcd snapshot, if the etcd datastore is used Kine/SQLite snapshot, if the Kine/SQLite datastore is used k0s.yaml any custom defined manifests under the <data-dir>/manifests any image bundles located under the <data-dir>/images any helm configuration Parts NOT covered by the backup utility: PersistentVolumes of any running application datastore, in case something else than etcd or Kine/SQLite is used any configuration to the cluster introduced by manual changes (e.g. changes that weren't saved under the <data-dir>/manifests ) Any of the backup/restore related operations MUST be performed on the controller node. Backup/restore a k0s node locally # Backup (local) # To create backup run the following command on the controller node: k0s backup --save-path = <directory> The directory used for the save-path value must exist and be writable. The default value is the current working directory. The command provides backup archive using following naming convention: k0s_backup_<ISODatetimeString>.tar.gz Because of the DateTime usage, it is guaranteed that none of the previously created archives would be overwritten. Restore (local) # To restore cluster state from the archive use the following command on the controller node: k0s restore /tmp/k0s_backup_2021-04-26T19_51_57_000Z.tar.gz The command would fail if the data directory for the current controller has overlapping data with the backup archive content. The command would use the archived k0s.yaml as the cluster configuration description. In case if your cluster is HA, after restoring single controller node, join the rest of the controller nodes to the cluster. E.g. steps for N nodes cluster would be: Restore backup on fresh machine Run controller there Join N-1 new machines to the cluster the same way as for the first setup. Backup/restore a k0s cluster using k0sctl # With k0sctl you can perform cluster level backup and restore remotely with one command. Backup (remote) # To create backup run the following command: k0sctl backup k0sctl connects to the cluster nodes to create a backup. The backup file is stored in the current working directory. Restore (remote) # To restore cluster state from the archive use the following command: k0sctl apply --restore-from /path/to/backup_file.tar.gz The control plane load balancer address (externalAddress) needs to remain the same between backup and restore. This is caused by the fact that all worker node components connect to this address and cannot currently be re-configured.","title":"Backup/Restore"},{"location":"backup/#backuprestore-overview","text":"k0s has integrated support for backing up cluster state and configuration. The k0s backup utility is aiming to back up and restore k0s managed parts of the cluster. The backups created by k0s backup command have following pieces of your cluster: certificates (the content of the <data-dir>/pki directory) etcd snapshot, if the etcd datastore is used Kine/SQLite snapshot, if the Kine/SQLite datastore is used k0s.yaml any custom defined manifests under the <data-dir>/manifests any image bundles located under the <data-dir>/images any helm configuration Parts NOT covered by the backup utility: PersistentVolumes of any running application datastore, in case something else than etcd or Kine/SQLite is used any configuration to the cluster introduced by manual changes (e.g. changes that weren't saved under the <data-dir>/manifests ) Any of the backup/restore related operations MUST be performed on the controller node.","title":"Backup/Restore overview"},{"location":"backup/#backuprestore-a-k0s-node-locally","text":"","title":"Backup/restore a k0s node locally"},{"location":"backup/#backup-local","text":"To create backup run the following command on the controller node: k0s backup --save-path = <directory> The directory used for the save-path value must exist and be writable. The default value is the current working directory. The command provides backup archive using following naming convention: k0s_backup_<ISODatetimeString>.tar.gz Because of the DateTime usage, it is guaranteed that none of the previously created archives would be overwritten.","title":"Backup (local)"},{"location":"backup/#restore-local","text":"To restore cluster state from the archive use the following command on the controller node: k0s restore /tmp/k0s_backup_2021-04-26T19_51_57_000Z.tar.gz The command would fail if the data directory for the current controller has overlapping data with the backup archive content. The command would use the archived k0s.yaml as the cluster configuration description. In case if your cluster is HA, after restoring single controller node, join the rest of the controller nodes to the cluster. E.g. steps for N nodes cluster would be: Restore backup on fresh machine Run controller there Join N-1 new machines to the cluster the same way as for the first setup.","title":"Restore (local)"},{"location":"backup/#backuprestore-a-k0s-cluster-using-k0sctl","text":"With k0sctl you can perform cluster level backup and restore remotely with one command.","title":"Backup/restore a k0s cluster using k0sctl"},{"location":"backup/#backup-remote","text":"To create backup run the following command: k0sctl backup k0sctl connects to the cluster nodes to create a backup. The backup file is stored in the current working directory.","title":"Backup (remote)"},{"location":"backup/#restore-remote","text":"To restore cluster state from the archive use the following command: k0sctl apply --restore-from /path/to/backup_file.tar.gz The control plane load balancer address (externalAddress) needs to remain the same between backup and restore. This is caused by the fact that all worker node components connect to this address and cannot currently be re-configured.","title":"Restore (remote)"},{"location":"cis_benchmark/","text":"CIS Benchmark # The Center for Internet Security provides set of benchmarks that help us harden our clusters. k0s clusters by default will pass CIS benchmarks with couple of exceptions . For CIS compliance verification we are using kube-bench . Run # kube-bench is CIS compliance verification tool. Follow install instructions . After installing the kube-bench on the host that is running k0s cluster run follwing command: kube-bench run --config-dir docs/kube-bench/cfg/ --benchmark k0s-1.0 Summary of disabled checks # Master Node Security Configuration # Current configuration has in total of 8 master checks disabled: id: 1.2.10 - EventRateLimit requires external yaml config. It is left for the users to configure it type : skip text : \"Ensure that the admission control plugin EventRateLimit is set (Manual)\" id: 1.2.12 - By default this isn't passed to the apiserver for air-gap functionality type : skip text : \"Ensure that the admission control plugin AlwaysPullImages is set (Manual)\" id: 1.2.22 - For sake of simplicity of k0s all audit configuration are skipped. It is left for the users to configure it type : skip text : \"Ensure that the --audit-log-path argument is set (Automated)\" id: 1.2.23 - For sake of simplicity of k0s all audit configuration are skipped. It is left for the users to configure it type : skip text : \"Ensure that the --audit-log-maxage argument is set to 30 or as appropriate (Automated)\" id: 1.2.24 - For sake of simplicity of k0s all audit configuration are skipped. It is left for the users to configure it type : skip text : \"Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate (Automated)\" id: 1.2.25 - For sake of simplicity of k0s all audit configuration are skipped. It is left for the users to configure it type : skip text : \"Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate (Automated)\" id: 1.2.33 - By default it is not enabled. Left for the users to decide type : skip text : \"Ensure that the --encryption-provider-config argument is set as appropriate (Manual)\" id: 1.2.34 - By default it is not enabled. Left for the users to decide type : skip text : \"Ensure that encryption providers are appropriately configured (Manual)\" Worker Node Security Configuration # and 4 node checks disabled: id: 4.1.1 - not applicable since k0s does not use kubelet service file type : skip text : \"Ensure that the kubelet service file permissions are set to 644 or more restrictive (Automated)\" id: 4.1.2 - not applicable since k0s does not use kubelet service file type : skip text : \"Ensure that the kubelet service file ownership is set to root:root (Automated)\" id: 4.2.6 - k0s does not set this. See https://github.com/kubernetes/kubernetes/issues/66693 type : skip text : \"Ensure that the --protect-kernel-defaults argument is set to true (Automated)\" id: 4.2.10 - k0s doesn't set this because certs get auto rotated type : skip text : \"Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Manual)\" Control Plane Configuration # 3 checks for control plane: id: 3.1.1 - For purpose of being fully automated k0s is skipping this check type : skip text : \"Client certificate authentication should not be used for users (Manual)\" id: 3.2.1 - out-of-the box configuration does not have any audit policy configuration but users can customize it in spec.api.extraArgs section of the config type : skip text : \"Ensure that a minimal audit policy is created (Manual)\" id: 3.2.2 - Same as previous type : skip text : \"Ensure that the audit policy covers key security concerns (Manual)\" Kubernetes Policies # Entire policies checks are disabled too. The checks are manual and are up to the end user to decide on them.","title":"CIS Benchmark"},{"location":"cis_benchmark/#cis-benchmark","text":"The Center for Internet Security provides set of benchmarks that help us harden our clusters. k0s clusters by default will pass CIS benchmarks with couple of exceptions . For CIS compliance verification we are using kube-bench .","title":"CIS Benchmark"},{"location":"cis_benchmark/#run","text":"kube-bench is CIS compliance verification tool. Follow install instructions . After installing the kube-bench on the host that is running k0s cluster run follwing command: kube-bench run --config-dir docs/kube-bench/cfg/ --benchmark k0s-1.0","title":"Run"},{"location":"cis_benchmark/#summary-of-disabled-checks","text":"","title":"Summary of disabled checks"},{"location":"cis_benchmark/#master-node-security-configuration","text":"Current configuration has in total of 8 master checks disabled: id: 1.2.10 - EventRateLimit requires external yaml config. It is left for the users to configure it type : skip text : \"Ensure that the admission control plugin EventRateLimit is set (Manual)\" id: 1.2.12 - By default this isn't passed to the apiserver for air-gap functionality type : skip text : \"Ensure that the admission control plugin AlwaysPullImages is set (Manual)\" id: 1.2.22 - For sake of simplicity of k0s all audit configuration are skipped. It is left for the users to configure it type : skip text : \"Ensure that the --audit-log-path argument is set (Automated)\" id: 1.2.23 - For sake of simplicity of k0s all audit configuration are skipped. It is left for the users to configure it type : skip text : \"Ensure that the --audit-log-maxage argument is set to 30 or as appropriate (Automated)\" id: 1.2.24 - For sake of simplicity of k0s all audit configuration are skipped. It is left for the users to configure it type : skip text : \"Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate (Automated)\" id: 1.2.25 - For sake of simplicity of k0s all audit configuration are skipped. It is left for the users to configure it type : skip text : \"Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate (Automated)\" id: 1.2.33 - By default it is not enabled. Left for the users to decide type : skip text : \"Ensure that the --encryption-provider-config argument is set as appropriate (Manual)\" id: 1.2.34 - By default it is not enabled. Left for the users to decide type : skip text : \"Ensure that encryption providers are appropriately configured (Manual)\"","title":"Master Node Security Configuration"},{"location":"cis_benchmark/#worker-node-security-configuration","text":"and 4 node checks disabled: id: 4.1.1 - not applicable since k0s does not use kubelet service file type : skip text : \"Ensure that the kubelet service file permissions are set to 644 or more restrictive (Automated)\" id: 4.1.2 - not applicable since k0s does not use kubelet service file type : skip text : \"Ensure that the kubelet service file ownership is set to root:root (Automated)\" id: 4.2.6 - k0s does not set this. See https://github.com/kubernetes/kubernetes/issues/66693 type : skip text : \"Ensure that the --protect-kernel-defaults argument is set to true (Automated)\" id: 4.2.10 - k0s doesn't set this because certs get auto rotated type : skip text : \"Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Manual)\"","title":"Worker Node Security Configuration"},{"location":"cis_benchmark/#control-plane-configuration","text":"3 checks for control plane: id: 3.1.1 - For purpose of being fully automated k0s is skipping this check type : skip text : \"Client certificate authentication should not be used for users (Manual)\" id: 3.2.1 - out-of-the box configuration does not have any audit policy configuration but users can customize it in spec.api.extraArgs section of the config type : skip text : \"Ensure that a minimal audit policy is created (Manual)\" id: 3.2.2 - Same as previous type : skip text : \"Ensure that the audit policy covers key security concerns (Manual)\"","title":"Control Plane Configuration"},{"location":"cis_benchmark/#kubernetes-policies","text":"Entire policies checks are disabled too. The checks are manual and are up to the end user to decide on them.","title":"Kubernetes Policies"},{"location":"cloud-providers/","text":"Cloud providers # k0s builds Kubernetes components in providerless mode, meaning that cloud providers are not built into k0s-managed Kubernetes components. As such, you must externally configure the cloud providers to enable their support in your k0s cluster (for more information on running Kubernetes with cloud providers, refer to the Kubernetes documentation . External Cloud Providers # Enable cloud provider support in kubelet # Even when all components are built with providerless mode, you must be able to enable cloud provider mode for kubelet. To do this, run the workers with --enable-cloud-provider=true . Deploy the cloud provider # The easiest way to deploy cloud provider controllers is on the k0s cluster. Use the built-in manifest deployer built into k0s to deploy your cloud provider as a k0s-managed stack. Next, just drop all required manifests into the /var/lib/k0s/manifests/aws/ directory, and k0s will handle the deployment. Note : The prerequisites for the various cloud providers can vary (for example, several require that configuration files be present on all of the nodes). Refer to your chosen cloud provider's documentation as necessary. k0s Cloud Provider # Alternatively, k0s provides its own lightweight cloud provider that can be used to statically assign ExternalIP values to worker nodes via Kubernetes annotations. This is beneficial for those who need to expose worker nodes externally via static IP assignments. To enable this functionality, add the parameter --enable-k0s-cloud-provider=true to all controllers, and --enable-cloud-provider=true to all workers. Adding a static IP address to a node using kubectl : kubectl annotate \\ node <node> \\ k0sproject.io/node-ip-external=<external IP> Both IPv4 and IPv6 addresses are supported. Defaults # The default node refresh interval is 2m , which can be overridden using the --k0s-cloud-provider-update-frequency=<duration> parameter when launching the controller(s). The default port that the cloud provider binds to can be overridden using the --k0s-cloud-provider-port=<int> parameter when launching the controller(s).","title":"Cloud Providers"},{"location":"cloud-providers/#cloud-providers","text":"k0s builds Kubernetes components in providerless mode, meaning that cloud providers are not built into k0s-managed Kubernetes components. As such, you must externally configure the cloud providers to enable their support in your k0s cluster (for more information on running Kubernetes with cloud providers, refer to the Kubernetes documentation .","title":"Cloud providers"},{"location":"cloud-providers/#external-cloud-providers","text":"","title":"External Cloud Providers"},{"location":"cloud-providers/#enable-cloud-provider-support-in-kubelet","text":"Even when all components are built with providerless mode, you must be able to enable cloud provider mode for kubelet. To do this, run the workers with --enable-cloud-provider=true .","title":"Enable cloud provider support in kubelet"},{"location":"cloud-providers/#deploy-the-cloud-provider","text":"The easiest way to deploy cloud provider controllers is on the k0s cluster. Use the built-in manifest deployer built into k0s to deploy your cloud provider as a k0s-managed stack. Next, just drop all required manifests into the /var/lib/k0s/manifests/aws/ directory, and k0s will handle the deployment. Note : The prerequisites for the various cloud providers can vary (for example, several require that configuration files be present on all of the nodes). Refer to your chosen cloud provider's documentation as necessary.","title":"Deploy the cloud provider"},{"location":"cloud-providers/#k0s-cloud-provider","text":"Alternatively, k0s provides its own lightweight cloud provider that can be used to statically assign ExternalIP values to worker nodes via Kubernetes annotations. This is beneficial for those who need to expose worker nodes externally via static IP assignments. To enable this functionality, add the parameter --enable-k0s-cloud-provider=true to all controllers, and --enable-cloud-provider=true to all workers. Adding a static IP address to a node using kubectl : kubectl annotate \\ node <node> \\ k0sproject.io/node-ip-external=<external IP> Both IPv4 and IPv6 addresses are supported.","title":"k0s Cloud Provider"},{"location":"cloud-providers/#defaults","text":"The default node refresh interval is 2m , which can be overridden using the --k0s-cloud-provider-update-frequency=<duration> parameter when launching the controller(s). The default port that the cloud provider binds to can be overridden using the --k0s-cloud-provider-port=<int> parameter when launching the controller(s).","title":"Defaults"},{"location":"configuration-validation/","text":"Configuration validation # k0s command-line interface has the ability to validate config syntax: k0s validate config --config path/to/config/file validate config sub-command can validate the following: YAML formatting SAN addresses Network providers Worker profiles","title":"Configuration Validation"},{"location":"configuration-validation/#configuration-validation","text":"k0s command-line interface has the ability to validate config syntax: k0s validate config --config path/to/config/file validate config sub-command can validate the following: YAML formatting SAN addresses Network providers Worker profiles","title":"Configuration validation"},{"location":"configuration/","text":"Configuration options # Using a configuration file # k0s can be installed without a config file. In that case the default configuration will be used. You can, though, create and run your own non-default configuration (used by the k0s controller nodes). Generate a yaml config file that uses the default settings. k0s default-config > k0s.yaml Modify the new yaml config file according to your needs, refer to Configuration file reference below. Install k0s with your new config file. sudo k0s install controller -c /path/to/your/config/file If you need to modify your existing configuration later on, you can change your config file also when k0s is running, but remember to restart k0s to apply your configuration changes. sudo k0s stop sudo k0s start Configuration file reference # CAUTION : As many of the available options affect items deep in the stack, you should fully understand the correlation between the configuration file components and your specific environment before making any changes. A YAML config file follows, with defaults as generated by the k0s default-config command: apiVersion : k0s.k0sproject.io/v1beta1 kind : Cluster metadata : name : k0s spec : api : address : 192.168.68.104 port : 6443 k0sApiPort : 9443 externalAddress : my-lb-address.example.com sans : - 192.168.68.104 storage : type : etcd etcd : peerAddress : 192.168.68.104 network : podCIDR : 10.244.0.0/16 serviceCIDR : 10.96.0.0/12 provider : kuberouter calico : null kuberouter : mtu : 0 peerRouterIPs : \"\" peerRouterASNs : \"\" autoMTU : true podSecurityPolicy : defaultPolicy : 00-k0s-privileged telemetry : enabled : true installConfig : users : etcdUser : etcd kineUser : kube-apiserver konnectivityUser : konnectivity-server kubeAPIserverUser : kube-apiserver kubeSchedulerUser : kube-scheduler images : konnectivity : image : us.gcr.io/k8s-artifacts-prod/kas-network-proxy/proxy-agent version : v0.0.16 metricsserver : image : gcr.io/k8s-staging-metrics-server/metrics-server version : v0.3.7 kubeproxy : image : k8s.gcr.io/kube-proxy version : v1.21.6 coredns : image : docker.io/coredns/coredns version : 1.7.0 calico : cni : image : docker.io/calico/cni version : v3.18.1 node : image : docker.io/calico/node version : v3.18.1 kubecontrollers : image : docker.io/calico/kube-controllers version : v3.18.1 kuberouter : cni : image : docker.io/cloudnativelabs/kube-router version : v1.2.1 cniInstaller : image : quay.io/k0sproject/cni-node version : 0.1.0 default_pull_policy : IfNotPresent konnectivity : agentPort : 8132 adminPort : 8133 spec Key Detail # spec.api # Element Description externalAddress The loadbalancer address (for k0s controllers running behind a loadbalancer). Configures all cluster components to connect to this address and also configures this address for use when joining new nodes to the cluster. address Local address on wihich to bind an API. Also serves as one of the addresses pushed on the k0s create service certificate on the API. Defaults to first non-local address found on the node. sans List of additional addresses to push to API servers serving the certificate. extraArgs Map of key-values (strings) for any extra arguments to pass down to Kubernetes api-server process. port \u00b9 Custom port for kube-api server to listen on (default: 6443) k0sApiPort \u00b9 Custom port for k0s-api server to listen on (default: 9443) \u00b9 If port and k0sApiPort are used with the externalAddress element, the loadbalancer serving at externalAddress must listen on the same ports. spec.storage # Element Description type Type of the data store (valid values: etcd or kine ). Note : Type etcd will cause k0s to create and manage an elastic etcd cluster within the controller nodes. etcd.peerAddress Node address used for etcd cluster peering. kine.dataSource kine datasource URL. spec.network # Element Description provider Network provider (valid values: calico , kuberouter , or custom ). For custom , you can push any network provider (default: kuberouter ). Be aware that it is your responsibility to configure all of the CNI-related setups, including the CNI provider itself and all necessary host levels setups (for example, CNI binaries). Note: Once you initialize the cluster with a network provider the only way to change providers is through a full cluster redeployment. podCIDR Pod network CIDR to use in the cluster. serviceCIDR Network CIDR to use for cluster VIP services. spec.network.calico # Element Description mode vxlan (default) or ipip vxlanPort The UDP port for VXLAN (default: 4789 ). vxlanVNI The virtual network ID for VXLAN (default: 4096 ). mtu MTU for overlay network (default: 0 , which causes Calico to detect optimal MTU during bootstrap). wireguard Enable wireguard-based encryption (default: false ). Your host system must be wireguard ready (refer to the Calico documentation for details). flexVolumeDriverPath The host path for Calicos flex-volume-driver(default: /usr/libexec/k0s/kubelet-plugins/volume/exec/nodeagent~uds ). Change this path only if the default path is unwriteable (refer to Project Calico Issue #2712 for details). Ideally, you will pair this option with a custom volumePluginDir in the profile you use for your worker nodes. ipAutodetectionMethod Use to force Calico to pick up the interface for pod network inter-node routing (default: \"\" , meaning not set, so that Calico will instead use its defaults). For more information, refer to the Calico documentation . spec.network.kuberouter # Element Description autoMTU Autodetection of used MTU (default: true ). mtu Override MTU setting, if autoMTU must be set to false ). peerRouterIPs Comma-separated list of global peer addresses . peerRouterASNs Comma-separated list of global peer ASNs . Note : Kube-router allows many networking aspects to be configured per node, service, and pod (for more information, refer to the Kube-router user guide ). spec.podSecurityPolicy # Use the spec.podSecurityPolicy key to configure the default PSP . k0s creates two PSPs out-of-the-box: PSP Description 00-k0s-privileged Default; no restrictions; used also for Kubernetes/k0s level system pods. 99-k0s-restricted Does not allow any host namespaces or root users, nor any bind mounts from the host Note : Users can create supplemental PSPs and bind them to users / access accounts as necessary. spec.controllerManager # Element Description extraArgs Map of key-values (strings) for any extra arguments you want to pass down to the Kubernetes controller manager process. spec.scheduler # Element Description extraArgs Map of key-values (strings) for any extra arguments you want to pass down to Kubernetes scheduler process. spec.workerProfiles # Array of spec.workerProfiles.workerProfile . Each element has following properties: Property Description name String; name to use as profile selector for the worker process values Mapping object For each profile, the control plane creates a separate ConfigMap with kubelet-config yaml . Based on the --profile argument given to the k0s worker , the corresponding ConfigMap is used to extract the kubelet-config.yaml file. values are recursively merged with default kubelet-config.yaml Note that there are several fields that cannot be overridden: clusterDNS clusterDomain apiVersion kind Examples # mapping: spec : workerProfiles : - name : custom-role values : key : value mapping : innerKey : innerValue Custom volumePluginDir: spec : workerProfiles : - name : custom-role values : volumePluginDir : /var/libexec/k0s/kubelet-plugins/volume/exec spec.images # Nodes under the images key all have the same basic structure: spec : images : coredns : image : docker.io/coredns/coredns version : 1.7.0 Available keys # spec.images.konnectivity spec.images.metricsserver spec.images.kubeproxy spec.images.coredns spec.images.calico.cni spec.images.calico.flexvolume spec.images.calico.node spec.images.calico.kubecontrollers spec.images.kuberouter.cni spec.images.kuberouter.cniInstaller spec.images.repository \u00b9 \u00b9 If spec.images.repository is set and not empty, every image will be pulled from images.repository If spec.images.default_pull_policy is set and not empty, it will be used as a pull policy for each bundled image. Example # images : repository : \"my.own.repo\" konnectivity : image : calico/kube-controllers version : v3.16.2 metricsserver : image : gcr.io/k8s-staging-metrics-server/metrics-server version : v0.3.7 In the runtime the image names are calculated as my.own.repo/calico/kube-controllers:v3.16.2 and my.own.repo/k8s-staging-metrics-server/metrics-server . This only affects the the imgages pull location, and thus omitting an image specification here will not disable component deployment. spec.extensions.helm # spec.extensions.helm is the config file key in which you configure the list of Helm repositories and charts to deploy during cluster bootstrap (for more information, refer to Helm Charts ). spec.konnectivity # The spec.konnectivity key is the config file key in which you configure Konnectivity-related settings. agentPort agent port to listen on (default 8132) adminPort admin port to listen on (default 8133) spec.telemetry # To improve the end-user experience k0s is configured by defaul to collect telemetry data from clusters and send it to the k0s development team. To disable the telemetry function, change the enabled setting to false . The telemetry interval is ten minutes. spec : telemetry : enabled : true","title":"Configuration Options"},{"location":"configuration/#configuration-options","text":"","title":"Configuration options"},{"location":"configuration/#using-a-configuration-file","text":"k0s can be installed without a config file. In that case the default configuration will be used. You can, though, create and run your own non-default configuration (used by the k0s controller nodes). Generate a yaml config file that uses the default settings. k0s default-config > k0s.yaml Modify the new yaml config file according to your needs, refer to Configuration file reference below. Install k0s with your new config file. sudo k0s install controller -c /path/to/your/config/file If you need to modify your existing configuration later on, you can change your config file also when k0s is running, but remember to restart k0s to apply your configuration changes. sudo k0s stop sudo k0s start","title":"Using a configuration file"},{"location":"configuration/#configuration-file-reference","text":"CAUTION : As many of the available options affect items deep in the stack, you should fully understand the correlation between the configuration file components and your specific environment before making any changes. A YAML config file follows, with defaults as generated by the k0s default-config command: apiVersion : k0s.k0sproject.io/v1beta1 kind : Cluster metadata : name : k0s spec : api : address : 192.168.68.104 port : 6443 k0sApiPort : 9443 externalAddress : my-lb-address.example.com sans : - 192.168.68.104 storage : type : etcd etcd : peerAddress : 192.168.68.104 network : podCIDR : 10.244.0.0/16 serviceCIDR : 10.96.0.0/12 provider : kuberouter calico : null kuberouter : mtu : 0 peerRouterIPs : \"\" peerRouterASNs : \"\" autoMTU : true podSecurityPolicy : defaultPolicy : 00-k0s-privileged telemetry : enabled : true installConfig : users : etcdUser : etcd kineUser : kube-apiserver konnectivityUser : konnectivity-server kubeAPIserverUser : kube-apiserver kubeSchedulerUser : kube-scheduler images : konnectivity : image : us.gcr.io/k8s-artifacts-prod/kas-network-proxy/proxy-agent version : v0.0.16 metricsserver : image : gcr.io/k8s-staging-metrics-server/metrics-server version : v0.3.7 kubeproxy : image : k8s.gcr.io/kube-proxy version : v1.21.6 coredns : image : docker.io/coredns/coredns version : 1.7.0 calico : cni : image : docker.io/calico/cni version : v3.18.1 node : image : docker.io/calico/node version : v3.18.1 kubecontrollers : image : docker.io/calico/kube-controllers version : v3.18.1 kuberouter : cni : image : docker.io/cloudnativelabs/kube-router version : v1.2.1 cniInstaller : image : quay.io/k0sproject/cni-node version : 0.1.0 default_pull_policy : IfNotPresent konnectivity : agentPort : 8132 adminPort : 8133","title":"Configuration file reference"},{"location":"configuration/#spec-key-detail","text":"","title":"spec Key Detail"},{"location":"configuration/#specapi","text":"Element Description externalAddress The loadbalancer address (for k0s controllers running behind a loadbalancer). Configures all cluster components to connect to this address and also configures this address for use when joining new nodes to the cluster. address Local address on wihich to bind an API. Also serves as one of the addresses pushed on the k0s create service certificate on the API. Defaults to first non-local address found on the node. sans List of additional addresses to push to API servers serving the certificate. extraArgs Map of key-values (strings) for any extra arguments to pass down to Kubernetes api-server process. port \u00b9 Custom port for kube-api server to listen on (default: 6443) k0sApiPort \u00b9 Custom port for k0s-api server to listen on (default: 9443) \u00b9 If port and k0sApiPort are used with the externalAddress element, the loadbalancer serving at externalAddress must listen on the same ports.","title":"spec.api"},{"location":"configuration/#specstorage","text":"Element Description type Type of the data store (valid values: etcd or kine ). Note : Type etcd will cause k0s to create and manage an elastic etcd cluster within the controller nodes. etcd.peerAddress Node address used for etcd cluster peering. kine.dataSource kine datasource URL.","title":"spec.storage"},{"location":"configuration/#specnetwork","text":"Element Description provider Network provider (valid values: calico , kuberouter , or custom ). For custom , you can push any network provider (default: kuberouter ). Be aware that it is your responsibility to configure all of the CNI-related setups, including the CNI provider itself and all necessary host levels setups (for example, CNI binaries). Note: Once you initialize the cluster with a network provider the only way to change providers is through a full cluster redeployment. podCIDR Pod network CIDR to use in the cluster. serviceCIDR Network CIDR to use for cluster VIP services.","title":"spec.network"},{"location":"configuration/#specpodsecuritypolicy","text":"Use the spec.podSecurityPolicy key to configure the default PSP . k0s creates two PSPs out-of-the-box: PSP Description 00-k0s-privileged Default; no restrictions; used also for Kubernetes/k0s level system pods. 99-k0s-restricted Does not allow any host namespaces or root users, nor any bind mounts from the host Note : Users can create supplemental PSPs and bind them to users / access accounts as necessary.","title":"spec.podSecurityPolicy"},{"location":"configuration/#speccontrollermanager","text":"Element Description extraArgs Map of key-values (strings) for any extra arguments you want to pass down to the Kubernetes controller manager process.","title":"spec.controllerManager"},{"location":"configuration/#specscheduler","text":"Element Description extraArgs Map of key-values (strings) for any extra arguments you want to pass down to Kubernetes scheduler process.","title":"spec.scheduler"},{"location":"configuration/#specworkerprofiles","text":"Array of spec.workerProfiles.workerProfile . Each element has following properties: Property Description name String; name to use as profile selector for the worker process values Mapping object For each profile, the control plane creates a separate ConfigMap with kubelet-config yaml . Based on the --profile argument given to the k0s worker , the corresponding ConfigMap is used to extract the kubelet-config.yaml file. values are recursively merged with default kubelet-config.yaml Note that there are several fields that cannot be overridden: clusterDNS clusterDomain apiVersion kind","title":"spec.workerProfiles"},{"location":"configuration/#specimages","text":"Nodes under the images key all have the same basic structure: spec : images : coredns : image : docker.io/coredns/coredns version : 1.7.0","title":"spec.images"},{"location":"configuration/#specextensionshelm","text":"spec.extensions.helm is the config file key in which you configure the list of Helm repositories and charts to deploy during cluster bootstrap (for more information, refer to Helm Charts ).","title":"spec.extensions.helm"},{"location":"configuration/#speckonnectivity","text":"The spec.konnectivity key is the config file key in which you configure Konnectivity-related settings. agentPort agent port to listen on (default 8132) adminPort admin port to listen on (default 8133)","title":"spec.konnectivity"},{"location":"configuration/#spectelemetry","text":"To improve the end-user experience k0s is configured by defaul to collect telemetry data from clusters and send it to the k0s development team. To disable the telemetry function, change the enabled setting to false . The telemetry interval is ten minutes. spec : telemetry : enabled : true","title":"spec.telemetry"},{"location":"conformance-testing/","text":"Kubernetes conformance testing for k0s # We run the conformance testing for the last RC build for a release. Follow the instructions as the conformance testing repository. In a nutshell, you need to: Setup k0s on some VMs/bare metal boxes Download, if you do not already have, sonobuoy tool Run the conformance tests with something like sonobuoy run --mode=certified-conformance Wait for couple hours Collect results","title":"Kubernetes conformance testing for k0s"},{"location":"conformance-testing/#kubernetes-conformance-testing-for-k0s","text":"We run the conformance testing for the last RC build for a release. Follow the instructions as the conformance testing repository. In a nutshell, you need to: Setup k0s on some VMs/bare metal boxes Download, if you do not already have, sonobuoy tool Run the conformance tests with something like sonobuoy run --mode=certified-conformance Wait for couple hours Collect results","title":"Kubernetes conformance testing for k0s"},{"location":"containerd_config/","text":"See runtime .","title":"Containerd config"},{"location":"custom-cri-runtime/","text":"See runtime .","title":"Custom cri runtime"},{"location":"dual-stack/","text":"Dual-stack Networking # Note: Dual stack networking setup requires that you configure Calico or a custom CNI as the CNI provider. Use the following k0s.yaml as a template to enable dual-stack networking. This configuration will set up bundled calico CNI, enable feature gates for the Kubernetes components, and set up kubernetes-controller-manager . spec : network : podCIDR : \"10.244.0.0/16\" serviceCIDR : \"10.96.0.0/12\" calico : mode : \"bird\" dualStack : enabled : true IPv6podCIDR : \"fd00::/108\" IPv6serviceCIDR : \"fd01::/108\" CNI Settings: Calico # For cross-pod connectivity, use BIRD for the backend. Calico does not support tunneling for the IPv6, and thus VXLAN and IPIP backends do not work. Note : In any Calico mode other than cross-pod, the pods can only reach pods on the same node. CNI Settings: External CNI # Although the k0s.yaml dualStack section enables all of the neccessary feature gates for the Kubernetes components, for use with an external CNI it must be set up to support IPv6. Additional Resources # https://kubernetes.io/docs/concepts/services-networking/dual-stack/ https://kubernetes.io/docs/tasks/network/validate-dual-stack/ https://www.projectcalico.org/dual-stack-operation-with-calico-on-kubernetes/ https://docs.projectcalico.org/networking/ipv6","title":"IPv4/IPv6 Dual-Stack"},{"location":"dual-stack/#dual-stack-networking","text":"Note: Dual stack networking setup requires that you configure Calico or a custom CNI as the CNI provider. Use the following k0s.yaml as a template to enable dual-stack networking. This configuration will set up bundled calico CNI, enable feature gates for the Kubernetes components, and set up kubernetes-controller-manager . spec : network : podCIDR : \"10.244.0.0/16\" serviceCIDR : \"10.96.0.0/12\" calico : mode : \"bird\" dualStack : enabled : true IPv6podCIDR : \"fd00::/108\" IPv6serviceCIDR : \"fd01::/108\"","title":"Dual-stack Networking"},{"location":"dual-stack/#cni-settings-calico","text":"For cross-pod connectivity, use BIRD for the backend. Calico does not support tunneling for the IPv6, and thus VXLAN and IPIP backends do not work. Note : In any Calico mode other than cross-pod, the pods can only reach pods on the same node.","title":"CNI Settings: Calico"},{"location":"dual-stack/#cni-settings-external-cni","text":"Although the k0s.yaml dualStack section enables all of the neccessary feature gates for the Kubernetes components, for use with an external CNI it must be set up to support IPv6.","title":"CNI Settings: External CNI"},{"location":"dual-stack/#additional-resources","text":"https://kubernetes.io/docs/concepts/services-networking/dual-stack/ https://kubernetes.io/docs/tasks/network/validate-dual-stack/ https://www.projectcalico.org/dual-stack-operation-with-calico-on-kubernetes/ https://docs.projectcalico.org/networking/ipv6","title":"Additional Resources"},{"location":"experimental-windows/","text":"Run k0s worker nodes in Windows # IMPORTANT : Windows support for k0s is under active development and must be considered experimental. Prerequisites # The cluster must be running at least one worker node and control plane on Linux. You ca use Windows to run additional worker nodes. Build k0s.exe # Invoke the make clean k0s.exe command to create k0s.exe with staged kubelet.exe and kube-proxy.exe. Note : The k0s.exe supervises kubelet.exe and kube-proxy.exe. During the first run, the calico install script is creaated as C:\\bootstrap.ps1 . This bootstrap script downloads the calico binaries, builds pause container and set ups vSwitch settings. Run k0s # Install Mirantis Container Runtime on the Windows node(s), as it is required for the initial Calico set up). k0s worker --cri-socket = docker:tcp://127.0.0.1:2375 --cidr-range = <cidr_range> --cluster-dns = <clusterdns> --api-server = <k0s api> <token> You must initiate the Cluster control with the correct config. Configuration # Strict-affinity # You must enable strict affinity to run the windows node. If the spec.network.calico.withWindowsNodes field is set to true (it is set to false by default) the additional calico related manifest /var/lib/k0s/manifests/calico/calico-IPAMConfig-ipamconfig.yaml is created with the following values: --- apiVersion : crd.projectcalico.org/v1 kind : IPAMConfig metadata : name : default spec : strictAffinity : true Alternately, you can manually execute calicoctl: calicoctl ipam configure --strictaffinity = true Network connectivity in AWS # Disable the Change Source/Dest. Check option for the network interface attached to your EC2 instance. In AWS, the console option for the network interface is in the Actions menu. Hacks # k0s offers the following CLI arguments in lieu of a formal means for passing cluster settings from controller plane to worker: cidr-range cluster-dns api-server Useful commands # Run pod with cmd.exe shell # kubectl run win --image = hello-world:nanoserver --command = true -i --attach = true -- cmd.exe Manifest for pod with IIS web-server # apiVersion : v1 kind : Pod metadata : name : iis spec : containers : - name : iis image : mcr.microsoft.com/windows/servercore/iis imagePullPolicy : IfNotPresent","title":"Windows (experimental)"},{"location":"experimental-windows/#run-k0s-worker-nodes-in-windows","text":"IMPORTANT : Windows support for k0s is under active development and must be considered experimental.","title":"Run k0s worker nodes in Windows"},{"location":"experimental-windows/#prerequisites","text":"The cluster must be running at least one worker node and control plane on Linux. You ca use Windows to run additional worker nodes.","title":"Prerequisites"},{"location":"experimental-windows/#build-k0sexe","text":"Invoke the make clean k0s.exe command to create k0s.exe with staged kubelet.exe and kube-proxy.exe. Note : The k0s.exe supervises kubelet.exe and kube-proxy.exe. During the first run, the calico install script is creaated as C:\\bootstrap.ps1 . This bootstrap script downloads the calico binaries, builds pause container and set ups vSwitch settings.","title":"Build k0s.exe"},{"location":"experimental-windows/#run-k0s","text":"Install Mirantis Container Runtime on the Windows node(s), as it is required for the initial Calico set up). k0s worker --cri-socket = docker:tcp://127.0.0.1:2375 --cidr-range = <cidr_range> --cluster-dns = <clusterdns> --api-server = <k0s api> <token> You must initiate the Cluster control with the correct config.","title":"Run k0s"},{"location":"experimental-windows/#configuration","text":"","title":"Configuration"},{"location":"experimental-windows/#strict-affinity","text":"You must enable strict affinity to run the windows node. If the spec.network.calico.withWindowsNodes field is set to true (it is set to false by default) the additional calico related manifest /var/lib/k0s/manifests/calico/calico-IPAMConfig-ipamconfig.yaml is created with the following values: --- apiVersion : crd.projectcalico.org/v1 kind : IPAMConfig metadata : name : default spec : strictAffinity : true Alternately, you can manually execute calicoctl: calicoctl ipam configure --strictaffinity = true","title":"Strict-affinity"},{"location":"experimental-windows/#network-connectivity-in-aws","text":"Disable the Change Source/Dest. Check option for the network interface attached to your EC2 instance. In AWS, the console option for the network interface is in the Actions menu.","title":"Network connectivity in AWS"},{"location":"experimental-windows/#hacks","text":"k0s offers the following CLI arguments in lieu of a formal means for passing cluster settings from controller plane to worker: cidr-range cluster-dns api-server","title":"Hacks"},{"location":"experimental-windows/#useful-commands","text":"","title":"Useful commands"},{"location":"experimental-windows/#run-pod-with-cmdexe-shell","text":"kubectl run win --image = hello-world:nanoserver --command = true -i --attach = true -- cmd.exe","title":"Run pod with cmd.exe shell"},{"location":"experimental-windows/#manifest-for-pod-with-iis-web-server","text":"apiVersion : v1 kind : Pod metadata : name : iis spec : containers : - name : iis image : mcr.microsoft.com/windows/servercore/iis imagePullPolicy : IfNotPresent","title":"Manifest for pod with IIS web-server"},{"location":"extensions/","text":"Cluster extensions # k0s allows users to use extensions to extend cluster functionality. At the moment the only supported type of extensions is helm based charts. The default configuration has no extensions. Helm based extensions # Configuration example # helm : repositories : - name : stable url : https://charts.helm.sh/stable - name : prometheus-community url : https://prometheus-community.github.io/helm-charts charts : - name : prometheus-stack chartname : prometheus-community/prometheus version : \"11.16.8\" values : | storageSpec: emptyDir: medium: Memory namespace : default By using the configuration above, the cluster would: add stable and prometheus-community chart repositories install the prometheus-community/prometheus chart of the specified version to the default namespace. The chart installation is implemented by using CRD helm.k0sproject.io/Chart . For every given helm extension the cluster creates a Chart CRD instance. The cluster has a controller which monitors for the Chart CRDs, supporting the following operations: install upgrade delete For security reasons, the cluster operates only on Chart CRDs instantiated in the kube-system namespace, however, the target namespace could be any. CRD definition # apiVersion : helm.k0sproject.io/v1beta1 kind : Chart metadata : creationTimestamp : \"2020-11-10T14:17:53Z\" generation : 2 labels : k0s.k0sproject.io/stack : helm name : k0s-addon-chart-test-addon namespace : kube-system resourceVersion : \"627\" selfLink : /apis/helm.k0sproject.io/v1beta1/namespaces/kube-system/charts/k0s-addon-chart-test-addon uid : ebe59ed4-1ff8-4d41-8e33-005b183651ed spec : chartName : prometheus-community/prometheus namespace : default values : | storageSpec: emptyDir: medium: Memory version : 11.16.8 status : appVersion : 2.21.0 namespace : default releaseName : prometheus-1605017878 revision : 2 updated : 2020-11-10 14:18:08.235656 +0000 UTC m=+41.871656901 version : 11.16.8 The Chart.spec defines the chart information. The Chart.status keeps the information about the last operation performed by the operator.","title":"Cluster extensions"},{"location":"extensions/#cluster-extensions","text":"k0s allows users to use extensions to extend cluster functionality. At the moment the only supported type of extensions is helm based charts. The default configuration has no extensions.","title":"Cluster extensions"},{"location":"extensions/#helm-based-extensions","text":"","title":"Helm based extensions"},{"location":"extensions/#configuration-example","text":"helm : repositories : - name : stable url : https://charts.helm.sh/stable - name : prometheus-community url : https://prometheus-community.github.io/helm-charts charts : - name : prometheus-stack chartname : prometheus-community/prometheus version : \"11.16.8\" values : | storageSpec: emptyDir: medium: Memory namespace : default By using the configuration above, the cluster would: add stable and prometheus-community chart repositories install the prometheus-community/prometheus chart of the specified version to the default namespace. The chart installation is implemented by using CRD helm.k0sproject.io/Chart . For every given helm extension the cluster creates a Chart CRD instance. The cluster has a controller which monitors for the Chart CRDs, supporting the following operations: install upgrade delete For security reasons, the cluster operates only on Chart CRDs instantiated in the kube-system namespace, however, the target namespace could be any.","title":"Configuration example"},{"location":"helm-charts/","text":"Helm Charts # Defining your extensions as Helm charts is one of two methods you can use to run k0s with your preferred extensions (the other being through the use of Manifest Deployer . k0s supports two methods for deploying applications using Helm charts: Use Helm command in runtime to install applications. Refer to the Helm Quickstart Guide for more information. Insert Helm charts directly into the k0s configuration file, k0s.yaml . This method does not require a separate install of helm tool and the charts automatically deploy at the k0s bootstrap phase. Helm charts in k0s configuration # Adding Helm charts into the k0s configuration file gives you a declarative way in which to configure the cluster. k0s controller manages the setup of Helm charts that are defined as extensions in the k0s configuration file. Example # In the example, Prometheus is configured from \"stable\" Helms chart repository. Add the following to k0s.yaml and restart k0s, after which Prometheus should start automatically with k0s. spec : extensions : helm : repositories : - name : stable url : https://charts.helm.sh/stable - name : prometheus-community url : https://prometheus-community.github.io/helm-charts charts : - name : prometheus-stack chartname : prometheus-community/prometheus version : \"11.16.8\" values : | storageSpec: emptyDir: medium: Memory namespace : default Example extensions that you can use with Helm charts include: Ingress controllers: Nginx ingress , Traefix ingress (refer to the k0s documentation for Installing the Traefik Ingress Controller ) Volume storage providers: OpenEBS , Rook , Longhorn Monitoring: Prometheus , Grafana","title":"Helm Charts"},{"location":"helm-charts/#helm-charts","text":"Defining your extensions as Helm charts is one of two methods you can use to run k0s with your preferred extensions (the other being through the use of Manifest Deployer . k0s supports two methods for deploying applications using Helm charts: Use Helm command in runtime to install applications. Refer to the Helm Quickstart Guide for more information. Insert Helm charts directly into the k0s configuration file, k0s.yaml . This method does not require a separate install of helm tool and the charts automatically deploy at the k0s bootstrap phase.","title":"Helm Charts"},{"location":"helm-charts/#helm-charts-in-k0s-configuration","text":"Adding Helm charts into the k0s configuration file gives you a declarative way in which to configure the cluster. k0s controller manages the setup of Helm charts that are defined as extensions in the k0s configuration file.","title":"Helm charts in k0s configuration"},{"location":"helm-charts/#example","text":"In the example, Prometheus is configured from \"stable\" Helms chart repository. Add the following to k0s.yaml and restart k0s, after which Prometheus should start automatically with k0s. spec : extensions : helm : repositories : - name : stable url : https://charts.helm.sh/stable - name : prometheus-community url : https://prometheus-community.github.io/helm-charts charts : - name : prometheus-stack chartname : prometheus-community/prometheus version : \"11.16.8\" values : | storageSpec: emptyDir: medium: Memory namespace : default Example extensions that you can use with Helm charts include: Ingress controllers: Nginx ingress , Traefix ingress (refer to the k0s documentation for Installing the Traefik Ingress Controller ) Volume storage providers: OpenEBS , Rook , Longhorn Monitoring: Prometheus , Grafana","title":"Example"},{"location":"high-availability/","text":"Control Plane High Availability # You can create high availability for the control plane by distributing the control plane across multiple nodes and installing a load balancer on top. Etcd can be colocated with the controller nodes (default in k0s) to achieve highly available datastore at the same time. Network considerations # You should plan to allocate the control plane nodes into different zones. This will avoid failures in case one zone fails. For etcd high availability it's recommended to configure 3 or 5 controller nodes. For more information, refer to the etcd documentation . Load Balancer # Control plane high availability requires a tcp load balancer, which acts as a single point of contact to access the controllers. The load balancer needs to allow and route traffic to each controller through the following ports: 6443 (for Kubernetes API) 8132 (for Konnectivity agent) 8133 (for Konnectivity server) 9443 (for controller join API) The load balancer can be implemented in many different ways and k0s doesn't have any additional requirements. You can use for example HAProxy, NGINX or your cloud provider's load balancer. Example configuration: HAProxy # Change the default mode to tcp under the 'defaults' section of haproxy.cfg. Add the following lines to the end of the haproxy.cfg: frontend kubeAPI bind :6443 default_backend back frontend konnectivityAgent bind :8132 default_backend back frontend konnectivityServer bind :8133 default_backend back frontend controllerJoinAPI bind :9443 default_backend back backend back server k0s-controller1 <ip-address1> server k0s-controller2 <ip-address2> server k0s-controller3 <ip-address3> Restart HAProxy to apply the configuration changes. k0s configuration # The load balancer address must be configured to k0s either by using k0s.yaml or by using k0sctl to automatically deploy all controllers with the same configuration: Configuration using k0s.yaml (for each controller) # Note to update your load balancer's public ip address into two places. spec : api : externalAddress : <load balancer public ip address> sans : - <load balancer public ip address> Configuration using k0sctl.yaml (for k0sctl) # Add the following lines to the end of the k0sctl.yaml. Note to update your load balancer's public ip address into two places. k0s : config : spec : api : externalAddress : <load balancer public ip address> sans : - <load balancer public ip address> For greater detail about k0s configuration, refer to the Full configuration file reference .","title":"Control Plane High Availability"},{"location":"high-availability/#control-plane-high-availability","text":"You can create high availability for the control plane by distributing the control plane across multiple nodes and installing a load balancer on top. Etcd can be colocated with the controller nodes (default in k0s) to achieve highly available datastore at the same time.","title":"Control Plane High Availability"},{"location":"high-availability/#network-considerations","text":"You should plan to allocate the control plane nodes into different zones. This will avoid failures in case one zone fails. For etcd high availability it's recommended to configure 3 or 5 controller nodes. For more information, refer to the etcd documentation .","title":"Network considerations"},{"location":"high-availability/#load-balancer","text":"Control plane high availability requires a tcp load balancer, which acts as a single point of contact to access the controllers. The load balancer needs to allow and route traffic to each controller through the following ports: 6443 (for Kubernetes API) 8132 (for Konnectivity agent) 8133 (for Konnectivity server) 9443 (for controller join API) The load balancer can be implemented in many different ways and k0s doesn't have any additional requirements. You can use for example HAProxy, NGINX or your cloud provider's load balancer.","title":"Load Balancer"},{"location":"high-availability/#example-configuration-haproxy","text":"Change the default mode to tcp under the 'defaults' section of haproxy.cfg. Add the following lines to the end of the haproxy.cfg: frontend kubeAPI bind :6443 default_backend back frontend konnectivityAgent bind :8132 default_backend back frontend konnectivityServer bind :8133 default_backend back frontend controllerJoinAPI bind :9443 default_backend back backend back server k0s-controller1 <ip-address1> server k0s-controller2 <ip-address2> server k0s-controller3 <ip-address3> Restart HAProxy to apply the configuration changes.","title":"Example configuration: HAProxy"},{"location":"high-availability/#k0s-configuration","text":"The load balancer address must be configured to k0s either by using k0s.yaml or by using k0sctl to automatically deploy all controllers with the same configuration:","title":"k0s configuration"},{"location":"high-availability/#configuration-using-k0syaml-for-each-controller","text":"Note to update your load balancer's public ip address into two places. spec : api : externalAddress : <load balancer public ip address> sans : - <load balancer public ip address>","title":"Configuration using k0s.yaml (for each controller)"},{"location":"high-availability/#configuration-using-k0sctlyaml-for-k0sctl","text":"Add the following lines to the end of the k0sctl.yaml. Note to update your load balancer's public ip address into two places. k0s : config : spec : api : externalAddress : <load balancer public ip address> sans : - <load balancer public ip address> For greater detail about k0s configuration, refer to the Full configuration file reference .","title":"Configuration using k0sctl.yaml (for k0sctl)"},{"location":"install/","text":"Quick Start Guide # On completion of the Quick Start you will have a full Kubernetes cluster with a single node that includes both the controller and the worker. Such a setup is ideal for environments that do not require high-availability and multiple nodes. Prerequisites # Note : Before proceeding, make sure to review the System Requirements . Though the Quick Start material is written for Debian/Ubuntu, you can use it for any Linux distro that is running either a Systemd or OpenRC init system. Install k0s # Download k0s Run the k0s download script to download the latest stable version of k0s and make it executable from /usr/bin/k0s. curl -sSLf https://get.k0s.sh | sudo sh Install k0s as a service The k0s install sub-command installs k0s as a system service on the local host that is running one of the supported init systems: Systemd or OpenRC. You can execute the install for workers, controllers or single node (controller+worker) instances. Run the following command to install a single node k0s that includes the controller and worker functions with the default configuration: sudo k0s install controller --single The k0s install controller sub-command accepts the same flags and parameters as the k0s controller . Refer to manual install for a custom config file example. Start k0s as a service To start the k0s service, run: sudo k0s start The k0s service will start automatically after the node restart. A minute or two typically passes before the node is ready to deploy applications. Check service, logs and k0s status To get general information about your k0s instance's status, run: $ sudo k0s status Version: v0.11.0 Process ID: 436 Parent Process ID: 1 Role: controller+worker Init System: linux-systemd Access your cluster using kubectl Note : k0s includes the Kubernetes command-line tool kubectl . Use kubectl to deploy your application or to check your node status: $ sudo k0s kubectl get nodes NAME STATUS ROLES AGE VERSION k0s Ready <none> 4m6s v1.21.6-k0s1 Uninstall k0s # The removal of k0s is a two-step process. Stop the service. sudo k0s stop Execute the k0s reset command. The k0s reset command cleans up the installed system service, data directories, containers, mounts and network namespaces. sudo k0s reset Reboot the system. A few small k0s fragments persist even after the reset (for example, iptables). As such, you should initiate a reboot after the running of the k0s reset command. Next Steps # Install using k0sctl : Deploy multi-node clusters using just one command Manual Install : (Advanced) Manually deploy multi-node clusters Control plane configuration options : Networking and datastore configuration Worker node configuration options : Node labels and kubelet arguments Support for cloud providers : Load balancer or storage configuration Installing the Traefik Ingress Controller : Ingress deployment information Airgap/Offline installation : Airgap deployment","title":"Quick Start Guide"},{"location":"install/#quick-start-guide","text":"On completion of the Quick Start you will have a full Kubernetes cluster with a single node that includes both the controller and the worker. Such a setup is ideal for environments that do not require high-availability and multiple nodes.","title":"Quick Start Guide"},{"location":"install/#prerequisites","text":"Note : Before proceeding, make sure to review the System Requirements . Though the Quick Start material is written for Debian/Ubuntu, you can use it for any Linux distro that is running either a Systemd or OpenRC init system.","title":"Prerequisites"},{"location":"install/#install-k0s","text":"Download k0s Run the k0s download script to download the latest stable version of k0s and make it executable from /usr/bin/k0s. curl -sSLf https://get.k0s.sh | sudo sh Install k0s as a service The k0s install sub-command installs k0s as a system service on the local host that is running one of the supported init systems: Systemd or OpenRC. You can execute the install for workers, controllers or single node (controller+worker) instances. Run the following command to install a single node k0s that includes the controller and worker functions with the default configuration: sudo k0s install controller --single The k0s install controller sub-command accepts the same flags and parameters as the k0s controller . Refer to manual install for a custom config file example. Start k0s as a service To start the k0s service, run: sudo k0s start The k0s service will start automatically after the node restart. A minute or two typically passes before the node is ready to deploy applications. Check service, logs and k0s status To get general information about your k0s instance's status, run: $ sudo k0s status Version: v0.11.0 Process ID: 436 Parent Process ID: 1 Role: controller+worker Init System: linux-systemd Access your cluster using kubectl Note : k0s includes the Kubernetes command-line tool kubectl . Use kubectl to deploy your application or to check your node status: $ sudo k0s kubectl get nodes NAME STATUS ROLES AGE VERSION k0s Ready <none> 4m6s v1.21.6-k0s1","title":"Install k0s"},{"location":"install/#uninstall-k0s","text":"The removal of k0s is a two-step process. Stop the service. sudo k0s stop Execute the k0s reset command. The k0s reset command cleans up the installed system service, data directories, containers, mounts and network namespaces. sudo k0s reset Reboot the system. A few small k0s fragments persist even after the reset (for example, iptables). As such, you should initiate a reboot after the running of the k0s reset command.","title":"Uninstall k0s"},{"location":"install/#next-steps","text":"Install using k0sctl : Deploy multi-node clusters using just one command Manual Install : (Advanced) Manually deploy multi-node clusters Control plane configuration options : Networking and datastore configuration Worker node configuration options : Node labels and kubelet arguments Support for cloud providers : Load balancer or storage configuration Installing the Traefik Ingress Controller : Ingress deployment information Airgap/Offline installation : Airgap deployment","title":"Next Steps"},{"location":"k0s-in-docker/","text":"Run k0s in Docker # You can create a k0s cluster on top of docker. In such a scenario, by default, both controller and worker nodes are run in the same container to provide an easy local testing \"cluster\". Prerequisites # You will require a Docker environment running on a Mac, Windows, or Linux system. Container images # The k0s containers are published both on Docker Hub and GitHub. For reasons of simplicity, the examples given here use Docker Hub (GitHub requires a separate authentication that is not covered). Alternative links include: docker.io/k0sproject/k0s:latest docker.pkg.github.com/k0sproject/k0s/k0s:\"version\" Start k0s # 1. Initiate k0s # You can run your own k0s in Docker: docker run -d --name k0s --hostname k0s --privileged -v /var/lib/k0s -p 6443 :6443 docker.io/k0sproject/k0s:latest 2. (Optional) Create additional workers # You can attach multiple workers nodes into the cluster to then distribute your application containers to separate workers. For each required worker: Acquire a join token for the worker: token = $( docker exec -t -i k0s k0s token create --role = worker ) Run the container to create and join the new worker: docker run -d --name k0s-worker1 --hostname k0s-worker1 --privileged -v /var/lib/k0s docker.io/k0sproject/k0s:latest k0s worker $token 3. Access your cluster # Access your cluster using kubectl: docker exec k0s kubectl get nodes Alternatively, grab the kubeconfig file with docker exec k0s cat /var/lib/k0s/pki/admin.conf and paste it into Lens . Use Docker Compose (alternative) # As an alternative you can run k0s using Docker Compose: version : \"3.9\" services : k0s : container_name : k0s image : docker.io/k0sproject/k0s:latest command : k0s controller --config=/etc/k0s/config.yaml --enable-worker hostname : k0s privileged : true volumes : - \"/var/lib/k0s\" tmpfs : - /run - /var/run ports : - \"6443:6443\" network_mode : \"bridge\" environment : K0S_CONFIG : |- apiVersion: k0s.k0sproject.io/v1beta1 kind: Cluster metadata: name: k0s # Any additional configuration goes here ... Known limitations # No custom Docker networks # Currently, k0s nodes cannot be run if the containers are configured to use custom networks (for example, with --net my-net ). This is because Docker sets up a custom DNS service within the network which creates issues with CoreDNS. No completely reliable workaounds are available, however no issues should arise from running k0s cluster(s) on a bridge network. Next Steps # Install using k0sctl : Deploy multi-node clusters using just one command Control plane configuration options : Networking and datastore configuration Worker node configuration options : Node labels and kubelet arguments Support for cloud providers : Load balancer or storage configuration Installing the Traefik Ingress Controller : Ingress deployment information","title":"Docker"},{"location":"k0s-in-docker/#run-k0s-in-docker","text":"You can create a k0s cluster on top of docker. In such a scenario, by default, both controller and worker nodes are run in the same container to provide an easy local testing \"cluster\".","title":"Run k0s in Docker"},{"location":"k0s-in-docker/#prerequisites","text":"You will require a Docker environment running on a Mac, Windows, or Linux system.","title":"Prerequisites"},{"location":"k0s-in-docker/#container-images","text":"The k0s containers are published both on Docker Hub and GitHub. For reasons of simplicity, the examples given here use Docker Hub (GitHub requires a separate authentication that is not covered). Alternative links include: docker.io/k0sproject/k0s:latest docker.pkg.github.com/k0sproject/k0s/k0s:\"version\"","title":"Container images"},{"location":"k0s-in-docker/#start-k0s","text":"","title":"Start k0s"},{"location":"k0s-in-docker/#1-initiate-k0s","text":"You can run your own k0s in Docker: docker run -d --name k0s --hostname k0s --privileged -v /var/lib/k0s -p 6443 :6443 docker.io/k0sproject/k0s:latest","title":"1. Initiate k0s"},{"location":"k0s-in-docker/#2-optional-create-additional-workers","text":"You can attach multiple workers nodes into the cluster to then distribute your application containers to separate workers. For each required worker: Acquire a join token for the worker: token = $( docker exec -t -i k0s k0s token create --role = worker ) Run the container to create and join the new worker: docker run -d --name k0s-worker1 --hostname k0s-worker1 --privileged -v /var/lib/k0s docker.io/k0sproject/k0s:latest k0s worker $token","title":"2. (Optional) Create additional workers"},{"location":"k0s-in-docker/#3-access-your-cluster","text":"Access your cluster using kubectl: docker exec k0s kubectl get nodes Alternatively, grab the kubeconfig file with docker exec k0s cat /var/lib/k0s/pki/admin.conf and paste it into Lens .","title":"3. Access your cluster"},{"location":"k0s-in-docker/#use-docker-compose-alternative","text":"As an alternative you can run k0s using Docker Compose: version : \"3.9\" services : k0s : container_name : k0s image : docker.io/k0sproject/k0s:latest command : k0s controller --config=/etc/k0s/config.yaml --enable-worker hostname : k0s privileged : true volumes : - \"/var/lib/k0s\" tmpfs : - /run - /var/run ports : - \"6443:6443\" network_mode : \"bridge\" environment : K0S_CONFIG : |- apiVersion: k0s.k0sproject.io/v1beta1 kind: Cluster metadata: name: k0s # Any additional configuration goes here ...","title":"Use Docker Compose (alternative)"},{"location":"k0s-in-docker/#known-limitations","text":"","title":"Known limitations"},{"location":"k0s-in-docker/#no-custom-docker-networks","text":"Currently, k0s nodes cannot be run if the containers are configured to use custom networks (for example, with --net my-net ). This is because Docker sets up a custom DNS service within the network which creates issues with CoreDNS. No completely reliable workaounds are available, however no issues should arise from running k0s cluster(s) on a bridge network.","title":"No custom Docker networks"},{"location":"k0s-in-docker/#next-steps","text":"Install using k0sctl : Deploy multi-node clusters using just one command Control plane configuration options : Networking and datastore configuration Worker node configuration options : Node labels and kubelet arguments Support for cloud providers : Load balancer or storage configuration Installing the Traefik Ingress Controller : Ingress deployment information","title":"Next Steps"},{"location":"k0s-multi-node/","text":"Manual Install (Advanced) # You can manually set up k0s nodes by creating a multi-node cluster that is locally managed on each node. This involves several steps, to first install each node separately, and to then connect the node together using access tokens. Prerequisites # Note : Before proceeding, make sure to review the System Requirements . Though the Manual Install material is written for Debian/Ubuntu, you can use it for any Linux distro that is running either a Systemd or OpenRC init system. You can speed up the use of the k0s command by enabling shell completion . Install k0s # 1. Download k0s # Run the k0s download script to download the latest stable version of k0s and make it executable from /usr/bin/k0s. curl -sSLf https://get.k0s.sh | sudo sh The download script accepts the following environment variables: Variable Purpose `K0S_VERSION=v1.21.6+k0s.0 Select the version of k0s to be installed DEBUG=true Output commands and their arguments at execution. Note : If you require environment variables and use sudo, you can do: curl -sSLf https://get.k0s.sh | sudo K0S_VERSION = v1.21.6+k0s.0 sh 2. Bootstrap a controller node # Create a configuration file: k0s default-config > k0s.yaml Note : For information on settings modification, refer to the configuration documentation. sudo k0s install controller -c k0s.yaml sudo k0s start k0s process acts as a \"supervisor\" for all of the control plane components. In moments the control plane will be up and running. 3. Create a join token # You need a token to join workers to the cluster. The token embeds information that enables mutual trust between the worker and controller(s) and which allows the node to join the cluster as worker. To get a token, run the following command on one of the existing controller nodes: k0s token create --role = worker The resulting output is a long token string, which you can use to add a worker to the cluster. For enhanced security, run the following command to set an expiration time for the token: k0s token create --role = worker --expiry = 100h > token-file 4. Add workers to the cluster # To join the worker, run k0s in the worker mode with the join token you created: sudo k0s install worker --token-file /path/to/token/file sudo k0s start About tokens # The join tokens are base64-encoded kubeconfigs for several reasons: Well-defined structure Capable of direct use as bootstrap auth configs for kubelet Embedding of CA info for mutual trust The bearer token embedded in the kubeconfig is a bootstrap token . For controller join tokens and worker join tokens k0s uses different usage attributes to ensure that k0s can validate the token role on the controller side. 5. Add controllers to the cluster # Note : Either etcd or an external data store (MySQL or Postgres) via kine must be in use to add new controller nodes to the cluster. Pay strict attention to the high availability configuration and make sure the configuration is identical for all controller nodes. To create a join token for the new controller, run the following command on an existing controller: k0s token create --role = controller --expiry = 1h > token-file On the new controller, run: sudo k0s install controller --token-file /path/to/token/file k0s start 6. Check k0s status # To get general information about your k0s instance's status: $ sudo k0s status Version: v1.21.6+k0s.0 Process ID: 2769 Parent Process ID: 1 Role: controller Init System: linux-systemd Service file: /etc/systemd/system/k0scontroller.service 7. Access your cluster # Use the Kubernetes 'kubectl' command-line tool that comes with k0s binary to deploy your application or check your node status: $ sudo k0s kubectl get nodes NAME STATUS ROLES AGE VERSION k0s Ready <none> 4m6s v1.21.6-k0s1 You can also access your cluster easily with Lens , simply by copying the kubeconfig and pasting it to Lens: sudo cat /var/lib/k0s/pki/admin.conf Note : To access the cluster from an external network you must replace localhost in the kubeconfig with the host ip address for your controller. Next Steps # Install using k0sctl : Deploy multi-node clusters using just one command Control plane configuration options : Networking and datastore configuration Worker node configuration options : Node labels and kubelet arguments Support for cloud providers : Load balancer or storage configuration Installing the Traefik Ingress Controller : Ingress deployment information","title":"Manual Install (advanced)"},{"location":"k0s-multi-node/#manual-install-advanced","text":"You can manually set up k0s nodes by creating a multi-node cluster that is locally managed on each node. This involves several steps, to first install each node separately, and to then connect the node together using access tokens.","title":"Manual Install (Advanced)"},{"location":"k0s-multi-node/#prerequisites","text":"Note : Before proceeding, make sure to review the System Requirements . Though the Manual Install material is written for Debian/Ubuntu, you can use it for any Linux distro that is running either a Systemd or OpenRC init system. You can speed up the use of the k0s command by enabling shell completion .","title":"Prerequisites"},{"location":"k0s-multi-node/#install-k0s","text":"","title":"Install k0s"},{"location":"k0s-multi-node/#1-download-k0s","text":"Run the k0s download script to download the latest stable version of k0s and make it executable from /usr/bin/k0s. curl -sSLf https://get.k0s.sh | sudo sh The download script accepts the following environment variables: Variable Purpose `K0S_VERSION=v1.21.6+k0s.0 Select the version of k0s to be installed DEBUG=true Output commands and their arguments at execution. Note : If you require environment variables and use sudo, you can do: curl -sSLf https://get.k0s.sh | sudo K0S_VERSION = v1.21.6+k0s.0 sh","title":"1. Download k0s"},{"location":"k0s-multi-node/#2-bootstrap-a-controller-node","text":"Create a configuration file: k0s default-config > k0s.yaml Note : For information on settings modification, refer to the configuration documentation. sudo k0s install controller -c k0s.yaml sudo k0s start k0s process acts as a \"supervisor\" for all of the control plane components. In moments the control plane will be up and running.","title":"2. Bootstrap a controller node"},{"location":"k0s-multi-node/#3-create-a-join-token","text":"You need a token to join workers to the cluster. The token embeds information that enables mutual trust between the worker and controller(s) and which allows the node to join the cluster as worker. To get a token, run the following command on one of the existing controller nodes: k0s token create --role = worker The resulting output is a long token string, which you can use to add a worker to the cluster. For enhanced security, run the following command to set an expiration time for the token: k0s token create --role = worker --expiry = 100h > token-file","title":"3. Create a join token"},{"location":"k0s-multi-node/#4-add-workers-to-the-cluster","text":"To join the worker, run k0s in the worker mode with the join token you created: sudo k0s install worker --token-file /path/to/token/file sudo k0s start","title":"4. Add workers to the cluster"},{"location":"k0s-multi-node/#5-add-controllers-to-the-cluster","text":"Note : Either etcd or an external data store (MySQL or Postgres) via kine must be in use to add new controller nodes to the cluster. Pay strict attention to the high availability configuration and make sure the configuration is identical for all controller nodes. To create a join token for the new controller, run the following command on an existing controller: k0s token create --role = controller --expiry = 1h > token-file On the new controller, run: sudo k0s install controller --token-file /path/to/token/file k0s start","title":"5. Add controllers to the cluster"},{"location":"k0s-multi-node/#6-check-k0s-status","text":"To get general information about your k0s instance's status: $ sudo k0s status Version: v1.21.6+k0s.0 Process ID: 2769 Parent Process ID: 1 Role: controller Init System: linux-systemd Service file: /etc/systemd/system/k0scontroller.service","title":"6. Check k0s status"},{"location":"k0s-multi-node/#7-access-your-cluster","text":"Use the Kubernetes 'kubectl' command-line tool that comes with k0s binary to deploy your application or check your node status: $ sudo k0s kubectl get nodes NAME STATUS ROLES AGE VERSION k0s Ready <none> 4m6s v1.21.6-k0s1 You can also access your cluster easily with Lens , simply by copying the kubeconfig and pasting it to Lens: sudo cat /var/lib/k0s/pki/admin.conf Note : To access the cluster from an external network you must replace localhost in the kubeconfig with the host ip address for your controller.","title":"7. Access your cluster"},{"location":"k0s-multi-node/#next-steps","text":"Install using k0sctl : Deploy multi-node clusters using just one command Control plane configuration options : Networking and datastore configuration Worker node configuration options : Node labels and kubelet arguments Support for cloud providers : Load balancer or storage configuration Installing the Traefik Ingress Controller : Ingress deployment information","title":"Next Steps"},{"location":"k0s-single-node/","text":"See the Quick Start Guide .","title":"K0s single node"},{"location":"k0sctl-install/","text":"Install using k0sctl # k0sctl is a command-line tool for bootstrapping and managing k0s clusters. k0sctl connects to the provided hosts using SSH and gathers information on the hosts, with which it forms a cluster by configuring the hosts, deploying k0s, and then connecting the k0s nodes together. With k0sctl, you can create multi-node clusters in a manner that is automatic and easily repeatable. This method is recommended for production cluster installation. Note : The k0sctl install method is necessary for automatic upgrade. Prerequisites # You can execute k0sctl on any system that supports the Go language. Pre-compiled k0sctl binaries are availble on the k0sctl releases page ). Note : For target host prerequisites information, refer to the k0s System Requirements . Install k0s # 1. Install k0sctl tool # k0sctl is a single binary, the instructions for downloading and installing of which are available in the k0sctl github repository . 2. Configure the cluster # Run the following command to create a k0sctl configuration file: k0sctl init > k0sctl.yaml This action creates a k0sctl.yaml file in the current directory: apiVersion : k0sctl.k0sproject.io/v1beta1 kind : Cluster metadata : name : k0s-cluster spec : hosts : - role : controller ssh : address : 10.0.0.1 # replace with the controller's IP address user : root keyPath : ~/.ssh/id_rsa - role : worker ssh : address : 10.0.0.2 # replace with the worker's IP address user : root keyPath : ~/.ssh/id_rsa Provide each host with a valid IP address that is reachable by k0ctl, and the connection details for an SSH connection. Note : Refer to the k0sctl documentation for k0sctl configuration specifications. 3. Deploy the cluster # Run k0sctl apply to perform the cluster deployment: $ k0sctl apply --config k0sctl.yaml \u2800\u28ff\u28ff\u2847\u2800\u2800\u2880\u28f4\u28fe\u28ff\u281f\u2801\u28b8\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u287f\u281b\u2801\u2800\u28b8\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u2800\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588 \u2800\u28ff\u28ff\u2847\u28e0\u28f6\u28ff\u287f\u280b\u2800\u2800\u2800\u28b8\u28ff\u2847\u2800\u2800\u2800\u28e0\u2800\u2800\u2880\u28e0\u2846\u28b8\u28ff\u28ff\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2588\u2588\u2588 \u2588\u2588\u2588 \u2588\u2588\u2588 \u2800\u28ff\u28ff\u28ff\u28ff\u28df\u280b\u2800\u2800\u2800\u2800\u2800\u28b8\u28ff\u2847\u2800\u28b0\u28fe\u28ff\u2800\u2800\u28ff\u28ff\u2847\u28b8\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u2800\u2588\u2588\u2588 \u2588\u2588\u2588 \u2588\u2588\u2588 \u2800\u28ff\u28ff\u284f\u283b\u28ff\u28f7\u28e4\u2840\u2800\u2800\u2800\u2838\u281b\u2801\u2800\u2838\u280b\u2801\u2800\u2800\u28ff\u28ff\u2847\u2808\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u28b9\u28ff\u28ff\u2800\u2588\u2588\u2588 \u2588\u2588\u2588 \u2588\u2588\u2588 \u2800\u28ff\u28ff\u2847\u2800\u2800\u2819\u28bf\u28ff\u28e6\u28c0\u2800\u2800\u2800\u28e0\u28f6\u28f6\u28f6\u28f6\u28f6\u28f6\u28ff\u28ff\u2847\u28b0\u28f6\u28f6\u28f6\u28f6\u28f6\u28f6\u28f6\u28f6\u28fe\u28ff\u28ff\u2800\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 INFO k0sctl 0 .0.0 Copyright 2021 , Mirantis Inc. INFO Anonymized telemetry will be sent to Mirantis. INFO By continuing to use k0sctl you agree to these terms: INFO https://k0sproject.io/licenses/eula INFO == > Running phase: Connect to hosts INFO [ ssh ] 10 .0.0.1:22: connected INFO [ ssh ] 10 .0.0.2:22: connected INFO == > Running phase: Detect host operating systems INFO [ ssh ] 10 .0.0.1:22: is running Ubuntu 20 .10 INFO [ ssh ] 10 .0.0.2:22: is running Ubuntu 20 .10 INFO == > Running phase: Prepare hosts INFO [ ssh ] 10 .0.0.1:22: installing kubectl INFO == > Running phase: Gather host facts INFO [ ssh ] 10 .0.0.1:22: discovered 10 .12.18.133 as private address INFO == > Running phase: Validate hosts INFO == > Running phase: Gather k0s facts INFO == > Running phase: Download K0s on the hosts INFO [ ssh ] 10 .0.0.2:22: downloading k0s 0 .11.0 INFO [ ssh ] 10 .0.0.1:22: downloading k0s 0 .11.0 INFO == > Running phase: Configure K0s WARN [ ssh ] 10 .0.0.1:22: generating default configuration INFO [ ssh ] 10 .0.0.1:22: validating configuration INFO [ ssh ] 10 .0.0.1:22: configuration was changed INFO == > Running phase: Initialize K0s Cluster INFO [ ssh ] 10 .0.0.1:22: installing k0s controller INFO [ ssh ] 10 .0.0.1:22: waiting for the k0s service to start INFO [ ssh ] 10 .0.0.1:22: waiting for kubernetes api to respond INFO == > Running phase: Install workers INFO [ ssh ] 10 .0.0.1:22: generating token INFO [ ssh ] 10 .0.0.2:22: writing join token INFO [ ssh ] 10 .0.0.2:22: installing k0s worker INFO [ ssh ] 10 .0.0.2:22: starting service INFO [ ssh ] 10 .0.0.2:22: waiting for node to become ready INFO == > Running phase: Disconnect from hosts INFO == > Finished in 2m2s INFO k0s cluster version 0 .11.0 is now installed INFO Tip: To access the cluster you can now fetch the admin kubeconfig using: INFO k0sctl kubeconfig 4. Access the cluster # To access your k0s cluster, use k0sctl to generate a kubeconfig for the purpose. k0sctl kubeconfig > kubeconfig With the kubeconfig , you can access your cluster using either kubectl or Lens . $ kubectl get pods --kubeconfig kubeconfig -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-5f6546844f-w8x27 1 /1 Running 0 3m50s kube-system calico-node-vd7lx 1 /1 Running 0 3m44s kube-system coredns-5c98d7d4d8-tmrwv 1 /1 Running 0 4m10s kube-system konnectivity-agent-d9xv2 1 /1 Running 0 3m31s kube-system kube-proxy-xp9r9 1 /1 Running 0 4m4s kube-system metrics-server-6fbcd86f7b-5frtn 1 /1 Running 0 3m51s Known limitations # k0sctl does not perform any discovery of hosts, and thus it only operates on the hosts listed in the provided configuration. k0sctl can only add more nodes to the cluster. It cannot remove existing nodes. Next Steps # Control plane configuration options : Networking and datastore configuration Worker node configuration options : Node labels and kubelet arguments Support for cloud providers : Load balancer or storage configuration Installing the Traefik Ingress Controller : Ingress deployment information","title":"Install using k0sctl"},{"location":"k0sctl-install/#install-using-k0sctl","text":"k0sctl is a command-line tool for bootstrapping and managing k0s clusters. k0sctl connects to the provided hosts using SSH and gathers information on the hosts, with which it forms a cluster by configuring the hosts, deploying k0s, and then connecting the k0s nodes together. With k0sctl, you can create multi-node clusters in a manner that is automatic and easily repeatable. This method is recommended for production cluster installation. Note : The k0sctl install method is necessary for automatic upgrade.","title":"Install using k0sctl"},{"location":"k0sctl-install/#prerequisites","text":"You can execute k0sctl on any system that supports the Go language. Pre-compiled k0sctl binaries are availble on the k0sctl releases page ). Note : For target host prerequisites information, refer to the k0s System Requirements .","title":"Prerequisites"},{"location":"k0sctl-install/#install-k0s","text":"","title":"Install k0s"},{"location":"k0sctl-install/#1-install-k0sctl-tool","text":"k0sctl is a single binary, the instructions for downloading and installing of which are available in the k0sctl github repository .","title":"1. Install k0sctl tool"},{"location":"k0sctl-install/#2-configure-the-cluster","text":"Run the following command to create a k0sctl configuration file: k0sctl init > k0sctl.yaml This action creates a k0sctl.yaml file in the current directory: apiVersion : k0sctl.k0sproject.io/v1beta1 kind : Cluster metadata : name : k0s-cluster spec : hosts : - role : controller ssh : address : 10.0.0.1 # replace with the controller's IP address user : root keyPath : ~/.ssh/id_rsa - role : worker ssh : address : 10.0.0.2 # replace with the worker's IP address user : root keyPath : ~/.ssh/id_rsa Provide each host with a valid IP address that is reachable by k0ctl, and the connection details for an SSH connection. Note : Refer to the k0sctl documentation for k0sctl configuration specifications.","title":"2. Configure the cluster"},{"location":"k0sctl-install/#3-deploy-the-cluster","text":"Run k0sctl apply to perform the cluster deployment: $ k0sctl apply --config k0sctl.yaml \u2800\u28ff\u28ff\u2847\u2800\u2800\u2880\u28f4\u28fe\u28ff\u281f\u2801\u28b8\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u287f\u281b\u2801\u2800\u28b8\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u2800\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588 \u2800\u28ff\u28ff\u2847\u28e0\u28f6\u28ff\u287f\u280b\u2800\u2800\u2800\u28b8\u28ff\u2847\u2800\u2800\u2800\u28e0\u2800\u2800\u2880\u28e0\u2846\u28b8\u28ff\u28ff\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2800\u2588\u2588\u2588 \u2588\u2588\u2588 \u2588\u2588\u2588 \u2800\u28ff\u28ff\u28ff\u28ff\u28df\u280b\u2800\u2800\u2800\u2800\u2800\u28b8\u28ff\u2847\u2800\u28b0\u28fe\u28ff\u2800\u2800\u28ff\u28ff\u2847\u28b8\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u2800\u2588\u2588\u2588 \u2588\u2588\u2588 \u2588\u2588\u2588 \u2800\u28ff\u28ff\u284f\u283b\u28ff\u28f7\u28e4\u2840\u2800\u2800\u2800\u2838\u281b\u2801\u2800\u2838\u280b\u2801\u2800\u2800\u28ff\u28ff\u2847\u2808\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u2809\u28b9\u28ff\u28ff\u2800\u2588\u2588\u2588 \u2588\u2588\u2588 \u2588\u2588\u2588 \u2800\u28ff\u28ff\u2847\u2800\u2800\u2819\u28bf\u28ff\u28e6\u28c0\u2800\u2800\u2800\u28e0\u28f6\u28f6\u28f6\u28f6\u28f6\u28f6\u28ff\u28ff\u2847\u28b0\u28f6\u28f6\u28f6\u28f6\u28f6\u28f6\u28f6\u28f6\u28fe\u28ff\u28ff\u2800\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 INFO k0sctl 0 .0.0 Copyright 2021 , Mirantis Inc. INFO Anonymized telemetry will be sent to Mirantis. INFO By continuing to use k0sctl you agree to these terms: INFO https://k0sproject.io/licenses/eula INFO == > Running phase: Connect to hosts INFO [ ssh ] 10 .0.0.1:22: connected INFO [ ssh ] 10 .0.0.2:22: connected INFO == > Running phase: Detect host operating systems INFO [ ssh ] 10 .0.0.1:22: is running Ubuntu 20 .10 INFO [ ssh ] 10 .0.0.2:22: is running Ubuntu 20 .10 INFO == > Running phase: Prepare hosts INFO [ ssh ] 10 .0.0.1:22: installing kubectl INFO == > Running phase: Gather host facts INFO [ ssh ] 10 .0.0.1:22: discovered 10 .12.18.133 as private address INFO == > Running phase: Validate hosts INFO == > Running phase: Gather k0s facts INFO == > Running phase: Download K0s on the hosts INFO [ ssh ] 10 .0.0.2:22: downloading k0s 0 .11.0 INFO [ ssh ] 10 .0.0.1:22: downloading k0s 0 .11.0 INFO == > Running phase: Configure K0s WARN [ ssh ] 10 .0.0.1:22: generating default configuration INFO [ ssh ] 10 .0.0.1:22: validating configuration INFO [ ssh ] 10 .0.0.1:22: configuration was changed INFO == > Running phase: Initialize K0s Cluster INFO [ ssh ] 10 .0.0.1:22: installing k0s controller INFO [ ssh ] 10 .0.0.1:22: waiting for the k0s service to start INFO [ ssh ] 10 .0.0.1:22: waiting for kubernetes api to respond INFO == > Running phase: Install workers INFO [ ssh ] 10 .0.0.1:22: generating token INFO [ ssh ] 10 .0.0.2:22: writing join token INFO [ ssh ] 10 .0.0.2:22: installing k0s worker INFO [ ssh ] 10 .0.0.2:22: starting service INFO [ ssh ] 10 .0.0.2:22: waiting for node to become ready INFO == > Running phase: Disconnect from hosts INFO == > Finished in 2m2s INFO k0s cluster version 0 .11.0 is now installed INFO Tip: To access the cluster you can now fetch the admin kubeconfig using: INFO k0sctl kubeconfig","title":"3. Deploy the cluster"},{"location":"k0sctl-install/#4-access-the-cluster","text":"To access your k0s cluster, use k0sctl to generate a kubeconfig for the purpose. k0sctl kubeconfig > kubeconfig With the kubeconfig , you can access your cluster using either kubectl or Lens . $ kubectl get pods --kubeconfig kubeconfig -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-5f6546844f-w8x27 1 /1 Running 0 3m50s kube-system calico-node-vd7lx 1 /1 Running 0 3m44s kube-system coredns-5c98d7d4d8-tmrwv 1 /1 Running 0 4m10s kube-system konnectivity-agent-d9xv2 1 /1 Running 0 3m31s kube-system kube-proxy-xp9r9 1 /1 Running 0 4m4s kube-system metrics-server-6fbcd86f7b-5frtn 1 /1 Running 0 3m51s","title":"4. Access the cluster"},{"location":"k0sctl-install/#known-limitations","text":"k0sctl does not perform any discovery of hosts, and thus it only operates on the hosts listed in the provided configuration. k0sctl can only add more nodes to the cluster. It cannot remove existing nodes.","title":"Known limitations"},{"location":"k0sctl-install/#next-steps","text":"Control plane configuration options : Networking and datastore configuration Worker node configuration options : Node labels and kubelet arguments Support for cloud providers : Load balancer or storage configuration Installing the Traefik Ingress Controller : Ingress deployment information","title":"Next Steps"},{"location":"manifests/","text":"Manifest Deployer # Included with k0s, Manifest Deployer is one of two methods you can use to run k0s with your preferred extensions (the other being by defining your extensions as Helm charts . Overview # Manifest Deployer runs on the controller nodes and provides an easy way to automatically deploy manifests at runtime. By default, k0s reads all manifests under /var/lib/k0s/manifests and ensures that their state matches the cluster state. Moreover, on removal of a manifest file, k0s will automatically prune all of it associated resources. The use of Manifest Deployer is quite similar to the use the kubectl apply command. The main difference between the two is that Manifest Deployer constantly monitors the directory for changes, and thus you do not need to manually apply changes that are made to the manifest files. Note # Each directory that is a direct descendant of /var/lib/k0s/manifests is considered to be its own \"stack\". Nested directories (further subfolders), however, are excluded from the stack mechanism and thus are not automatically deployed by the Manifest Deployer. k0s uses the indepenent stack mechanism for some of its internal in-cluster components, as well as for other resources. Be sure to only touch the manifests that are not managed by k0s. Explicitly define the namespace in the manifests (Manifest Deployer does not have a default namespace). Example # To try Manifest Deployer, create a new folder under /var/lib/k0s/manifests and then create a manifest file (such as nginx.yaml ) with the following content: apiVersion : v1 kind : Namespace metadata : name : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment namespace : nginx spec : selector : matchLabels : app : nginx replicas : 3 template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:latest ports : - containerPort : 80 New pods will appear soon thereafter. $ sudo k0s kubectl get pods --namespace nginx NAME READY STATUS RESTARTS AGE nginx-deployment-66b6c48dd5-8zq7d 1 /1 Running 0 10m nginx-deployment-66b6c48dd5-br4jv 1 /1 Running 0 10m nginx-deployment-66b6c48dd5-sqvhb 1 /1 Running 0 10m","title":"Manifest Deployer"},{"location":"manifests/#manifest-deployer","text":"Included with k0s, Manifest Deployer is one of two methods you can use to run k0s with your preferred extensions (the other being by defining your extensions as Helm charts .","title":"Manifest Deployer"},{"location":"manifests/#overview","text":"Manifest Deployer runs on the controller nodes and provides an easy way to automatically deploy manifests at runtime. By default, k0s reads all manifests under /var/lib/k0s/manifests and ensures that their state matches the cluster state. Moreover, on removal of a manifest file, k0s will automatically prune all of it associated resources. The use of Manifest Deployer is quite similar to the use the kubectl apply command. The main difference between the two is that Manifest Deployer constantly monitors the directory for changes, and thus you do not need to manually apply changes that are made to the manifest files.","title":"Overview"},{"location":"manifests/#note","text":"Each directory that is a direct descendant of /var/lib/k0s/manifests is considered to be its own \"stack\". Nested directories (further subfolders), however, are excluded from the stack mechanism and thus are not automatically deployed by the Manifest Deployer. k0s uses the indepenent stack mechanism for some of its internal in-cluster components, as well as for other resources. Be sure to only touch the manifests that are not managed by k0s. Explicitly define the namespace in the manifests (Manifest Deployer does not have a default namespace).","title":"Note"},{"location":"manifests/#example","text":"To try Manifest Deployer, create a new folder under /var/lib/k0s/manifests and then create a manifest file (such as nginx.yaml ) with the following content: apiVersion : v1 kind : Namespace metadata : name : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment namespace : nginx spec : selector : matchLabels : app : nginx replicas : 3 template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:latest ports : - containerPort : 80 New pods will appear soon thereafter. $ sudo k0s kubectl get pods --namespace nginx NAME READY STATUS RESTARTS AGE nginx-deployment-66b6c48dd5-8zq7d 1 /1 Running 0 10m nginx-deployment-66b6c48dd5-br4jv 1 /1 Running 0 10m nginx-deployment-66b6c48dd5-sqvhb 1 /1 Running 0 10m","title":"Example"},{"location":"networking/","text":"Networking # In-cluster networking # k0s supports two Container Network Interface (CNI) providers out-of-box, Kube-router and Calico . In addition, k0s can support your own CNI configuration. Notes # When deploying k0s with the default settings, all pods on a node can communicate with all pods on all nodes. No configuration changes are needed to get started. Once you initialize the cluster with a network provider the only way to change providers is through a full cluster redeployment. Kube-router # Kube-router is built into k0s, and so by default the distribution uses it for network provision. Kube-router uses the standard Linux networking stack and toolset, and you can set up CNI networking without any overlays by using BGP as the main mechanism for in-cluster networking. Supports armv7 (among many other archs) Uses bit less resources (~15%) Does NOT support dual-stack (IPv4/IPv6) networking Does NOT support Windows nodes Calico # In addition to Kube-router, k0s also offers Calico as an alternative, built-in network provider. Calico is a layer 3 container networking solution that routes packets to pods. It supports, for example, pod-specific network policies that help to secure kubernetes clusters in demanding use cases. Calico uses the vxlan overlay network by default, and you can configure it to support ipip (IP-in-IP). Does NOT support armv7 Uses bit more resources Supports dual-stack (IPv4/IPv6) networking Supports Windows nodes Custom CNI configuration # You can opt-out of having k0s manage the network setup and choose instead to use any network plugin that adheres to the CNI specification. To do so, configure custom as the network provider in the k0s configurtion file ( k0s.yaml ). You can do this, for example, by pushing network provider manifests into /var/lib/k0s/manifests , from where k0s controllers will collect them for deployment into the cluster (for more information, refer to Manifest Deployer . Controller-Worker communication # One goal of k0s is to allow for the deployment of an isolated control plane, which may prevent the establishment of an IP route between controller nodes and the pod network. Thus, to enable this communication path (which is mandated by conformance tests), k0s deploys Konnectivity service to proxy traffic from the API server (control plane) into the worker nodes. This ensures that we can always fulfill all the Kubernetes API functionalities, but still operate the control plane in total isolation from the workers. Note : To allow Konnectivity agents running on the worker nodes to establish the connection, configure your firewalls for outbound access. Required ports and protocols # Protocol Port Service Direction Notes TCP 2380 etcd peers controller <-> controller TCP 6443 kube-apiserver Worker, CLI => controller Authenticated Kube API using Kube TLS client certs, ServiceAccount tokens with RBAC TCP 179 kube-router worker <-> worker BGP routing sessions between peers UDP 4789 Calico worker <-> worker Calico VXLAN overlay TCP 10250 kubelet Master, Worker => Host * Authenticated kubelet API for the master node kube-apiserver (and heapster / metrics-server addons) using TLS client certs TCP 9443 k0s-api controller <-> controller k0s controller join API, TLS with token auth TCP 8132,8133 konnectivity server worker <-> controller Konnectivity is used as \"reverse\" tunnel between kube-apiserver and worker kubelets","title":"Networking (CNI)"},{"location":"networking/#networking","text":"","title":"Networking"},{"location":"networking/#in-cluster-networking","text":"k0s supports two Container Network Interface (CNI) providers out-of-box, Kube-router and Calico . In addition, k0s can support your own CNI configuration.","title":"In-cluster networking"},{"location":"networking/#notes","text":"When deploying k0s with the default settings, all pods on a node can communicate with all pods on all nodes. No configuration changes are needed to get started. Once you initialize the cluster with a network provider the only way to change providers is through a full cluster redeployment.","title":"Notes"},{"location":"networking/#kube-router","text":"Kube-router is built into k0s, and so by default the distribution uses it for network provision. Kube-router uses the standard Linux networking stack and toolset, and you can set up CNI networking without any overlays by using BGP as the main mechanism for in-cluster networking. Supports armv7 (among many other archs) Uses bit less resources (~15%) Does NOT support dual-stack (IPv4/IPv6) networking Does NOT support Windows nodes","title":"Kube-router"},{"location":"networking/#calico","text":"In addition to Kube-router, k0s also offers Calico as an alternative, built-in network provider. Calico is a layer 3 container networking solution that routes packets to pods. It supports, for example, pod-specific network policies that help to secure kubernetes clusters in demanding use cases. Calico uses the vxlan overlay network by default, and you can configure it to support ipip (IP-in-IP). Does NOT support armv7 Uses bit more resources Supports dual-stack (IPv4/IPv6) networking Supports Windows nodes","title":"Calico"},{"location":"networking/#custom-cni-configuration","text":"You can opt-out of having k0s manage the network setup and choose instead to use any network plugin that adheres to the CNI specification. To do so, configure custom as the network provider in the k0s configurtion file ( k0s.yaml ). You can do this, for example, by pushing network provider manifests into /var/lib/k0s/manifests , from where k0s controllers will collect them for deployment into the cluster (for more information, refer to Manifest Deployer .","title":"Custom CNI configuration"},{"location":"networking/#controller-worker-communication","text":"One goal of k0s is to allow for the deployment of an isolated control plane, which may prevent the establishment of an IP route between controller nodes and the pod network. Thus, to enable this communication path (which is mandated by conformance tests), k0s deploys Konnectivity service to proxy traffic from the API server (control plane) into the worker nodes. This ensures that we can always fulfill all the Kubernetes API functionalities, but still operate the control plane in total isolation from the workers. Note : To allow Konnectivity agents running on the worker nodes to establish the connection, configure your firewalls for outbound access.","title":"Controller-Worker communication"},{"location":"networking/#required-ports-and-protocols","text":"Protocol Port Service Direction Notes TCP 2380 etcd peers controller <-> controller TCP 6443 kube-apiserver Worker, CLI => controller Authenticated Kube API using Kube TLS client certs, ServiceAccount tokens with RBAC TCP 179 kube-router worker <-> worker BGP routing sessions between peers UDP 4789 Calico worker <-> worker Calico VXLAN overlay TCP 10250 kubelet Master, Worker => Host * Authenticated kubelet API for the master node kube-apiserver (and heapster / metrics-server addons) using TLS client certs TCP 9443 k0s-api controller <-> controller k0s controller join API, TLS with token auth TCP 8132,8133 konnectivity server worker <-> controller Konnectivity is used as \"reverse\" tunnel between kube-apiserver and worker kubelets","title":"Required ports and protocols"},{"location":"raspberry-pi4/","text":"Create a Raspberry Pi 4 Cluster # You can deploy the k0s distribution of Kubernetes to a cluster comprised of Raspberry Pi 4 Computers with Ubuntu 20.04 LTS as the operating system. Prerequisites # Install the following tools on your local system: Kubectl v1.19.4 + Raspberry Pi Imager v1.5 + Raspberry Pi 4 Model B Computers with 8GB of RAM and 64GB SD Cards. Note: If you use lower spec Raspberry Pi machines, it may be necessary to manually edit the example code and k0s configuration. Install k0s # Set up hardware and operating system # Download and install the Ubuntu Server 20.04.1 LTS RASPI 4 Image . Note : In addition to the documentation Ubuntu provides documentation on installing Ubuntu on Raspberry Pi Computers , the Raspberry Pi Foundation offers an imaging tool that you can use to write the Ubuntu image to your noe SD cards. Once cloud-init finishes bootstrapping the system, the default login credentials are set to user ubuntu with password ubuntu (which you will be prompted to change on first login). Network configurations # Note : For network configurtion purposes, this documentation assumes that all of your computers are connected on the same subnet. Review the k0s required ports documentation to ensure that your network and firewall configurations allow necessary traffic for the cluster. Review the Ubuntu Server Networking Configuration Documentation to ensure that all systems have a static IP address on the network, or that the network is providing a static DHCP lease for the nodes. OpenSSH # Ubuntu Server deploys and enables OpenSSH by default. Confirm, though, that for whichever user you will deploy the cluster with on the build system, their SSH Key is copied to each node's root user . Before you start, the configuration should be such that the current user can run: ssh root@ ${ HOST } Where ${HOST} is any node and the login can succeed with no further prompts. Set up Nodes # Every node (whether control plane or not) requires additional configuration in preparation for k0s deployment. CGroup Configuration # Ensure that the following packages are installed on each node: apt-get install cgroup-lite cgroup-tools cgroupfs-mount Enable the memory cgroup in the Kernel by adding it to the Kernel command line. Open the file /boot/firmware/cmdline.txt (responsible for managing the Kernel parameters), and confirm that the following parameters exist (and add them as necessary): cgroup_enable = cpuset cgroup_enable = memory cgroup_memory = 1 Be sure to reboot each node to ensure the memory cgroup is loaded. Swap (Optional) # While swap is technically optional , enable it to ease memory pressure. To create a swapfile: fallocate -l 2G /swapfile && \\ chmod 0600 /swapfile && \\ mkswap /swapfile && \\ swapon -a Ensure that the usage of swap is not too agressive by setting the sudo sysctl vm.swappiness=10 (the default is generally higher) and configuring it to be persistent in /etc/sysctl.d/* . Ensure that your swap is mounted after reboots by confirming that the following line exists in your /etc/fstab configuration: /swapfile none swap sw 0 0 Kernel Modules # Ensure the loading of the overlay , nf_conntrack and br_netfilter modules: modprobe overlay modprobe nf_conntrack modprobe br_netfilter In addition, add each of these modules to your /etc/modules-load.d/modules.conf file to ensure they persist following reboot. Download k0s # Download a k0s release . For example: wget -O /tmp/k0s https://github.com/k0sproject/k0s/releases/download/v0.9.1/k0s-v0.9.1-arm64 # replace version number! sudo install /tmp/k0s /usr/local/bin/k0s -- or -- Use the k0s download script (as one command) to download the latest stable k0s and make it executable in /usr/bin/k0s . curl -sSLf https://get.k0s.sh | sudo sh At this point you can run k0s : $ k0s version v0.9.1 Deploy Kubernetes # Each node can now serve as a control plane node or worker node. Control Plane Node # Use a non-ha control plane with a single node. Systemd Service (controller) # Create a systemd service: sudo k0s install controller Start the service: sudo k0s start Run sudo k0s status or systemctl status k0scontroller to verify the service status. Worker Tokens # For each worker node that you expect to have, create a join token: k0s token create --role worker Save the join token for subsequent steps. Worker # You must deploy and start a worker service for each worker nodes for which you created join tokens. Systemd Service (worker) # Create the join token file for the worker (where TOKEN_CONTENT is one of the join tokens created in the control plane setup): mkdir -p /var/lib/k0s/ echo TOKEN_CONTENT > /var/lib/k0s/join-token Deploy the systemd service for the worker: sudo k0s install worker --token-file /var/lib/k0s/join-token Start the service: sudo k0s start Run sudo k0s status or systemctl status k0sworker to verify the service status. Connect To Your Cluster # Generate a kubeconfig for the cluster and begin managing it with kubectl (where CONTROL_PLANE_NODE is the control plane node address): ssh root@CONTROL_PLANE_NODE k0s kubeconfig create --groups \"system:masters\" k0s > config.yaml export KUBECONFIG = $( pwd ) /config.yaml kubectl create clusterrolebinding k0s-admin-binding --clusterrole = admin --user = k0s You can now access and use the cluster: $ kubectl get nodes,deployments,pods -A NAME STATUS ROLES AGE VERSION node/k8s-4 Ready <none> 5m9s v1.20.1-k0s1 node/k8s-5 Ready <none> 5m v1.20.1-k0s1 node/k8s-6 Ready <none> 4m45s v1.20.1-k0s1 NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE kube-system deployment.apps/calico-kube-controllers 1 /1 1 1 12m kube-system deployment.apps/coredns 1 /1 1 1 12m NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/calico-kube-controllers-5f6546844f-rjdkz 1 /1 Running 0 12m kube-system pod/calico-node-j475n 1 /1 Running 0 5m9s kube-system pod/calico-node-lnfrf 1 /1 Running 0 4m45s kube-system pod/calico-node-pzp7x 1 /1 Running 0 5m kube-system pod/coredns-5c98d7d4d8-bg9pl 1 /1 Running 0 12m kube-system pod/konnectivity-agent-548hp 1 /1 Running 0 4m45s kube-system pod/konnectivity-agent-66cr8 1 /1 Running 0 4m49s kube-system pod/konnectivity-agent-lxt9z 1 /1 Running 0 4m58s kube-system pod/kube-proxy-ct6bg 1 /1 Running 0 5m kube-system pod/kube-proxy-hg8t2 1 /1 Running 0 4m45s kube-system pod/kube-proxy-vghs9 1 /1 Running 0 5m9s","title":"Raspberry Pi 4"},{"location":"raspberry-pi4/#create-a-raspberry-pi-4-cluster","text":"You can deploy the k0s distribution of Kubernetes to a cluster comprised of Raspberry Pi 4 Computers with Ubuntu 20.04 LTS as the operating system.","title":"Create a Raspberry Pi 4 Cluster"},{"location":"raspberry-pi4/#prerequisites","text":"Install the following tools on your local system: Kubectl v1.19.4 + Raspberry Pi Imager v1.5 + Raspberry Pi 4 Model B Computers with 8GB of RAM and 64GB SD Cards. Note: If you use lower spec Raspberry Pi machines, it may be necessary to manually edit the example code and k0s configuration.","title":"Prerequisites"},{"location":"raspberry-pi4/#install-k0s","text":"","title":"Install k0s"},{"location":"raspberry-pi4/#set-up-hardware-and-operating-system","text":"Download and install the Ubuntu Server 20.04.1 LTS RASPI 4 Image . Note : In addition to the documentation Ubuntu provides documentation on installing Ubuntu on Raspberry Pi Computers , the Raspberry Pi Foundation offers an imaging tool that you can use to write the Ubuntu image to your noe SD cards. Once cloud-init finishes bootstrapping the system, the default login credentials are set to user ubuntu with password ubuntu (which you will be prompted to change on first login).","title":"Set up hardware and operating system"},{"location":"raspberry-pi4/#network-configurations","text":"Note : For network configurtion purposes, this documentation assumes that all of your computers are connected on the same subnet. Review the k0s required ports documentation to ensure that your network and firewall configurations allow necessary traffic for the cluster. Review the Ubuntu Server Networking Configuration Documentation to ensure that all systems have a static IP address on the network, or that the network is providing a static DHCP lease for the nodes.","title":"Network configurations"},{"location":"raspberry-pi4/#set-up-nodes","text":"Every node (whether control plane or not) requires additional configuration in preparation for k0s deployment.","title":"Set up Nodes"},{"location":"raspberry-pi4/#deploy-kubernetes","text":"Each node can now serve as a control plane node or worker node.","title":"Deploy Kubernetes"},{"location":"raspberry-pi4/#connect-to-your-cluster","text":"Generate a kubeconfig for the cluster and begin managing it with kubectl (where CONTROL_PLANE_NODE is the control plane node address): ssh root@CONTROL_PLANE_NODE k0s kubeconfig create --groups \"system:masters\" k0s > config.yaml export KUBECONFIG = $( pwd ) /config.yaml kubectl create clusterrolebinding k0s-admin-binding --clusterrole = admin --user = k0s You can now access and use the cluster: $ kubectl get nodes,deployments,pods -A NAME STATUS ROLES AGE VERSION node/k8s-4 Ready <none> 5m9s v1.20.1-k0s1 node/k8s-5 Ready <none> 5m v1.20.1-k0s1 node/k8s-6 Ready <none> 4m45s v1.20.1-k0s1 NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE kube-system deployment.apps/calico-kube-controllers 1 /1 1 1 12m kube-system deployment.apps/coredns 1 /1 1 1 12m NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/calico-kube-controllers-5f6546844f-rjdkz 1 /1 Running 0 12m kube-system pod/calico-node-j475n 1 /1 Running 0 5m9s kube-system pod/calico-node-lnfrf 1 /1 Running 0 4m45s kube-system pod/calico-node-pzp7x 1 /1 Running 0 5m kube-system pod/coredns-5c98d7d4d8-bg9pl 1 /1 Running 0 12m kube-system pod/konnectivity-agent-548hp 1 /1 Running 0 4m45s kube-system pod/konnectivity-agent-66cr8 1 /1 Running 0 4m49s kube-system pod/konnectivity-agent-lxt9z 1 /1 Running 0 4m58s kube-system pod/kube-proxy-ct6bg 1 /1 Running 0 5m kube-system pod/kube-proxy-hg8t2 1 /1 Running 0 4m45s kube-system pod/kube-proxy-vghs9 1 /1 Running 0 5m9s","title":"Connect To Your Cluster"},{"location":"reset/","text":"Uninstall/Reset # k0s can be uninstalled locally with k0s reset command and remotely with k0sctl reset command. They remove all k0s-related files from the host. reset operates under the assumption that k0s is installed as a service on the host. Uninstall a k0s node locally # To prevent accidental triggering, k0s reset will not run if the k0s service is running, so you must first stop the service: Stop the service: sudo k0s stop Invoke the reset command: $ sudo k0s reset INFO [ 2021 -06-29 13 :08:39 ] * containers steps INFO [ 2021 -06-29 13 :08:44 ] successfully removed k0s containers! INFO [ 2021 -06-29 13 :08:44 ] no config file given, using defaults INFO [ 2021 -06-29 13 :08:44 ] * remove k0s users step: INFO [ 2021 -06-29 13 :08:44 ] no config file given, using defaults INFO [ 2021 -06-29 13 :08:44 ] * uninstall service step INFO [ 2021 -06-29 13 :08:44 ] Uninstalling the k0s service INFO [ 2021 -06-29 13 :08:45 ] * remove directories step INFO [ 2021 -06-29 13 :08:45 ] * CNI leftovers cleanup step INFO k0s cleanup operations done . To ensure a full reset, a node reboot is recommended. Uninstall a k0s cluster using k0sctl # k0sctl can be used to connect each node and remove all k0s-related files and processes from the hosts. Invoke k0sctl reset command: $ k0sctl reset --config k0sctl.yaml k0sctl v0.9.0 Copyright 2021 , k0sctl authors. ? Going to reset all of the hosts, which will destroy all configuration and data, Are you sure? Yes INFO == > Running phase: Connect to hosts INFO [ ssh ] 13 .53.43.63:22: connected INFO [ ssh ] 13 .53.218.149:22: connected INFO == > Running phase: Detect host operating systems INFO [ ssh ] 13 .53.43.63:22: is running Ubuntu 20 .04.2 LTS INFO [ ssh ] 13 .53.218.149:22: is running Ubuntu 20 .04.2 LTS INFO == > Running phase: Prepare hosts INFO == > Running phase: Gather k0s facts INFO [ ssh ] 13 .53.43.63:22: found existing configuration INFO [ ssh ] 13 .53.43.63:22: is running k0s controller version 1 .21.6+k0s.0 INFO [ ssh ] 13 .53.218.149:22: is running k0s worker version 1 .21.6+k0s.0 INFO [ ssh ] 13 .53.43.63:22: checking if worker has joined INFO == > Running phase: Reset hosts INFO [ ssh ] 13 .53.43.63:22: stopping k0s INFO [ ssh ] 13 .53.218.149:22: stopping k0s INFO [ ssh ] 13 .53.218.149:22: running k0s reset INFO [ ssh ] 13 .53.43.63:22: running k0s reset INFO == > Running phase: Disconnect from hosts INFO == > Finished in 8s","title":"Uninstall/Reset"},{"location":"reset/#uninstallreset","text":"k0s can be uninstalled locally with k0s reset command and remotely with k0sctl reset command. They remove all k0s-related files from the host. reset operates under the assumption that k0s is installed as a service on the host.","title":"Uninstall/Reset"},{"location":"reset/#uninstall-a-k0s-node-locally","text":"To prevent accidental triggering, k0s reset will not run if the k0s service is running, so you must first stop the service: Stop the service: sudo k0s stop Invoke the reset command: $ sudo k0s reset INFO [ 2021 -06-29 13 :08:39 ] * containers steps INFO [ 2021 -06-29 13 :08:44 ] successfully removed k0s containers! INFO [ 2021 -06-29 13 :08:44 ] no config file given, using defaults INFO [ 2021 -06-29 13 :08:44 ] * remove k0s users step: INFO [ 2021 -06-29 13 :08:44 ] no config file given, using defaults INFO [ 2021 -06-29 13 :08:44 ] * uninstall service step INFO [ 2021 -06-29 13 :08:44 ] Uninstalling the k0s service INFO [ 2021 -06-29 13 :08:45 ] * remove directories step INFO [ 2021 -06-29 13 :08:45 ] * CNI leftovers cleanup step INFO k0s cleanup operations done . To ensure a full reset, a node reboot is recommended.","title":"Uninstall a k0s node locally"},{"location":"reset/#uninstall-a-k0s-cluster-using-k0sctl","text":"k0sctl can be used to connect each node and remove all k0s-related files and processes from the hosts. Invoke k0sctl reset command: $ k0sctl reset --config k0sctl.yaml k0sctl v0.9.0 Copyright 2021 , k0sctl authors. ? Going to reset all of the hosts, which will destroy all configuration and data, Are you sure? Yes INFO == > Running phase: Connect to hosts INFO [ ssh ] 13 .53.43.63:22: connected INFO [ ssh ] 13 .53.218.149:22: connected INFO == > Running phase: Detect host operating systems INFO [ ssh ] 13 .53.43.63:22: is running Ubuntu 20 .04.2 LTS INFO [ ssh ] 13 .53.218.149:22: is running Ubuntu 20 .04.2 LTS INFO == > Running phase: Prepare hosts INFO == > Running phase: Gather k0s facts INFO [ ssh ] 13 .53.43.63:22: found existing configuration INFO [ ssh ] 13 .53.43.63:22: is running k0s controller version 1 .21.6+k0s.0 INFO [ ssh ] 13 .53.218.149:22: is running k0s worker version 1 .21.6+k0s.0 INFO [ ssh ] 13 .53.43.63:22: checking if worker has joined INFO == > Running phase: Reset hosts INFO [ ssh ] 13 .53.43.63:22: stopping k0s INFO [ ssh ] 13 .53.218.149:22: stopping k0s INFO [ ssh ] 13 .53.218.149:22: running k0s reset INFO [ ssh ] 13 .53.43.63:22: running k0s reset INFO == > Running phase: Disconnect from hosts INFO == > Finished in 8s","title":"Uninstall a k0s cluster using k0sctl"},{"location":"runtime/","text":"Runtime # k0s uses containerd as the default Container Runtime Interface (CRI) and runc as the default low-level runtime. In most cases they don't require any configuration changes. However, if custom configuration is needed, this page provides some examples. containerd configuration # To make changes to containerd configuration you must first generate a default containerd configuration, with the default values set to /etc/k0s/containerd.toml : containerd config default > /etc/k0s/containerd.toml k0s runs containerd with the following default values: /var/lib/k0s/bin/containerd \\ --root = /var/lib/k0s/containerd \\ --state = /var/lib/k0s/run/containerd \\ --address = /var/lib/k0s/run/containerd.sock \\ --config = /etc/k0s/containerd.toml Next, add the following default values to the configuration file: version = 2 root = \"/var/lib/k0s/containerd\" state = \"/var/lib/k0s/run/containerd\" ... [grpc] address = \"/var/lib/k0s/run/containerd.sock\" Finally, if you want to change CRI look into: [plugins.\"io.containerd.runtime.v1.linux\"] shim = \"containerd-shim\" runtime = \"runc\" Using gVisor # gVisor is an application kernel, written in Go, that implements a substantial portion of the Linux system call interface. It provides an additional layer of isolation between running applications and the host operating system. Install the needed gVisor binaries into the host. ( set -e URL = https://storage.googleapis.com/gvisor/releases/release/latest wget ${ URL } /runsc ${ URL } /runsc.sha512 \\ ${ URL } /gvisor-containerd-shim ${ URL } /gvisor-containerd-shim.sha512 \\ ${ URL } /containerd-shim-runsc-v1 ${ URL } /containerd-shim-runsc-v1.sha512 sha512sum -c runsc.sha512 \\ -c gvisor-containerd-shim.sha512 \\ -c containerd-shim-runsc-v1.sha512 rm -f *.sha512 chmod a+rx runsc gvisor-containerd-shim containerd-shim-runsc-v1 sudo mv runsc gvisor-containerd-shim containerd-shim-runsc-v1 /usr/local/bin ) Refer to the gVisor install docs for more information. Prepare the config for k0s managed containerD, to utilize gVisor as additional runtime: cat <<EOF | sudo tee /etc/k0s/containerd.toml disabled_plugins = [\"restart\"] [plugins.linux] shim_debug = true [plugins.cri.containerd.runtimes.runsc] runtime_type = \"io.containerd.runsc.v1\" EOF Start and join the worker into the cluster, as normal: k0s worker $token Register containerd to the Kubernetes side to make gVisor runtime usable for workloads (by default, containerd uses normal runc as the runtime): cat <<EOF | kubectl apply -f - apiVersion: node.k8s.io/v1beta1 kind: RuntimeClass metadata: name: gvisor handler: runsc EOF At this point, you can use gVisor runtime for your workloads: apiVersion : v1 kind : Pod metadata : name : nginx-gvisor spec : runtimeClassName : gvisor containers : - name : nginx image : nginx (Optional) Verify tht the created nginx pod is running under gVisor runtime: # kubectl exec nginx-gvisor -- dmesg | grep -i gvisor [ 0 .000000 ] Starting gVisor... Using nvidia-container-runtime # By default, CRI is set to runC. As such, you must configure Nvidia GPU support by replacing runc with nvidia-container-runtime : [plugins.\"io.containerd.runtime.v1.linux\"] shim = \"containerd-shim\" runtime = \"nvidia-container-runtime\" Note Detailed instruction on how to run nvidia-container-runtime on your node is available here . After editing the configuration, restart k0s to get containerd using the newly configured runtime. Using custom CRI runtime # Warning : You can use your own CRI runtime with k0s (for example, docker ). However, k0s will not start or manage the runtime, and configuration is solely your responsibility. Use the option --cri-socket to run a k0s worker with a custom CRI runtime. the option takes input in the form of <type>:<socket_path> (for type , use docker for a pure Docker setup and remote for anything else). To run k0s with a pre-existing Docker setup, run the worker with k0s worker --cri-socket docker:unix:///var/run/docker.sock <token> . When docker is used as a runtime, k0s configures kubelet to create the dockershim socket at /var/run/dockershim.sock .","title":"Runtime (CRI)"},{"location":"runtime/#runtime","text":"k0s uses containerd as the default Container Runtime Interface (CRI) and runc as the default low-level runtime. In most cases they don't require any configuration changes. However, if custom configuration is needed, this page provides some examples.","title":"Runtime"},{"location":"runtime/#containerd-configuration","text":"To make changes to containerd configuration you must first generate a default containerd configuration, with the default values set to /etc/k0s/containerd.toml : containerd config default > /etc/k0s/containerd.toml k0s runs containerd with the following default values: /var/lib/k0s/bin/containerd \\ --root = /var/lib/k0s/containerd \\ --state = /var/lib/k0s/run/containerd \\ --address = /var/lib/k0s/run/containerd.sock \\ --config = /etc/k0s/containerd.toml Next, add the following default values to the configuration file: version = 2 root = \"/var/lib/k0s/containerd\" state = \"/var/lib/k0s/run/containerd\" ... [grpc] address = \"/var/lib/k0s/run/containerd.sock\" Finally, if you want to change CRI look into: [plugins.\"io.containerd.runtime.v1.linux\"] shim = \"containerd-shim\" runtime = \"runc\"","title":"containerd configuration"},{"location":"runtime/#using-gvisor","text":"gVisor is an application kernel, written in Go, that implements a substantial portion of the Linux system call interface. It provides an additional layer of isolation between running applications and the host operating system. Install the needed gVisor binaries into the host. ( set -e URL = https://storage.googleapis.com/gvisor/releases/release/latest wget ${ URL } /runsc ${ URL } /runsc.sha512 \\ ${ URL } /gvisor-containerd-shim ${ URL } /gvisor-containerd-shim.sha512 \\ ${ URL } /containerd-shim-runsc-v1 ${ URL } /containerd-shim-runsc-v1.sha512 sha512sum -c runsc.sha512 \\ -c gvisor-containerd-shim.sha512 \\ -c containerd-shim-runsc-v1.sha512 rm -f *.sha512 chmod a+rx runsc gvisor-containerd-shim containerd-shim-runsc-v1 sudo mv runsc gvisor-containerd-shim containerd-shim-runsc-v1 /usr/local/bin ) Refer to the gVisor install docs for more information. Prepare the config for k0s managed containerD, to utilize gVisor as additional runtime: cat <<EOF | sudo tee /etc/k0s/containerd.toml disabled_plugins = [\"restart\"] [plugins.linux] shim_debug = true [plugins.cri.containerd.runtimes.runsc] runtime_type = \"io.containerd.runsc.v1\" EOF Start and join the worker into the cluster, as normal: k0s worker $token Register containerd to the Kubernetes side to make gVisor runtime usable for workloads (by default, containerd uses normal runc as the runtime): cat <<EOF | kubectl apply -f - apiVersion: node.k8s.io/v1beta1 kind: RuntimeClass metadata: name: gvisor handler: runsc EOF At this point, you can use gVisor runtime for your workloads: apiVersion : v1 kind : Pod metadata : name : nginx-gvisor spec : runtimeClassName : gvisor containers : - name : nginx image : nginx (Optional) Verify tht the created nginx pod is running under gVisor runtime: # kubectl exec nginx-gvisor -- dmesg | grep -i gvisor [ 0 .000000 ] Starting gVisor...","title":"Using gVisor"},{"location":"runtime/#using-nvidia-container-runtime","text":"By default, CRI is set to runC. As such, you must configure Nvidia GPU support by replacing runc with nvidia-container-runtime : [plugins.\"io.containerd.runtime.v1.linux\"] shim = \"containerd-shim\" runtime = \"nvidia-container-runtime\" Note Detailed instruction on how to run nvidia-container-runtime on your node is available here . After editing the configuration, restart k0s to get containerd using the newly configured runtime.","title":"Using nvidia-container-runtime"},{"location":"runtime/#using-custom-cri-runtime","text":"Warning : You can use your own CRI runtime with k0s (for example, docker ). However, k0s will not start or manage the runtime, and configuration is solely your responsibility. Use the option --cri-socket to run a k0s worker with a custom CRI runtime. the option takes input in the form of <type>:<socket_path> (for type , use docker for a pure Docker setup and remote for anything else). To run k0s with a pre-existing Docker setup, run the worker with k0s worker --cri-socket docker:unix:///var/run/docker.sock <token> . When docker is used as a runtime, k0s configures kubelet to create the dockershim socket at /var/run/dockershim.sock .","title":"Using custom CRI runtime"},{"location":"shell-completion/","text":"Enabling Shell Completion # Generate the k0s completion script using the k0s completion <shell_name> command, for Bash, Zsh, fish, or PowerShell. Sourcing the completion script in your shell enables k0s autocompletion. Bash # echo 'source <(k0s completion bash)' >>~/.bashrc To load completions for each session, execute once: k0s completion bash > /etc/bash_completion.d/k0s Zsh # If shell completion is not already enabled in Zsh environment you will need to enable it: echo \"autoload -U compinit; compinit\" >> ~/.zshrc To load completions for each session, execute once: k0s completion zsh > \" ${ fpath [1] } /_k0s\" Note : You must start a new shell for the setup to take effect. Fish # k0s completion fish | source To load completions for each session, execute once: k0s completion fish > ~/.config/fish/completions/k0s.fish","title":"Shell Completion"},{"location":"shell-completion/#enabling-shell-completion","text":"Generate the k0s completion script using the k0s completion <shell_name> command, for Bash, Zsh, fish, or PowerShell. Sourcing the completion script in your shell enables k0s autocompletion.","title":"Enabling Shell Completion"},{"location":"shell-completion/#bash","text":"echo 'source <(k0s completion bash)' >>~/.bashrc To load completions for each session, execute once: k0s completion bash > /etc/bash_completion.d/k0s","title":"Bash"},{"location":"shell-completion/#zsh","text":"If shell completion is not already enabled in Zsh environment you will need to enable it: echo \"autoload -U compinit; compinit\" >> ~/.zshrc To load completions for each session, execute once: k0s completion zsh > \" ${ fpath [1] } /_k0s\" Note : You must start a new shell for the setup to take effect.","title":"Zsh"},{"location":"shell-completion/#fish","text":"k0s completion fish | source To load completions for each session, execute once: k0s completion fish > ~/.config/fish/completions/k0s.fish","title":"Fish"},{"location":"storage/","text":"Storage (CSI) # k0s supports a wide range of different storage options. There are no \"selected\" storage in k0s. Instead, all Kubernetes storage solutions are supported and users can easily select the storage that fits best for their needs. When the storage solution implements Container Storage Interface (CSI), containers can communicate with the storage for creation and configuration of persistent volumes. This makes it easy to dynamically provision the requested volumes. It also expands the supported storage solutions from the previous generation, in-tree volume plugins. More information about the CSI concept is described on the Kubernetes Blog . Example storage solutions # Different Kubernetes storage solutions are explained in the official Kubernetes storage documentation . All of them can be used with k0s. Here are some popular ones: Rook-Ceph (Open Source) OpenEBS (Open Source) MinIO (Open Source) Gluster (Open Source) Longhorn (Open Source) Amazon EBS Google Persistent Disk Azure Disk Portworx If you are looking for a fault-tolerant storage with data replication, you can find a k0s tutorial for configuring Ceph storage with Rook in here . If you are looking for a bit more simple solution and use a folder from the node local disk, you can take a look at OpenEBS . With OpenEBS, you can either create a simple local storage or a highly available distributed storage.","title":"Storage (CSI)"},{"location":"storage/#storage-csi","text":"k0s supports a wide range of different storage options. There are no \"selected\" storage in k0s. Instead, all Kubernetes storage solutions are supported and users can easily select the storage that fits best for their needs. When the storage solution implements Container Storage Interface (CSI), containers can communicate with the storage for creation and configuration of persistent volumes. This makes it easy to dynamically provision the requested volumes. It also expands the supported storage solutions from the previous generation, in-tree volume plugins. More information about the CSI concept is described on the Kubernetes Blog .","title":"Storage (CSI)"},{"location":"storage/#example-storage-solutions","text":"Different Kubernetes storage solutions are explained in the official Kubernetes storage documentation . All of them can be used with k0s. Here are some popular ones: Rook-Ceph (Open Source) OpenEBS (Open Source) MinIO (Open Source) Gluster (Open Source) Longhorn (Open Source) Amazon EBS Google Persistent Disk Azure Disk Portworx If you are looking for a fault-tolerant storage with data replication, you can find a k0s tutorial for configuring Ceph storage with Rook in here . If you are looking for a bit more simple solution and use a folder from the node local disk, you can take a look at OpenEBS . With OpenEBS, you can either create a simple local storage or a highly available distributed storage.","title":"Example storage solutions"},{"location":"system-requirements/","text":"System requirements # Verify that your environment meets the system requirements for k0s. Hardware # The minimum hardware requirements for k0s detailed below are approximations and thus results may vary. Role Virtual CPU (vCPU) Memory (RAM) Controller node 1 vCPU (2 recommended) 1 GB (2 recommended) Worker node 1 vCPU (2 recommended) 0.5 GB (1 recommended) Controller + worker 1 vCPU (2 recommended) 1 GB (2 recommended) Note : Use an SSD for optimal storage performance (cluster latency and throughput are sensitive to storage). The specific storage consumption for k0s is as follows: Role Storage (k0s part) Controller node ~0.5 GB Worker node ~1.3 GB Controller + worker ~1.7 GB Note : The operating system and application requirements must be considered in addition to the k0s part. Host operating system # Linux (kernel v3.10 or later) Windows Server 2019 Architecture # x86-64 ARM64 ARMv7 Networking # For information on the ports that k0s needs to function, refer to networking .","title":"System Requirements"},{"location":"system-requirements/#system-requirements","text":"Verify that your environment meets the system requirements for k0s.","title":"System requirements"},{"location":"system-requirements/#hardware","text":"The minimum hardware requirements for k0s detailed below are approximations and thus results may vary. Role Virtual CPU (vCPU) Memory (RAM) Controller node 1 vCPU (2 recommended) 1 GB (2 recommended) Worker node 1 vCPU (2 recommended) 0.5 GB (1 recommended) Controller + worker 1 vCPU (2 recommended) 1 GB (2 recommended) Note : Use an SSD for optimal storage performance (cluster latency and throughput are sensitive to storage). The specific storage consumption for k0s is as follows: Role Storage (k0s part) Controller node ~0.5 GB Worker node ~1.3 GB Controller + worker ~1.7 GB Note : The operating system and application requirements must be considered in addition to the k0s part.","title":"Hardware"},{"location":"system-requirements/#host-operating-system","text":"Linux (kernel v3.10 or later) Windows Server 2019","title":"Host operating system"},{"location":"system-requirements/#architecture","text":"x86-64 ARM64 ARMv7","title":"Architecture"},{"location":"system-requirements/#networking","text":"For information on the ports that k0s needs to function, refer to networking .","title":"Networking"},{"location":"troubleshooting/","text":"Common Pitfalls # There are few common cases we've seen where k0s fails to run properly. CoreDNS in crashloop # The most common case we've encountered so far has been CoreDNS getting into crashloop on the node(s). With kubectl you see something like this: $ kubectl get pod --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-5f6546844f-25px6 1 /1 Running 0 167m kube-system calico-node-fwjx5 1 /1 Running 0 164m kube-system calico-node-t4tx5 1 /1 Running 0 164m kube-system calico-node-whwsg 1 /1 Running 0 164m kube-system coredns-5c98d7d4d8-tfs4q 1 /1 Error 17 167m kube-system konnectivity-agent-9jkfd 1 /1 Running 0 164m kube-system konnectivity-agent-bvhdb 1 /1 Running 0 164m kube-system konnectivity-agent-r6mzj 1 /1 Running 0 164m kube-system kube-proxy-kr2r9 1 /1 Running 0 164m kube-system kube-proxy-tbljr 1 /1 Running 0 164m kube-system kube-proxy-xbw7p 1 /1 Running 0 164m kube-system metrics-server-7d4bcb75dd-pqkrs 1 /1 Running 0 167m When you check the logs, it'll show something like this: $ kubectl -n kube-system logs coredns-5c98d7d4d8-tfs4q plugin/loop: Loop ( 127 .0.0.1:55953 -> :1053 ) detected for zone \".\" , see https://coredns.io/plugins/loop#troubleshooting. Query: \"HINFO 4547991504243258144.3688648895315093531.\" This is most often caused by systemd-resolved stub (or something similar) running locally and CoreDNS detects a possible loop with DNS queries. The easiest but most crude way to workaround is to disable the systemd-resolved stub and revert the hosts /etc/resolv.conf to original Read more at CoreDNS troubleshooting docs . k0s controller fails on ARM boxes # In the logs you probably see ETCD not starting up properly. Etcd is not fully supported on ARM architecture, thus you need to run k0s controller and thus also etcd process with env ETCD_UNSUPPORTED_ARCH=arm64 . As Etcd is not fully supported on ARM architecture it also means that k0s controlplane with etcd itself is not fully supported on ARM either. Pods pending when using cloud providers # Once we enable cloud provider support on kubelet on worker nodes, kubelet will automatically add a taint node.cloudprovider.kubernetes.io/uninitialized for the node. This tain will prevent normal workloads to be scheduled on the node until the cloud provider controller actually runs second initialization on the node and removes the taint. This means that these nodes are not available for scheduling until the cloud provider controller is actually successfully running on the cluster. For troubleshooting your specific cloud provider see its documentation. k0s not working with read only /usr # By default k0s does not run on nodes where /usr is read only. This can be fixed by changing the default path for volumePluginDir in your k0s config. You will need to change to values, one for the kubelet itself, and one for Calico. Here is a snippet of an example config with the default values changed: spec : controllerManager : extraArgs : flex-volume-plugin-dir : \"/etc/kubernetes/kubelet-plugins/volume/exec\" network : calico : flexVolumeDriverPath : /etc/k0s/kubelet-plugins/volume/exec/nodeagent~uds workerProfiles : - name : coreos values : volumePluginDir : /etc/k0s/kubelet-plugins/volume/exec/ With this config you can start your controller as usual. Any workers will need to be started with k0s worker --profile coreos [ TOKEN ] Profiling # We drop any debug related information and symbols from the compiled binary by utilzing -w -s linker flags. To keep those symbols use DEBUG env variable: DEBUG = true make k0s Any value not equal to the \"false\" would work. To add custom linker flags use LDFLAGS variable. LD_FLAGS = \"--custom-flag=value\" make k0s","title":"Common Pitfalls"},{"location":"troubleshooting/#common-pitfalls","text":"There are few common cases we've seen where k0s fails to run properly.","title":"Common Pitfalls"},{"location":"troubleshooting/#coredns-in-crashloop","text":"The most common case we've encountered so far has been CoreDNS getting into crashloop on the node(s). With kubectl you see something like this: $ kubectl get pod --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-5f6546844f-25px6 1 /1 Running 0 167m kube-system calico-node-fwjx5 1 /1 Running 0 164m kube-system calico-node-t4tx5 1 /1 Running 0 164m kube-system calico-node-whwsg 1 /1 Running 0 164m kube-system coredns-5c98d7d4d8-tfs4q 1 /1 Error 17 167m kube-system konnectivity-agent-9jkfd 1 /1 Running 0 164m kube-system konnectivity-agent-bvhdb 1 /1 Running 0 164m kube-system konnectivity-agent-r6mzj 1 /1 Running 0 164m kube-system kube-proxy-kr2r9 1 /1 Running 0 164m kube-system kube-proxy-tbljr 1 /1 Running 0 164m kube-system kube-proxy-xbw7p 1 /1 Running 0 164m kube-system metrics-server-7d4bcb75dd-pqkrs 1 /1 Running 0 167m When you check the logs, it'll show something like this: $ kubectl -n kube-system logs coredns-5c98d7d4d8-tfs4q plugin/loop: Loop ( 127 .0.0.1:55953 -> :1053 ) detected for zone \".\" , see https://coredns.io/plugins/loop#troubleshooting. Query: \"HINFO 4547991504243258144.3688648895315093531.\" This is most often caused by systemd-resolved stub (or something similar) running locally and CoreDNS detects a possible loop with DNS queries. The easiest but most crude way to workaround is to disable the systemd-resolved stub and revert the hosts /etc/resolv.conf to original Read more at CoreDNS troubleshooting docs .","title":"CoreDNS in crashloop"},{"location":"troubleshooting/#k0s-controller-fails-on-arm-boxes","text":"In the logs you probably see ETCD not starting up properly. Etcd is not fully supported on ARM architecture, thus you need to run k0s controller and thus also etcd process with env ETCD_UNSUPPORTED_ARCH=arm64 . As Etcd is not fully supported on ARM architecture it also means that k0s controlplane with etcd itself is not fully supported on ARM either.","title":"k0s controller fails on ARM boxes"},{"location":"troubleshooting/#pods-pending-when-using-cloud-providers","text":"Once we enable cloud provider support on kubelet on worker nodes, kubelet will automatically add a taint node.cloudprovider.kubernetes.io/uninitialized for the node. This tain will prevent normal workloads to be scheduled on the node until the cloud provider controller actually runs second initialization on the node and removes the taint. This means that these nodes are not available for scheduling until the cloud provider controller is actually successfully running on the cluster. For troubleshooting your specific cloud provider see its documentation.","title":"Pods pending when using cloud providers"},{"location":"troubleshooting/#k0s-not-working-with-read-only-usr","text":"By default k0s does not run on nodes where /usr is read only. This can be fixed by changing the default path for volumePluginDir in your k0s config. You will need to change to values, one for the kubelet itself, and one for Calico. Here is a snippet of an example config with the default values changed: spec : controllerManager : extraArgs : flex-volume-plugin-dir : \"/etc/kubernetes/kubelet-plugins/volume/exec\" network : calico : flexVolumeDriverPath : /etc/k0s/kubelet-plugins/volume/exec/nodeagent~uds workerProfiles : - name : coreos values : volumePluginDir : /etc/k0s/kubelet-plugins/volume/exec/ With this config you can start your controller as usual. Any workers will need to be started with k0s worker --profile coreos [ TOKEN ]","title":"k0s not working with read only /usr"},{"location":"troubleshooting/#profiling","text":"We drop any debug related information and symbols from the compiled binary by utilzing -w -s linker flags. To keep those symbols use DEBUG env variable: DEBUG = true make k0s Any value not equal to the \"false\" would work. To add custom linker flags use LDFLAGS variable. LD_FLAGS = \"--custom-flag=value\" make k0s","title":"Profiling"},{"location":"upgrade/","text":"Upgrade # The k0s upgrade is a simple process due to its single binary distribution. The k0s single binary file includes all the necessary parts for the upgrade and essentially the upgrade process is to replace that file and restart the service. This tutorial explains two different approaches for k0s upgrade: Upgrade a k0s node locally Upgrade a k0s cluster using k0sctl Upgrade a k0s node locally # If your k0s cluster has been deployed with k0sctl, then k0sctl provides the easiest upgrade method. In that case jump to the next chapter. However, if you have deployed k0s without k0sctl, then follow the upgrade method explained in this chapter. Before starting the upgrade, consider moving your applications to another node if you want to avoid downtime. This can be done by draining a worker node . Remember to uncordon the worker node afterwards to tell Kubernetes that it can resume scheduling new pods onto the node. The upgrade process is started by stopping the currently running k0s service. sudo k0s stop Now you can replace the old k0s binary file. The easiest way is to use the download script. It will download the latest k0s binary and replace the old binary with it. You can also do this manually without the download script. curl -sSLf https://get.k0s.sh | sudo sh Then you can start the service (with the upgraded k0s) and your upgrade is done. sudo k0s start Upgrade a k0s cluster using k0sctl # The upgrading of k0s clusters using k0sctl occurs not through a particular command (there is no upgrade sub-command in k0sctl) but by way of the configuration file. The configuration file describes the desired state of the cluster, and when you pass the description to the k0sctl apply command a discovery of the current state is performed and the system does whatever is necessary to bring the cluster to the desired state (for example, perform an upgrade). k0sctl cluster upgrade process # The following operations occur during a k0sctl upgrade: Upgrade of each controller, one at a time. There is no downtime if multiple controllers are configured. Upgrade of workers, in batches of 10%. Draining of workers, which allows the workload to move to other nodes prior to the actual upgrade of the worker node components. (To skip the drain process, use the --no-drain option.) The upgrade process continues once the upgraded nodes return to Ready state. You can configure the desired cluster version in the k0sctl configuration by setting the value of spec.k0s.version : spec : k0s : version : 1.21.6+k0s.0 If you do not specify a version, k0sctl checks online for the latest version and defaults to it. $ k0sctl apply ... ... INFO [ 0001 ] == > Running phase: Upgrade controllers INFO [ 0001 ] [ ssh ] 10 .0.0.23:22: starting upgrade INFO [ 0001 ] [ ssh ] 10 .0.0.23:22: Running with legacy service name, migrating... INFO [ 0011 ] [ ssh ] 10 .0.0.23:22: waiting for the k0s service to start INFO [ 0016 ] == > Running phase: Upgrade workers INFO [ 0016 ] Upgrading 1 workers in parallel INFO [ 0016 ] [ ssh ] 10 .0.0.17:22: upgrade starting INFO [ 0027 ] [ ssh ] 10 .0.0.17:22: waiting for node to become ready again INFO [ 0027 ] [ ssh ] 10 .0.0.17:22: upgrade successful INFO [ 0027 ] == > Running phase: Disconnect from hosts INFO [ 0027 ] == > Finished in 27s INFO [ 0027 ] k0s cluster version 1 .21.6+k0s.0 is now installed INFO [ 0027 ] Tip: To access the cluster you can now fetch the admin kubeconfig using: INFO [ 0027 ] k0sctl kubeconfig","title":"Upgrade"},{"location":"upgrade/#upgrade","text":"The k0s upgrade is a simple process due to its single binary distribution. The k0s single binary file includes all the necessary parts for the upgrade and essentially the upgrade process is to replace that file and restart the service. This tutorial explains two different approaches for k0s upgrade: Upgrade a k0s node locally Upgrade a k0s cluster using k0sctl","title":"Upgrade"},{"location":"upgrade/#upgrade-a-k0s-node-locally","text":"If your k0s cluster has been deployed with k0sctl, then k0sctl provides the easiest upgrade method. In that case jump to the next chapter. However, if you have deployed k0s without k0sctl, then follow the upgrade method explained in this chapter. Before starting the upgrade, consider moving your applications to another node if you want to avoid downtime. This can be done by draining a worker node . Remember to uncordon the worker node afterwards to tell Kubernetes that it can resume scheduling new pods onto the node. The upgrade process is started by stopping the currently running k0s service. sudo k0s stop Now you can replace the old k0s binary file. The easiest way is to use the download script. It will download the latest k0s binary and replace the old binary with it. You can also do this manually without the download script. curl -sSLf https://get.k0s.sh | sudo sh Then you can start the service (with the upgraded k0s) and your upgrade is done. sudo k0s start","title":"Upgrade a k0s node locally"},{"location":"upgrade/#upgrade-a-k0s-cluster-using-k0sctl","text":"The upgrading of k0s clusters using k0sctl occurs not through a particular command (there is no upgrade sub-command in k0sctl) but by way of the configuration file. The configuration file describes the desired state of the cluster, and when you pass the description to the k0sctl apply command a discovery of the current state is performed and the system does whatever is necessary to bring the cluster to the desired state (for example, perform an upgrade).","title":"Upgrade a k0s cluster using k0sctl"},{"location":"upgrade/#k0sctl-cluster-upgrade-process","text":"The following operations occur during a k0sctl upgrade: Upgrade of each controller, one at a time. There is no downtime if multiple controllers are configured. Upgrade of workers, in batches of 10%. Draining of workers, which allows the workload to move to other nodes prior to the actual upgrade of the worker node components. (To skip the drain process, use the --no-drain option.) The upgrade process continues once the upgraded nodes return to Ready state. You can configure the desired cluster version in the k0sctl configuration by setting the value of spec.k0s.version : spec : k0s : version : 1.21.6+k0s.0 If you do not specify a version, k0sctl checks online for the latest version and defaults to it. $ k0sctl apply ... ... INFO [ 0001 ] == > Running phase: Upgrade controllers INFO [ 0001 ] [ ssh ] 10 .0.0.23:22: starting upgrade INFO [ 0001 ] [ ssh ] 10 .0.0.23:22: Running with legacy service name, migrating... INFO [ 0011 ] [ ssh ] 10 .0.0.23:22: waiting for the k0s service to start INFO [ 0016 ] == > Running phase: Upgrade workers INFO [ 0016 ] Upgrading 1 workers in parallel INFO [ 0016 ] [ ssh ] 10 .0.0.17:22: upgrade starting INFO [ 0027 ] [ ssh ] 10 .0.0.17:22: waiting for node to become ready again INFO [ 0027 ] [ ssh ] 10 .0.0.17:22: upgrade successful INFO [ 0027 ] == > Running phase: Disconnect from hosts INFO [ 0027 ] == > Finished in 27s INFO [ 0027 ] k0s cluster version 1 .21.6+k0s.0 is now installed INFO [ 0027 ] Tip: To access the cluster you can now fetch the admin kubeconfig using: INFO [ 0027 ] k0sctl kubeconfig","title":"k0sctl cluster upgrade process"},{"location":"user-management/","text":"User Management # Adding a Cluster User # Run the kubeconfig create command on the controller to add a user to the cluster. The command outputs a kubeconfig for the user, to use for authentication. k0s kubeconfig create [ username ] Enabling Access to Cluster Resources # Create the user with the system:masters group to grant the user access to the cluster: k0s kubeconfig create --groups \"system:masters\" testUser > k0s.config Create a roleBinding to grant the user access to the resources: k0s kubectl create clusterrolebinding --kubeconfig k0s.config testUser-admin-binding --clusterrole = admin --user = testUser","title":"User Management"},{"location":"user-management/#user-management","text":"","title":"User Management"},{"location":"user-management/#adding-a-cluster-user","text":"Run the kubeconfig create command on the controller to add a user to the cluster. The command outputs a kubeconfig for the user, to use for authentication. k0s kubeconfig create [ username ]","title":"Adding a Cluster User"},{"location":"user-management/#enabling-access-to-cluster-resources","text":"Create the user with the system:masters group to grant the user access to the cluster: k0s kubeconfig create --groups \"system:masters\" testUser > k0s.config Create a roleBinding to grant the user access to the resources: k0s kubectl create clusterrolebinding --kubeconfig k0s.config testUser-admin-binding --clusterrole = admin --user = testUser","title":"Enabling Access to Cluster Resources"},{"location":"worker-node-config/","text":"Configuration options for worker nodes # Although the k0s worker command does not take in any special yaml configuration, there are still methods for configuring the workers to run various components. Node labels # The k0s worker command accepts the --labels flag, with which you can make the newly joined worker node the register itself, in the Kubernetes API, with the given set of labels. For example, running the worker with k0s worker --token-file k0s.token --labels=\"k0sproject.io/foo=bar,k0sproject.io/other=xyz\" results in: $ kubectl get node --show-labels NAME STATUS ROLES AGE VERSION LABELS worker0 NotReady <none> 10s v1.20.2-k0s1 beta.kubernetes.io/arch = amd64,beta.kubernetes.io/os = linux,k0sproject.io/foo = bar,k0sproject.io/other = xyz,kubernetes.io/arch = amd64,kubernetes.io/hostname = worker0,kubernetes.io/os = linux Note: Setting the labels is only effective on the first registration of the node. Changing the labels thereafter has no effect. Kubelet args # The k0s worker command accepts a generic flag to pass in any set of arguments for kubelet process. For example, running k0s worker --token-file=k0s.token --kubelet-extra-args=\"--node-ip=1.2.3.4 --address=0.0.0.0\" passes in the given flags to kubelet as-is. As such, you must confirm that any flags you are passing in are properly formatted and valued as k0s will not validate those flags.","title":"Worker Node Configuration"},{"location":"worker-node-config/#configuration-options-for-worker-nodes","text":"Although the k0s worker command does not take in any special yaml configuration, there are still methods for configuring the workers to run various components.","title":"Configuration options for worker nodes"},{"location":"worker-node-config/#node-labels","text":"The k0s worker command accepts the --labels flag, with which you can make the newly joined worker node the register itself, in the Kubernetes API, with the given set of labels. For example, running the worker with k0s worker --token-file k0s.token --labels=\"k0sproject.io/foo=bar,k0sproject.io/other=xyz\" results in: $ kubectl get node --show-labels NAME STATUS ROLES AGE VERSION LABELS worker0 NotReady <none> 10s v1.20.2-k0s1 beta.kubernetes.io/arch = amd64,beta.kubernetes.io/os = linux,k0sproject.io/foo = bar,k0sproject.io/other = xyz,kubernetes.io/arch = amd64,kubernetes.io/hostname = worker0,kubernetes.io/os = linux Note: Setting the labels is only effective on the first registration of the node. Changing the labels thereafter has no effect.","title":"Node labels"},{"location":"worker-node-config/#kubelet-args","text":"The k0s worker command accepts a generic flag to pass in any set of arguments for kubelet process. For example, running k0s worker --token-file=k0s.token --kubelet-extra-args=\"--node-ip=1.2.3.4 --address=0.0.0.0\" passes in the given flags to kubelet as-is. As such, you must confirm that any flags you are passing in are properly formatted and valued as k0s will not validate those flags.","title":"Kubelet args"},{"location":"cli/","text":"k0s # k0s - Zero Friction Kubernetes Synopsis # k0s - The zero friction Kubernetes - https://k0sproject.io This software is built and distributed by Mirantis, Inc., and is subject to EULA https://k0sproject.io/licenses/eula Options # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) -h, --help help for k0s --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s airgap - Manage airgap setup k0s api - Run the controller api k0s backup - Back-Up k0s configuration. Must be run as root (or with sudo) k0s completion - Generate completion script k0s controller - Run controller k0s ctr - containerd CLI k0s default-config - Output the default k0s configuration yaml to stdout k0s docs - Generate k0s command documentation k0s etcd - Manage etcd cluster k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo) k0s kubeconfig - Create a kubeconfig file for a specified user k0s kubectl - kubectl controls the Kubernetes cluster manager k0s reset - Helper command for uninstalling k0s. Must be run as root (or with sudo) k0s restore - restore k0s state from given backup archive. Must be run as root (or with sudo) k0s start - Start the k0s service configured on this host. Must be run as root (or with sudo) k0s status - Helper command for get general information about k0s k0s stop - Stop the k0s service configured on this host. Must be run as root (or with sudo) k0s sysinfo - Display system information k0s token - Manage join tokens k0s validate - Helper command for validating the config file k0s version - Print the k0s version k0s worker - Run worker","title":"Index"},{"location":"cli/#k0s","text":"k0s - Zero Friction Kubernetes","title":"k0s"},{"location":"cli/#synopsis","text":"k0s - The zero friction Kubernetes - https://k0sproject.io This software is built and distributed by Mirantis, Inc., and is subject to EULA https://k0sproject.io/licenses/eula","title":"Synopsis"},{"location":"cli/#options","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) -h, --help help for k0s --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options"},{"location":"cli/#see-also","text":"k0s airgap - Manage airgap setup k0s api - Run the controller api k0s backup - Back-Up k0s configuration. Must be run as root (or with sudo) k0s completion - Generate completion script k0s controller - Run controller k0s ctr - containerd CLI k0s default-config - Output the default k0s configuration yaml to stdout k0s docs - Generate k0s command documentation k0s etcd - Manage etcd cluster k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo) k0s kubeconfig - Create a kubeconfig file for a specified user k0s kubectl - kubectl controls the Kubernetes cluster manager k0s reset - Helper command for uninstalling k0s. Must be run as root (or with sudo) k0s restore - restore k0s state from given backup archive. Must be run as root (or with sudo) k0s start - Start the k0s service configured on this host. Must be run as root (or with sudo) k0s status - Helper command for get general information about k0s k0s stop - Stop the k0s service configured on this host. Must be run as root (or with sudo) k0s sysinfo - Display system information k0s token - Manage join tokens k0s validate - Helper command for validating the config file k0s version - Print the k0s version k0s worker - Run worker","title":"SEE ALSO"},{"location":"cli/k0s/","text":"k0s # k0s - Zero Friction Kubernetes Synopsis # k0s - The zero friction Kubernetes - https://k0sproject.io This software is built and distributed by Mirantis, Inc., and is subject to EULA https://k0sproject.io/licenses/eula Options # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) -h, --help help for k0s --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s airgap - Manage airgap setup k0s api - Run the controller api k0s backup - Back-Up k0s configuration. Must be run as root (or with sudo) k0s completion - Generate completion script k0s controller - Run controller k0s ctr - containerd CLI k0s default-config - Output the default k0s configuration yaml to stdout k0s docs - Generate k0s command documentation k0s etcd - Manage etcd cluster k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo) k0s kubeconfig - Create a kubeconfig file for a specified user k0s kubectl - kubectl controls the Kubernetes cluster manager k0s reset - Helper command for uninstalling k0s. Must be run as root (or with sudo) k0s restore - restore k0s state from given backup archive. Must be run as root (or with sudo) k0s start - Start the k0s service configured on this host. Must be run as root (or with sudo) k0s status - Helper command for get general information about k0s k0s stop - Stop the k0s service configured on this host. Must be run as root (or with sudo) k0s sysinfo - Display system information k0s token - Manage join tokens k0s validate - Helper command for validating the config file k0s version - Print the k0s version k0s worker - Run worker","title":"K0s"},{"location":"cli/k0s/#k0s","text":"k0s - Zero Friction Kubernetes","title":"k0s"},{"location":"cli/k0s/#synopsis","text":"k0s - The zero friction Kubernetes - https://k0sproject.io This software is built and distributed by Mirantis, Inc., and is subject to EULA https://k0sproject.io/licenses/eula","title":"Synopsis"},{"location":"cli/k0s/#options","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) -h, --help help for k0s --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options"},{"location":"cli/k0s/#see-also","text":"k0s airgap - Manage airgap setup k0s api - Run the controller api k0s backup - Back-Up k0s configuration. Must be run as root (or with sudo) k0s completion - Generate completion script k0s controller - Run controller k0s ctr - containerd CLI k0s default-config - Output the default k0s configuration yaml to stdout k0s docs - Generate k0s command documentation k0s etcd - Manage etcd cluster k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo) k0s kubeconfig - Create a kubeconfig file for a specified user k0s kubectl - kubectl controls the Kubernetes cluster manager k0s reset - Helper command for uninstalling k0s. Must be run as root (or with sudo) k0s restore - restore k0s state from given backup archive. Must be run as root (or with sudo) k0s start - Start the k0s service configured on this host. Must be run as root (or with sudo) k0s status - Helper command for get general information about k0s k0s stop - Stop the k0s service configured on this host. Must be run as root (or with sudo) k0s sysinfo - Display system information k0s token - Manage join tokens k0s validate - Helper command for validating the config file k0s version - Print the k0s version k0s worker - Run worker","title":"SEE ALSO"},{"location":"cli/k0s_airgap/","text":"k0s airgap # Manage airgap setup Options # -c, --config string config file, use '-' to read the config from stdin --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") -h, --help help for airgap Options inherited from parent commands # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s - k0s - Zero Friction Kubernetes k0s airgap list-images - List image names and version needed for air-gap install","title":"K0s airgap"},{"location":"cli/k0s_airgap/#k0s-airgap","text":"Manage airgap setup","title":"k0s airgap"},{"location":"cli/k0s_airgap/#options","text":"-c, --config string config file, use '-' to read the config from stdin --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") -h, --help help for airgap","title":"Options"},{"location":"cli/k0s_airgap/#options-inherited-from-parent-commands","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_airgap/#see-also","text":"k0s - k0s - Zero Friction Kubernetes k0s airgap list-images - List image names and version needed for air-gap install","title":"SEE ALSO"},{"location":"cli/k0s_airgap_list-images/","text":"k0s airgap list-images # List image names and version needed for air-gap install k0s airgap list-images [flags] Examples # k0s airgap list-images Options # -h, --help help for list-images Options inherited from parent commands # -c, --config string config file, use '-' to read the config from stdin --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s airgap - Manage airgap setup","title":"K0s airgap list images"},{"location":"cli/k0s_airgap_list-images/#k0s-airgap-list-images","text":"List image names and version needed for air-gap install k0s airgap list-images [flags]","title":"k0s airgap list-images"},{"location":"cli/k0s_airgap_list-images/#examples","text":"k0s airgap list-images","title":"Examples"},{"location":"cli/k0s_airgap_list-images/#options","text":"-h, --help help for list-images","title":"Options"},{"location":"cli/k0s_airgap_list-images/#options-inherited-from-parent-commands","text":"-c, --config string config file, use '-' to read the config from stdin --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_airgap_list-images/#see-also","text":"k0s airgap - Manage airgap setup","title":"SEE ALSO"},{"location":"cli/k0s_api/","text":"k0s api # Run the controller api k0s api [flags] Options # -c, --config string config file, use '-' to read the config from stdin --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") -h, --help help for api Options inherited from parent commands # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s api"},{"location":"cli/k0s_api/#k0s-api","text":"Run the controller api k0s api [flags]","title":"k0s api"},{"location":"cli/k0s_api/#options","text":"-c, --config string config file, use '-' to read the config from stdin --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") -h, --help help for api","title":"Options"},{"location":"cli/k0s_api/#options-inherited-from-parent-commands","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_api/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_backup/","text":"k0s backup # Back-Up k0s configuration. Must be run as root (or with sudo) k0s backup [flags] Options # -c, --config string config file, use '-' to read the config from stdin --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") -h, --help help for backup --save-path string destination directory path for backup assets Options inherited from parent commands # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s backup"},{"location":"cli/k0s_backup/#k0s-backup","text":"Back-Up k0s configuration. Must be run as root (or with sudo) k0s backup [flags]","title":"k0s backup"},{"location":"cli/k0s_backup/#options","text":"-c, --config string config file, use '-' to read the config from stdin --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") -h, --help help for backup --save-path string destination directory path for backup assets","title":"Options"},{"location":"cli/k0s_backup/#options-inherited-from-parent-commands","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_backup/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_completion/","text":"k0s completion # Generate completion script Synopsis # To load completions: Bash: $ source <(k0s completion bash) To load completions for each session, execute once: # $ k0s completion bash > /etc/bash_completion.d/k0s Zsh: If shell completion is not already enabled in your environment you will need # to enable it. You can execute the following once: # $ echo \"autoload -U compinit; compinit\" >> ~/.zshrc To load completions for each session, execute once: # $ k0s completion zsh > \"${fpath[1]}/_k0s\" You will need to start a new shell for this setup to take effect. # Fish: $ k0s completion fish | source To load completions for each session, execute once: # $ k0s completion fish > ~/.config/fish/completions/k0s.fish k0s completion <bash|zsh|fish|powershell> Options # -h, --help help for completion Options inherited from parent commands # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s completion"},{"location":"cli/k0s_completion/#k0s-completion","text":"Generate completion script","title":"k0s completion"},{"location":"cli/k0s_completion/#synopsis","text":"To load completions: Bash: $ source <(k0s completion bash)","title":"Synopsis"},{"location":"cli/k0s_completion/#to-load-completions-for-each-session-execute-once","text":"$ k0s completion bash > /etc/bash_completion.d/k0s Zsh:","title":"To load completions for each session, execute once:"},{"location":"cli/k0s_completion/#if-shell-completion-is-not-already-enabled-in-your-environment-you-will-need","text":"","title":"If shell completion is not already enabled in your environment you will need"},{"location":"cli/k0s_completion/#to-enable-it-you-can-execute-the-following-once","text":"$ echo \"autoload -U compinit; compinit\" >> ~/.zshrc","title":"to enable it.  You can execute the following once:"},{"location":"cli/k0s_completion/#to-load-completions-for-each-session-execute-once_1","text":"$ k0s completion zsh > \"${fpath[1]}/_k0s\"","title":"To load completions for each session, execute once:"},{"location":"cli/k0s_completion/#you-will-need-to-start-a-new-shell-for-this-setup-to-take-effect","text":"Fish: $ k0s completion fish | source","title":"You will need to start a new shell for this setup to take effect."},{"location":"cli/k0s_completion/#to-load-completions-for-each-session-execute-once_2","text":"$ k0s completion fish > ~/.config/fish/completions/k0s.fish k0s completion <bash|zsh|fish|powershell>","title":"To load completions for each session, execute once:"},{"location":"cli/k0s_completion/#options","text":"-h, --help help for completion","title":"Options"},{"location":"cli/k0s_completion/#options-inherited-from-parent-commands","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_completion/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_controller/","text":"k0s controller # Run controller k0s controller [join-token] [flags] Examples # Command to associate master nodes: CLI argument: $ k0s controller [join-token] or CLI flag: $ k0s controller --token-file [path_to_file] Note: Token can be passed either as a CLI argument or as a flag Options # --api-server string HACK: api-server for the windows worker node --cidr-range string HACK: cidr range for the windows worker node (default \"10.96.0.0/12\") --cluster-dns string HACK: cluster dns for the windows worker node (default \"10.96.0.10\") -c, --config string config file, use '-' to read the config from stdin --cri-socket string container runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket] --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") --enable-cloud-provider Whether or not to enable cloud provider support in kubelet --enable-k0s-cloud-provider enables the k0s-cloud-provider (default false) --enable-worker enable worker (default false) -h, --help help for controller --k0s-cloud-provider-port int the port that k0s-cloud-provider binds on (default 10258) --k0s-cloud-provider-update-frequency duration the frequency of k0s-cloud-provider node updates (default 2m0s) --kubelet-extra-args string extra args for kubelet --labels strings Node labels, list of key=value pairs -l, --logging stringToString Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info]) --profile string worker profile to use on the node (default \"default\") --single enable single node (implies --enable-worker, default false) --token-file string Path to the file containing join-token. Options inherited from parent commands # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s controller"},{"location":"cli/k0s_controller/#k0s-controller","text":"Run controller k0s controller [join-token] [flags]","title":"k0s controller"},{"location":"cli/k0s_controller/#examples","text":"Command to associate master nodes: CLI argument: $ k0s controller [join-token] or CLI flag: $ k0s controller --token-file [path_to_file] Note: Token can be passed either as a CLI argument or as a flag","title":"Examples"},{"location":"cli/k0s_controller/#options","text":"--api-server string HACK: api-server for the windows worker node --cidr-range string HACK: cidr range for the windows worker node (default \"10.96.0.0/12\") --cluster-dns string HACK: cluster dns for the windows worker node (default \"10.96.0.10\") -c, --config string config file, use '-' to read the config from stdin --cri-socket string container runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket] --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") --enable-cloud-provider Whether or not to enable cloud provider support in kubelet --enable-k0s-cloud-provider enables the k0s-cloud-provider (default false) --enable-worker enable worker (default false) -h, --help help for controller --k0s-cloud-provider-port int the port that k0s-cloud-provider binds on (default 10258) --k0s-cloud-provider-update-frequency duration the frequency of k0s-cloud-provider node updates (default 2m0s) --kubelet-extra-args string extra args for kubelet --labels strings Node labels, list of key=value pairs -l, --logging stringToString Logging Levels for the different components (default [konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info]) --profile string worker profile to use on the node (default \"default\") --single enable single node (implies --enable-worker, default false) --token-file string Path to the file containing join-token.","title":"Options"},{"location":"cli/k0s_controller/#options-inherited-from-parent-commands","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_controller/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_ctr/","text":"k0s ctr # containerd CLI Synopsis # ctr is an unsupported debug and administrative client for interacting with the containerd daemon. Because it is unsupported, the commands, options, and operations are not guaranteed to be backward compatible or stable from release to release of the containerd project. k0s ctr [flags] Options # -h, --help help for ctr Options inherited from parent commands # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s ctr"},{"location":"cli/k0s_ctr/#k0s-ctr","text":"containerd CLI","title":"k0s ctr"},{"location":"cli/k0s_ctr/#synopsis","text":"ctr is an unsupported debug and administrative client for interacting with the containerd daemon. Because it is unsupported, the commands, options, and operations are not guaranteed to be backward compatible or stable from release to release of the containerd project. k0s ctr [flags]","title":"Synopsis"},{"location":"cli/k0s_ctr/#options","text":"-h, --help help for ctr","title":"Options"},{"location":"cli/k0s_ctr/#options-inherited-from-parent-commands","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_ctr/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_default-config/","text":"k0s default-config # Output the default k0s configuration yaml to stdout k0s default-config [flags] Options # -c, --config string config file, use '-' to read the config from stdin --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") -h, --help help for default-config Options inherited from parent commands # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s default config"},{"location":"cli/k0s_default-config/#k0s-default-config","text":"Output the default k0s configuration yaml to stdout k0s default-config [flags]","title":"k0s default-config"},{"location":"cli/k0s_default-config/#options","text":"-c, --config string config file, use '-' to read the config from stdin --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") -h, --help help for default-config","title":"Options"},{"location":"cli/k0s_default-config/#options-inherited-from-parent-commands","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_default-config/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_docs/","text":"k0s docs # Generate k0s command documentation k0s docs <markdown|man> [flags] Options # -h, --help help for docs Options inherited from parent commands # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s docs"},{"location":"cli/k0s_docs/#k0s-docs","text":"Generate k0s command documentation k0s docs <markdown|man> [flags]","title":"k0s docs"},{"location":"cli/k0s_docs/#options","text":"-h, --help help for docs","title":"Options"},{"location":"cli/k0s_docs/#options-inherited-from-parent-commands","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_docs/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_etcd/","text":"k0s etcd # Manage etcd cluster Options # -c, --config string config file, use '-' to read the config from stdin --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") -h, --help help for etcd Options inherited from parent commands # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s - k0s - Zero Friction Kubernetes k0s etcd leave - Sign off a given etc node from etcd cluster k0s etcd member-list - Returns etcd cluster members list","title":"K0s etcd"},{"location":"cli/k0s_etcd/#k0s-etcd","text":"Manage etcd cluster","title":"k0s etcd"},{"location":"cli/k0s_etcd/#options","text":"-c, --config string config file, use '-' to read the config from stdin --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") -h, --help help for etcd","title":"Options"},{"location":"cli/k0s_etcd/#options-inherited-from-parent-commands","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_etcd/#see-also","text":"k0s - k0s - Zero Friction Kubernetes k0s etcd leave - Sign off a given etc node from etcd cluster k0s etcd member-list - Returns etcd cluster members list","title":"SEE ALSO"},{"location":"cli/k0s_etcd_leave/","text":"k0s etcd leave # Sign off a given etc node from etcd cluster k0s etcd leave [flags] Options # -h, --help help for leave --peer-address string etcd peer address Options inherited from parent commands # -c, --config string config file, use '-' to read the config from stdin --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s etcd - Manage etcd cluster","title":"K0s etcd leave"},{"location":"cli/k0s_etcd_leave/#k0s-etcd-leave","text":"Sign off a given etc node from etcd cluster k0s etcd leave [flags]","title":"k0s etcd leave"},{"location":"cli/k0s_etcd_leave/#options","text":"-h, --help help for leave --peer-address string etcd peer address","title":"Options"},{"location":"cli/k0s_etcd_leave/#options-inherited-from-parent-commands","text":"-c, --config string config file, use '-' to read the config from stdin --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_etcd_leave/#see-also","text":"k0s etcd - Manage etcd cluster","title":"SEE ALSO"},{"location":"cli/k0s_etcd_member-list/","text":"k0s etcd member-list # Returns etcd cluster members list k0s etcd member-list [flags] Options # -h, --help help for member-list Options inherited from parent commands # -c, --config string config file, use '-' to read the config from stdin --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s etcd - Manage etcd cluster","title":"K0s etcd member list"},{"location":"cli/k0s_etcd_member-list/#k0s-etcd-member-list","text":"Returns etcd cluster members list k0s etcd member-list [flags]","title":"k0s etcd member-list"},{"location":"cli/k0s_etcd_member-list/#options","text":"-h, --help help for member-list","title":"Options"},{"location":"cli/k0s_etcd_member-list/#options-inherited-from-parent-commands","text":"-c, --config string config file, use '-' to read the config from stdin --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_etcd_member-list/#see-also","text":"k0s etcd - Manage etcd cluster","title":"SEE ALSO"},{"location":"cli/k0s_install/","text":"k0s install # Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo) Options # -c, --config string config file, use '-' to read the config from stdin --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") -h, --help help for install Options inherited from parent commands # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s - k0s - Zero Friction Kubernetes k0s install controller - Helper command for setting up k0s as controller node on a brand-new system. Must be run as root (or with sudo) k0s install worker - Helper command for setting up k0s as a worker node on a brand-new system. Must be run as root (or with sudo)","title":"K0s install"},{"location":"cli/k0s_install/#k0s-install","text":"Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo)","title":"k0s install"},{"location":"cli/k0s_install/#options","text":"-c, --config string config file, use '-' to read the config from stdin --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") -h, --help help for install","title":"Options"},{"location":"cli/k0s_install/#options-inherited-from-parent-commands","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_install/#see-also","text":"k0s - k0s - Zero Friction Kubernetes k0s install controller - Helper command for setting up k0s as controller node on a brand-new system. Must be run as root (or with sudo) k0s install worker - Helper command for setting up k0s as a worker node on a brand-new system. Must be run as root (or with sudo)","title":"SEE ALSO"},{"location":"cli/k0s_install_controller/","text":"k0s install controller # Helper command for setting up k0s as controller node on a brand-new system. Must be run as root (or with sudo) k0s install controller [flags] Examples # All default values of controller command will be passed to the service stub unless overriden. With controller subcommand you can setup a single node cluster by running: k0s install controller --enable-worker Options # --api-server string HACK: api-server for the windows worker node --cidr-range string HACK: cidr range for the windows worker node (default \"10.96.0.0/12\") --cluster-dns string HACK: cluster dns for the windows worker node (default \"10.96.0.10\") --cri-socket string container runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket] --enable-cloud-provider Whether or not to enable cloud provider support in kubelet --enable-k0s-cloud-provider enables the k0s-cloud-provider (default false) --enable-worker enable worker (default false) -h, --help help for controller --k0s-cloud-provider-port int the port that k0s-cloud-provider binds on (default 10258) --k0s-cloud-provider-update-frequency duration the frequency of k0s-cloud-provider node updates (default 2m0s) --kubelet-extra-args string extra args for kubelet --labels strings Node labels, list of key=value pairs -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) --profile string worker profile to use on the node (default \"default\") --single enable single node (implies --enable-worker, default false) --token-file string Path to the file containing join-token. Options inherited from parent commands # -c, --config string config file, use '-' to read the config from stdin --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo)","title":"K0s install controller"},{"location":"cli/k0s_install_controller/#k0s-install-controller","text":"Helper command for setting up k0s as controller node on a brand-new system. Must be run as root (or with sudo) k0s install controller [flags]","title":"k0s install controller"},{"location":"cli/k0s_install_controller/#examples","text":"All default values of controller command will be passed to the service stub unless overriden. With controller subcommand you can setup a single node cluster by running: k0s install controller --enable-worker","title":"Examples"},{"location":"cli/k0s_install_controller/#options","text":"--api-server string HACK: api-server for the windows worker node --cidr-range string HACK: cidr range for the windows worker node (default \"10.96.0.0/12\") --cluster-dns string HACK: cluster dns for the windows worker node (default \"10.96.0.10\") --cri-socket string container runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket] --enable-cloud-provider Whether or not to enable cloud provider support in kubelet --enable-k0s-cloud-provider enables the k0s-cloud-provider (default false) --enable-worker enable worker (default false) -h, --help help for controller --k0s-cloud-provider-port int the port that k0s-cloud-provider binds on (default 10258) --k0s-cloud-provider-update-frequency duration the frequency of k0s-cloud-provider node updates (default 2m0s) --kubelet-extra-args string extra args for kubelet --labels strings Node labels, list of key=value pairs -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) --profile string worker profile to use on the node (default \"default\") --single enable single node (implies --enable-worker, default false) --token-file string Path to the file containing join-token.","title":"Options"},{"location":"cli/k0s_install_controller/#options-inherited-from-parent-commands","text":"-c, --config string config file, use '-' to read the config from stdin --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_install_controller/#see-also","text":"k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo)","title":"SEE ALSO"},{"location":"cli/k0s_install_worker/","text":"k0s install worker # Helper command for setting up k0s as a worker node on a brand-new system. Must be run as root (or with sudo) k0s install worker [flags] Examples # Worker subcommand allows you to pass in all available worker parameters. All default values of worker command will be passed to the service stub unless overriden. Windows flags like \"--api-server\", \"--cidr-range\" and \"--cluster-dns\" will be ignored since install command doesn't yet support Windows services Options # --api-server string HACK: api-server for the windows worker node --cidr-range string HACK: cidr range for the windows worker node (default \"10.96.0.0/12\") --cluster-dns string HACK: cluster dns for the windows worker node (default \"10.96.0.10\") --cri-socket string container runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket] --enable-cloud-provider Whether or not to enable cloud provider support in kubelet -h, --help help for worker --kubelet-extra-args string extra args for kubelet --labels strings Node labels, list of key=value pairs -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) --profile string worker profile to use on the node (default \"default\") --token-file string Path to the file containing token. Options inherited from parent commands # -c, --config string config file, use '-' to read the config from stdin --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo)","title":"K0s install worker"},{"location":"cli/k0s_install_worker/#k0s-install-worker","text":"Helper command for setting up k0s as a worker node on a brand-new system. Must be run as root (or with sudo) k0s install worker [flags]","title":"k0s install worker"},{"location":"cli/k0s_install_worker/#examples","text":"Worker subcommand allows you to pass in all available worker parameters. All default values of worker command will be passed to the service stub unless overriden. Windows flags like \"--api-server\", \"--cidr-range\" and \"--cluster-dns\" will be ignored since install command doesn't yet support Windows services","title":"Examples"},{"location":"cli/k0s_install_worker/#options","text":"--api-server string HACK: api-server for the windows worker node --cidr-range string HACK: cidr range for the windows worker node (default \"10.96.0.0/12\") --cluster-dns string HACK: cluster dns for the windows worker node (default \"10.96.0.10\") --cri-socket string container runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket] --enable-cloud-provider Whether or not to enable cloud provider support in kubelet -h, --help help for worker --kubelet-extra-args string extra args for kubelet --labels strings Node labels, list of key=value pairs -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) --profile string worker profile to use on the node (default \"default\") --token-file string Path to the file containing token.","title":"Options"},{"location":"cli/k0s_install_worker/#options-inherited-from-parent-commands","text":"-c, --config string config file, use '-' to read the config from stdin --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_install_worker/#see-also","text":"k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo)","title":"SEE ALSO"},{"location":"cli/k0s_kubeconfig/","text":"k0s kubeconfig # Create a kubeconfig file for a specified user k0s kubeconfig [command] [flags] Options # -c, --config string config file, use '-' to read the config from stdin --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") -h, --help help for kubeconfig Options inherited from parent commands # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s - k0s - Zero Friction Kubernetes k0s kubeconfig admin - Display Admin's Kubeconfig file k0s kubeconfig create - Create a kubeconfig for a user","title":"K0s kubeconfig"},{"location":"cli/k0s_kubeconfig/#k0s-kubeconfig","text":"Create a kubeconfig file for a specified user k0s kubeconfig [command] [flags]","title":"k0s kubeconfig"},{"location":"cli/k0s_kubeconfig/#options","text":"-c, --config string config file, use '-' to read the config from stdin --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") -h, --help help for kubeconfig","title":"Options"},{"location":"cli/k0s_kubeconfig/#options-inherited-from-parent-commands","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_kubeconfig/#see-also","text":"k0s - k0s - Zero Friction Kubernetes k0s kubeconfig admin - Display Admin's Kubeconfig file k0s kubeconfig create - Create a kubeconfig for a user","title":"SEE ALSO"},{"location":"cli/k0s_kubeconfig_admin/","text":"k0s kubeconfig admin # Display Admin's Kubeconfig file Synopsis # Print kubeconfig for the Admin user to stdout k0s kubeconfig admin [command] [flags] Examples # $ k0s kubeconfig admin > ~/.kube/config $ export KUBECONFIG=~/.kube/config $ kubectl get nodes Options # -h, --help help for admin Options inherited from parent commands # -c, --config string config file, use '-' to read the config from stdin --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s kubeconfig - Create a kubeconfig file for a specified user","title":"K0s kubeconfig admin"},{"location":"cli/k0s_kubeconfig_admin/#k0s-kubeconfig-admin","text":"Display Admin's Kubeconfig file","title":"k0s kubeconfig admin"},{"location":"cli/k0s_kubeconfig_admin/#synopsis","text":"Print kubeconfig for the Admin user to stdout k0s kubeconfig admin [command] [flags]","title":"Synopsis"},{"location":"cli/k0s_kubeconfig_admin/#examples","text":"$ k0s kubeconfig admin > ~/.kube/config $ export KUBECONFIG=~/.kube/config $ kubectl get nodes","title":"Examples"},{"location":"cli/k0s_kubeconfig_admin/#options","text":"-h, --help help for admin","title":"Options"},{"location":"cli/k0s_kubeconfig_admin/#options-inherited-from-parent-commands","text":"-c, --config string config file, use '-' to read the config from stdin --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_kubeconfig_admin/#see-also","text":"k0s kubeconfig - Create a kubeconfig file for a specified user","title":"SEE ALSO"},{"location":"cli/k0s_kubeconfig_create/","text":"k0s kubeconfig create # Create a kubeconfig for a user Synopsis # Create a kubeconfig with a signed certificate and public key for a given user (and optionally user groups) Note: A certificate once signed cannot be revoked for a particular user k0s kubeconfig create [username] [flags] Examples # Command to create a kubeconfig for a user: CLI argument: $ k0s kubeconfig create [username] optionally add groups: $ k0s kubeconfig create [username] --groups [groups] Options # --groups string Specify groups -h, --help help for create Options inherited from parent commands # -c, --config string config file, use '-' to read the config from stdin --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s kubeconfig - Create a kubeconfig file for a specified user","title":"K0s kubeconfig create"},{"location":"cli/k0s_kubeconfig_create/#k0s-kubeconfig-create","text":"Create a kubeconfig for a user","title":"k0s kubeconfig create"},{"location":"cli/k0s_kubeconfig_create/#synopsis","text":"Create a kubeconfig with a signed certificate and public key for a given user (and optionally user groups) Note: A certificate once signed cannot be revoked for a particular user k0s kubeconfig create [username] [flags]","title":"Synopsis"},{"location":"cli/k0s_kubeconfig_create/#examples","text":"Command to create a kubeconfig for a user: CLI argument: $ k0s kubeconfig create [username] optionally add groups: $ k0s kubeconfig create [username] --groups [groups]","title":"Examples"},{"location":"cli/k0s_kubeconfig_create/#options","text":"--groups string Specify groups -h, --help help for create","title":"Options"},{"location":"cli/k0s_kubeconfig_create/#options-inherited-from-parent-commands","text":"-c, --config string config file, use '-' to read the config from stdin --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_kubeconfig_create/#see-also","text":"k0s kubeconfig - Create a kubeconfig file for a specified user","title":"SEE ALSO"},{"location":"cli/k0s_kubectl/","text":"k0s kubectl # kubectl controls the Kubernetes cluster manager Synopsis # kubectl controls the Kubernetes cluster manager. Find more information at: https://kubernetes.io/docs/reference/kubectl/overview/ k0s kubectl [flags] Options # --add-dir-header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/merlin/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use -h, --help help for kubectl --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --kubeconfig string Path to the kubeconfig file to use for CLI requests. --log-backtrace-at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log-dir string If non-empty, write log files in this directory --log-file string If non-empty, use this log file --log-file-max-size uint Defines the maximum size a log file can grow to. Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --match-server-version Require server version to match client version -n, --namespace string If present, the namespace scope for this CLI request --one-output If true, only write logs to their native severity level (vs also writing to each lower severity level --password string Password for basic authentication to the API server --profile string Name of profile to capture. One of (none|cpu|heap|goroutine|threadcreate|block|mutex) (default \"none\") --profile-output string Name of the file to write the profile to (default \"profile.pprof\") --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --skip-headers If true, avoid header prefixes in the log messages --skip-log-headers If true, avoid headers when opening log files --stderrthreshold severity logs at or above this threshold go to stderr (default 2) --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging --warnings-as-errors Treat warnings received from the server as errors and exit with a non-zero exit code Options inherited from parent commands # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s kubectl"},{"location":"cli/k0s_kubectl/#k0s-kubectl","text":"kubectl controls the Kubernetes cluster manager","title":"k0s kubectl"},{"location":"cli/k0s_kubectl/#synopsis","text":"kubectl controls the Kubernetes cluster manager. Find more information at: https://kubernetes.io/docs/reference/kubectl/overview/ k0s kubectl [flags]","title":"Synopsis"},{"location":"cli/k0s_kubectl/#options","text":"--add-dir-header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/merlin/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use -h, --help help for kubectl --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --kubeconfig string Path to the kubeconfig file to use for CLI requests. --log-backtrace-at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log-dir string If non-empty, write log files in this directory --log-file string If non-empty, use this log file --log-file-max-size uint Defines the maximum size a log file can grow to. Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --match-server-version Require server version to match client version -n, --namespace string If present, the namespace scope for this CLI request --one-output If true, only write logs to their native severity level (vs also writing to each lower severity level --password string Password for basic authentication to the API server --profile string Name of profile to capture. One of (none|cpu|heap|goroutine|threadcreate|block|mutex) (default \"none\") --profile-output string Name of the file to write the profile to (default \"profile.pprof\") --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --skip-headers If true, avoid header prefixes in the log messages --skip-log-headers If true, avoid headers when opening log files --stderrthreshold severity logs at or above this threshold go to stderr (default 2) --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging --warnings-as-errors Treat warnings received from the server as errors and exit with a non-zero exit code","title":"Options"},{"location":"cli/k0s_kubectl/#options-inherited-from-parent-commands","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_kubectl/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_reset/","text":"k0s reset # Helper command for uninstalling k0s. Must be run as root (or with sudo) k0s reset [flags] Options # -c, --config string config file, use '-' to read the config from stdin --cri-socket string container runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket] --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") -h, --help help for reset Options inherited from parent commands # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s reset"},{"location":"cli/k0s_reset/#k0s-reset","text":"Helper command for uninstalling k0s. Must be run as root (or with sudo) k0s reset [flags]","title":"k0s reset"},{"location":"cli/k0s_reset/#options","text":"-c, --config string config file, use '-' to read the config from stdin --cri-socket string container runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket] --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") -h, --help help for reset","title":"Options"},{"location":"cli/k0s_reset/#options-inherited-from-parent-commands","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_reset/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_restore/","text":"k0s restore # restore k0s state from given backup archive. Must be run as root (or with sudo) k0s restore [flags] Options # -c, --config string config file, use '-' to read the config from stdin --config-out string Specify desired name and full path for the restored k0s.yaml file (default: ${cwd}/k0s_<archive timestamp>.yaml) --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") -h, --help help for restore Options inherited from parent commands # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s restore"},{"location":"cli/k0s_restore/#k0s-restore","text":"restore k0s state from given backup archive. Must be run as root (or with sudo) k0s restore [flags]","title":"k0s restore"},{"location":"cli/k0s_restore/#options","text":"-c, --config string config file, use '-' to read the config from stdin --config-out string Specify desired name and full path for the restored k0s.yaml file (default: ${cwd}/k0s_<archive timestamp>.yaml) --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") -h, --help help for restore","title":"Options"},{"location":"cli/k0s_restore/#options-inherited-from-parent-commands","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_restore/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_start/","text":"k0s start # Start the k0s service configured on this host. Must be run as root (or with sudo) k0s start [flags] Options # -h, --help help for start Options inherited from parent commands # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s start"},{"location":"cli/k0s_start/#k0s-start","text":"Start the k0s service configured on this host. Must be run as root (or with sudo) k0s start [flags]","title":"k0s start"},{"location":"cli/k0s_start/#options","text":"-h, --help help for start","title":"Options"},{"location":"cli/k0s_start/#options-inherited-from-parent-commands","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_start/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_status/","text":"k0s status # Helper command for get general information about k0s k0s status [flags] Examples # The command will return information about system init, PID, k0s role, kubeconfig and similar. Options # -h, --help help for status -o, --out string sets type of output to json or yaml Options inherited from parent commands # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s status"},{"location":"cli/k0s_status/#k0s-status","text":"Helper command for get general information about k0s k0s status [flags]","title":"k0s status"},{"location":"cli/k0s_status/#examples","text":"The command will return information about system init, PID, k0s role, kubeconfig and similar.","title":"Examples"},{"location":"cli/k0s_status/#options","text":"-h, --help help for status -o, --out string sets type of output to json or yaml","title":"Options"},{"location":"cli/k0s_status/#options-inherited-from-parent-commands","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_status/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_stop/","text":"k0s stop # Stop the k0s service configured on this host. Must be run as root (or with sudo) k0s stop [flags] Options # -h, --help help for stop Options inherited from parent commands # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s stop"},{"location":"cli/k0s_stop/#k0s-stop","text":"Stop the k0s service configured on this host. Must be run as root (or with sudo) k0s stop [flags]","title":"k0s stop"},{"location":"cli/k0s_stop/#options","text":"-h, --help help for stop","title":"Options"},{"location":"cli/k0s_stop/#options-inherited-from-parent-commands","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_stop/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_sysinfo/","text":"k0s sysinfo # Display system information k0s sysinfo [flags] Options # -h, --help help for sysinfo Options inherited from parent commands # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s sysinfo"},{"location":"cli/k0s_sysinfo/#k0s-sysinfo","text":"Display system information k0s sysinfo [flags]","title":"k0s sysinfo"},{"location":"cli/k0s_sysinfo/#options","text":"-h, --help help for sysinfo","title":"Options"},{"location":"cli/k0s_sysinfo/#options-inherited-from-parent-commands","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_sysinfo/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_token/","text":"k0s token # Manage join tokens Options # -c, --config string config file, use '-' to read the config from stdin --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") -h, --help help for token Options inherited from parent commands # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s - k0s - Zero Friction Kubernetes k0s token create - Create join token k0s token invalidate - Invalidates existing join token k0s token list - List join tokens","title":"K0s token"},{"location":"cli/k0s_token/#k0s-token","text":"Manage join tokens","title":"k0s token"},{"location":"cli/k0s_token/#options","text":"-c, --config string config file, use '-' to read the config from stdin --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") -h, --help help for token","title":"Options"},{"location":"cli/k0s_token/#options-inherited-from-parent-commands","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_token/#see-also","text":"k0s - k0s - Zero Friction Kubernetes k0s token create - Create join token k0s token invalidate - Invalidates existing join token k0s token list - List join tokens","title":"SEE ALSO"},{"location":"cli/k0s_token_create/","text":"k0s token create # Create join token k0s token create [flags] Examples # k0s token create --role worker --expiry 100h //sets expiration time to 100 hours k0s token create --role worker --expiry 10m //sets expiration time to 10 minutes Options # --expiry string Expiration time of the token. Format 1.5h, 2h45m or 300ms. (default \"0s\") -h, --help help for create --role string Either worker or controller (default \"worker\") --wait wait forever (default false) Options inherited from parent commands # -c, --config string config file, use '-' to read the config from stdin --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s token - Manage join tokens","title":"K0s token create"},{"location":"cli/k0s_token_create/#k0s-token-create","text":"Create join token k0s token create [flags]","title":"k0s token create"},{"location":"cli/k0s_token_create/#examples","text":"k0s token create --role worker --expiry 100h //sets expiration time to 100 hours k0s token create --role worker --expiry 10m //sets expiration time to 10 minutes","title":"Examples"},{"location":"cli/k0s_token_create/#options","text":"--expiry string Expiration time of the token. Format 1.5h, 2h45m or 300ms. (default \"0s\") -h, --help help for create --role string Either worker or controller (default \"worker\") --wait wait forever (default false)","title":"Options"},{"location":"cli/k0s_token_create/#options-inherited-from-parent-commands","text":"-c, --config string config file, use '-' to read the config from stdin --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_token_create/#see-also","text":"k0s token - Manage join tokens","title":"SEE ALSO"},{"location":"cli/k0s_token_invalidate/","text":"k0s token invalidate # Invalidates existing join token k0s token invalidate [flags] Examples # k0s token invalidate xyz123 Options # -h, --help help for invalidate Options inherited from parent commands # -c, --config string config file, use '-' to read the config from stdin --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s token - Manage join tokens","title":"K0s token invalidate"},{"location":"cli/k0s_token_invalidate/#k0s-token-invalidate","text":"Invalidates existing join token k0s token invalidate [flags]","title":"k0s token invalidate"},{"location":"cli/k0s_token_invalidate/#examples","text":"k0s token invalidate xyz123","title":"Examples"},{"location":"cli/k0s_token_invalidate/#options","text":"-h, --help help for invalidate","title":"Options"},{"location":"cli/k0s_token_invalidate/#options-inherited-from-parent-commands","text":"-c, --config string config file, use '-' to read the config from stdin --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_token_invalidate/#see-also","text":"k0s token - Manage join tokens","title":"SEE ALSO"},{"location":"cli/k0s_token_list/","text":"k0s token list # List join tokens k0s token list [flags] Examples # k0s token list --role worker // list worker tokens Options # -h, --help help for list --role string Either worker, controller or empty for all roles Options inherited from parent commands # -c, --config string config file, use '-' to read the config from stdin --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s token - Manage join tokens","title":"K0s token list"},{"location":"cli/k0s_token_list/#k0s-token-list","text":"List join tokens k0s token list [flags]","title":"k0s token list"},{"location":"cli/k0s_token_list/#examples","text":"k0s token list --role worker // list worker tokens","title":"Examples"},{"location":"cli/k0s_token_list/#options","text":"-h, --help help for list --role string Either worker, controller or empty for all roles","title":"Options"},{"location":"cli/k0s_token_list/#options-inherited-from-parent-commands","text":"-c, --config string config file, use '-' to read the config from stdin --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_token_list/#see-also","text":"k0s token - Manage join tokens","title":"SEE ALSO"},{"location":"cli/k0s_validate/","text":"k0s validate # Helper command for validating the config file Options # -h, --help help for validate Options inherited from parent commands # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s - k0s - Zero Friction Kubernetes k0s validate config - Helper command for validating the config file","title":"K0s validate"},{"location":"cli/k0s_validate/#k0s-validate","text":"Helper command for validating the config file","title":"k0s validate"},{"location":"cli/k0s_validate/#options","text":"-h, --help help for validate","title":"Options"},{"location":"cli/k0s_validate/#options-inherited-from-parent-commands","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_validate/#see-also","text":"k0s - k0s - Zero Friction Kubernetes k0s validate config - Helper command for validating the config file","title":"SEE ALSO"},{"location":"cli/k0s_validate_config/","text":"k0s validate config # Helper command for validating the config file Synopsis # Example: k0s validate config --config path_to_config.yaml k0s validate config [flags] Options # -c, --config string config file, use '-' to read the config from stdin --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") -h, --help help for config Options inherited from parent commands # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s validate - Helper command for validating the config file","title":"K0s validate config"},{"location":"cli/k0s_validate_config/#k0s-validate-config","text":"Helper command for validating the config file","title":"k0s validate config"},{"location":"cli/k0s_validate_config/#synopsis","text":"Example: k0s validate config --config path_to_config.yaml k0s validate config [flags]","title":"Synopsis"},{"location":"cli/k0s_validate_config/#options","text":"-c, --config string config file, use '-' to read the config from stdin --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") -h, --help help for config","title":"Options"},{"location":"cli/k0s_validate_config/#options-inherited-from-parent-commands","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_validate_config/#see-also","text":"k0s validate - Helper command for validating the config file","title":"SEE ALSO"},{"location":"cli/k0s_version/","text":"k0s version # Print the k0s version k0s version [flags] Options # -a, --all use to print all k0s version info -h, --help help for version -j, --json use to print all k0s version info in json Options inherited from parent commands # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s version"},{"location":"cli/k0s_version/#k0s-version","text":"Print the k0s version k0s version [flags]","title":"k0s version"},{"location":"cli/k0s_version/#options","text":"-a, --all use to print all k0s version info -h, --help help for version -j, --json use to print all k0s version info in json","title":"Options"},{"location":"cli/k0s_version/#options-inherited-from-parent-commands","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_version/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_worker/","text":"k0s worker # Run worker k0s worker [join-token] [flags] Examples # Command to add worker node to the master node: CLI argument: $ k0s worker [token] or CLI flag: $ k0s worker --token-file [path_to_file] Note: Token can be passed either as a CLI argument or as a flag Options # --api-server string HACK: api-server for the windows worker node --cidr-range string HACK: cidr range for the windows worker node (default \"10.96.0.0/12\") --cluster-dns string HACK: cluster dns for the windows worker node (default \"10.96.0.10\") -c, --config string config file, use '-' to read the config from stdin --cri-socket string container runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket] --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") --enable-cloud-provider Whether or not to enable cloud provider support in kubelet -h, --help help for worker --kubelet-extra-args string extra args for kubelet --labels strings Node labels, list of key=value pairs -l, --logging stringToString Logging Levels for the different components (default [kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info,konnectivity-server=1]) --profile string worker profile to use on the node (default \"default\") --token-file string Path to the file containing token. Options inherited from parent commands # --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s worker"},{"location":"cli/k0s_worker/#k0s-worker","text":"Run worker k0s worker [join-token] [flags]","title":"k0s worker"},{"location":"cli/k0s_worker/#examples","text":"Command to add worker node to the master node: CLI argument: $ k0s worker [token] or CLI flag: $ k0s worker --token-file [path_to_file] Note: Token can be passed either as a CLI argument or as a flag","title":"Examples"},{"location":"cli/k0s_worker/#options","text":"--api-server string HACK: api-server for the windows worker node --cidr-range string HACK: cidr range for the windows worker node (default \"10.96.0.0/12\") --cluster-dns string HACK: cluster dns for the windows worker node (default \"10.96.0.10\") -c, --config string config file, use '-' to read the config from stdin --cri-socket string container runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket] --debugListenOn string Http listenOn for Debug pprof handler (default \":6060\") --enable-cloud-provider Whether or not to enable cloud provider support in kubelet -h, --help help for worker --kubelet-extra-args string extra args for kubelet --labels strings Node labels, list of key=value pairs -l, --logging stringToString Logging Levels for the different components (default [kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info,containerd=info,konnectivity-server=1]) --profile string worker profile to use on the node (default \"default\") --token-file string Path to the file containing token.","title":"Options"},{"location":"cli/k0s_worker/#options-inherited-from-parent-commands","text":"--data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! --debug Debug logging (default: false) --log-flush-frequency duration Maximum number of seconds between log flushes (default 5s) --version version[=true] Print version information and quit","title":"Options inherited from parent commands"},{"location":"cli/k0s_worker/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"contributors/CODE_OF_CONDUCT/","text":"k0s Community Code of Conduct # k0s follows the CNCF Code of Conduct .","title":"k0s Community Code of Conduct"},{"location":"contributors/CODE_OF_CONDUCT/#k0s-community-code-of-conduct","text":"k0s follows the CNCF Code of Conduct .","title":"k0s Community Code of Conduct"},{"location":"contributors/github_workflow/","text":"Github Workflow # This guide assumes you have already cloned the upstream repo to your system via git clone, or via go get github.com/k0sproject/k0s . Fork The Project # Go to http://github.com/k0sproject/k0s On the top, right-hand side, click on \"fork\" and select your username for the fork destination. Adding the Forked Remote # export GITHUB_USER ={ your github ' s username } cd $WORKDIR /k0s git remote add $GITHUB_USER git@github.com: ${ GITHUB_USER } /k0s.git # Prevent push to Upstream git remote set-url --push origin no_push # Set your fork remote as a default push target git push --set-upstream $GITHUB_USER main Your remotes should look something like this: $ git remote -v origin https://github.com/k0sproject/k0s ( fetch ) origin no_push ( push ) my_fork git@github.com: { github_username } /k0s.git ( fetch ) my_fork git@github.com: { github_username } /k0s.git ( push ) Create & Rebase Your Feature Branch # Create a feature branch and switch to it: git checkout -b my_feature_branch Rebase your branch: $ git fetch origin $ git rebase origin/main Current branch my_feature_branch is up to date. Please don't use git pull instead of the above fetch / rebase . git pull does a merge, which leaves merge commits. These make the commit history messy and violate the principle that commits ought to be individually understandable and useful. Commit & Push # Commit and sign your changes: git commit --signoff The commit message should have a short title as first line, an empty line and then a longer description that explains why the change was made, unless it is obvious. You can go back and edit/build/test some more, then commit --amend in a few cycles. When ready, push your changes to your fork's repository: git push --set-upstream my_fork my_feature_branch Open a Pull Request # Github Docs Get a code review # Once your pull request has been opened it will be assigned to one or more reviewers, and will go through a series of smoke tests. Commit changes made in response to review comments should be added to the same branch on your fork. Very small PRs are easy to review. Very large PRs are very difficult to review. Squashing Commits # Commits on your branch should represent meaningful milestones or units of work. Small commits that contain typo fixes, rebases, review feedbacks, etc should be squashed. To do that, it's best to perform an interactive rebase : Example # Rebase your feature branch against upstream main branch: git rebase -i origin/main If your PR has 3 commits, output would be similar to this: pick f7f3f6d Changed some code pick 310154e fixed some typos pick a5f4a0d made some review changes # Rebase 710f0f8..a5f4a0d onto 710f0f8 # # Commands: # p, pick <commit> = use commit # r, reword <commit> = use commit, but edit the commit message # e, edit <commit> = use commit, but stop for amending # s, squash <commit> = use commit, but meld into previous commit # f, fixup <commit> = like \"squash\", but discard this commit's log message # x, exec <command> = run command (the rest of the line) using shell # b, break = stop here (continue rebase later with 'git rebase --continue') # d, drop <commit> = remove commit # l, label <label> = label current HEAD with a name # t, reset <label> = reset HEAD to a label # m, merge [-C <commit> | -c <commit>] <label> [# <oneline>] # . create a merge commit using the original merge commit's # . message (or the oneline, if no original merge commit was # . specified). Use -c <commit> to reword the commit message. # # These lines can be re-ordered; they are executed from top to bottom. # # However, if you remove everything, the rebase will be aborted. # # Note that empty commits are commented out Use a command line text editor to change the word pick to f of fixup for the commits you want to squash, then save your changes and continue the rebase: Per the output above, you can see that: fixup <commit> = like \"squash\" , but discard this commit ' s log message Which means that when rebased, the commit message \"fixed some typos\" will be removed, and squashed with the parent commit. Push Your Final Changes # Once done, you can push the final commits to your branch: git push --force You can run multiple iteration of rebase / push -f , if needed.","title":"GitHub Workflow"},{"location":"contributors/github_workflow/#github-workflow","text":"This guide assumes you have already cloned the upstream repo to your system via git clone, or via go get github.com/k0sproject/k0s .","title":"Github Workflow"},{"location":"contributors/github_workflow/#fork-the-project","text":"Go to http://github.com/k0sproject/k0s On the top, right-hand side, click on \"fork\" and select your username for the fork destination.","title":"Fork The Project"},{"location":"contributors/github_workflow/#adding-the-forked-remote","text":"export GITHUB_USER ={ your github ' s username } cd $WORKDIR /k0s git remote add $GITHUB_USER git@github.com: ${ GITHUB_USER } /k0s.git # Prevent push to Upstream git remote set-url --push origin no_push # Set your fork remote as a default push target git push --set-upstream $GITHUB_USER main Your remotes should look something like this: $ git remote -v origin https://github.com/k0sproject/k0s ( fetch ) origin no_push ( push ) my_fork git@github.com: { github_username } /k0s.git ( fetch ) my_fork git@github.com: { github_username } /k0s.git ( push )","title":"Adding the Forked Remote"},{"location":"contributors/github_workflow/#create-rebase-your-feature-branch","text":"Create a feature branch and switch to it: git checkout -b my_feature_branch Rebase your branch: $ git fetch origin $ git rebase origin/main Current branch my_feature_branch is up to date. Please don't use git pull instead of the above fetch / rebase . git pull does a merge, which leaves merge commits. These make the commit history messy and violate the principle that commits ought to be individually understandable and useful.","title":"Create &amp; Rebase Your Feature Branch"},{"location":"contributors/github_workflow/#commit-push","text":"Commit and sign your changes: git commit --signoff The commit message should have a short title as first line, an empty line and then a longer description that explains why the change was made, unless it is obvious. You can go back and edit/build/test some more, then commit --amend in a few cycles. When ready, push your changes to your fork's repository: git push --set-upstream my_fork my_feature_branch","title":"Commit &amp; Push"},{"location":"contributors/github_workflow/#open-a-pull-request","text":"Github Docs","title":"Open a Pull Request"},{"location":"contributors/github_workflow/#get-a-code-review","text":"Once your pull request has been opened it will be assigned to one or more reviewers, and will go through a series of smoke tests. Commit changes made in response to review comments should be added to the same branch on your fork. Very small PRs are easy to review. Very large PRs are very difficult to review.","title":"Get a code review"},{"location":"contributors/github_workflow/#squashing-commits","text":"Commits on your branch should represent meaningful milestones or units of work. Small commits that contain typo fixes, rebases, review feedbacks, etc should be squashed. To do that, it's best to perform an interactive rebase :","title":"Squashing Commits"},{"location":"contributors/github_workflow/#push-your-final-changes","text":"Once done, you can push the final commits to your branch: git push --force You can run multiple iteration of rebase / push -f , if needed.","title":"Push Your Final Changes"},{"location":"contributors/overview/","text":"Contributing to k0s # Thank you for taking the time to make a contribution to k0s. The following document is a set of guidelines and instructions for contributing to k0s. When contributing to this repository, please consider first discussing the change you wish to make by opening an issue. Code of Conduct # Our code of conduct can be found in the link below. Please follow it in all your interactions with the project. Code Of Conduct Github Workflow # We Use Github Flow , so all code changes are tracked via Pull Requests. A detailed guide on the recommended workflow can be found below: Github Workflow Code Testing # All submitted PRs go through a set of tests and reviews. You can run most of these tests before a PR is submitted. In fact, we recommend it, because it will save on many possible review iterations and automated tests. The testing guidelines can be found here: Contributor's Guide to Testing License # By contributing, you agree that your contributions will be licensed as followed: All content residing under the \"docs/\" directory of this repository is licensed under \"Creative Commons Attribution Share Alike 4.0 International\" (CC-BY-SA-4.0). See docs/LICENCE for details. Content outside of the above mentioned directories or restrictions above is available under the \"Apache License 2.0\". Community # Some of you might have noticed we have official community blog hosted on Medium . If you are not yet following us, we'd like to invite you to do so now! Make sure to follow us on Twitter as well \ud83d\ude0a We have also agreed to share Slack with Lens IDE team. They are close friends of the k0s crew, and many of the Kubernetes users are using Lens anyway, so it felt very natural for us to share their Slack. You\u2019ll find us on dedicated k0s channel. Please join the k0s Slack channel to hear the latest news, discussions and provide your feedback!","title":"Overview"},{"location":"contributors/overview/#contributing-to-k0s","text":"Thank you for taking the time to make a contribution to k0s. The following document is a set of guidelines and instructions for contributing to k0s. When contributing to this repository, please consider first discussing the change you wish to make by opening an issue.","title":"Contributing to k0s"},{"location":"contributors/overview/#code-of-conduct","text":"Our code of conduct can be found in the link below. Please follow it in all your interactions with the project. Code Of Conduct","title":"Code of Conduct"},{"location":"contributors/overview/#github-workflow","text":"We Use Github Flow , so all code changes are tracked via Pull Requests. A detailed guide on the recommended workflow can be found below: Github Workflow","title":"Github Workflow"},{"location":"contributors/overview/#code-testing","text":"All submitted PRs go through a set of tests and reviews. You can run most of these tests before a PR is submitted. In fact, we recommend it, because it will save on many possible review iterations and automated tests. The testing guidelines can be found here: Contributor's Guide to Testing","title":"Code Testing"},{"location":"contributors/overview/#license","text":"By contributing, you agree that your contributions will be licensed as followed: All content residing under the \"docs/\" directory of this repository is licensed under \"Creative Commons Attribution Share Alike 4.0 International\" (CC-BY-SA-4.0). See docs/LICENCE for details. Content outside of the above mentioned directories or restrictions above is available under the \"Apache License 2.0\".","title":"License"},{"location":"contributors/overview/#community","text":"Some of you might have noticed we have official community blog hosted on Medium . If you are not yet following us, we'd like to invite you to do so now! Make sure to follow us on Twitter as well \ud83d\ude0a We have also agreed to share Slack with Lens IDE team. They are close friends of the k0s crew, and many of the Kubernetes users are using Lens anyway, so it felt very natural for us to share their Slack. You\u2019ll find us on dedicated k0s channel. Please join the k0s Slack channel to hear the latest news, discussions and provide your feedback!","title":"Community"},{"location":"contributors/testing/","text":"Testing Your Code # k0s uses github actions to run automated tests on any PR, before merging. However, a PR will not be reviewed before all tests are green, so to save time and prevent your PR from going stale, it is best to test it before submitting the PR. Run Local Verifications # Please run the following style and formatting commands and fix/check-in any changes: Linting We use golangci-lint for style verification. In the repository's root directory, simply run: make lint Go fmt go fmt ./... Pre-submit Flight Checks In the repository root directory, make sure that: make build runs successfully. make check-basic runs successfully. make check-unit has no errors. make check-hacontrolplane runs successfully. Please note that this last test is prone to \"flakiness\", so it might fail on occasion. If it fails constantly, take a deeper look at your code to find the source of the problem. If you find that all tests passed, you may open a pull request upstream. Opening A Pull Request # Draft Mode # You may open a pull request in draft mode . All automated tests will still run against the PR, but the PR will not be assigned for review. Once a PR is ready for review, transition it from Draft mode, and code owners will be notified. Conformance Testing # Once a PR has been reviewed and all other tests have passed, a code owner will run a full end-to-end conformance test against the PR. This is usually the last step before merging. Pre-Requisites for PR Merge # In order for a PR to be merged, the following conditions should exist: The PR has passed all the automated tests (style, build & conformance tests). PR commits have been signed with the --signoff option. PR was reviewed and approved by a code owner. PR is rebased against upstream's main branch.","title":"Testing"},{"location":"contributors/testing/#testing-your-code","text":"k0s uses github actions to run automated tests on any PR, before merging. However, a PR will not be reviewed before all tests are green, so to save time and prevent your PR from going stale, it is best to test it before submitting the PR.","title":"Testing Your Code"},{"location":"contributors/testing/#run-local-verifications","text":"Please run the following style and formatting commands and fix/check-in any changes: Linting We use golangci-lint for style verification. In the repository's root directory, simply run: make lint Go fmt go fmt ./... Pre-submit Flight Checks In the repository root directory, make sure that: make build runs successfully. make check-basic runs successfully. make check-unit has no errors. make check-hacontrolplane runs successfully. Please note that this last test is prone to \"flakiness\", so it might fail on occasion. If it fails constantly, take a deeper look at your code to find the source of the problem. If you find that all tests passed, you may open a pull request upstream.","title":"Run Local Verifications"},{"location":"contributors/testing/#opening-a-pull-request","text":"","title":"Opening A Pull Request"},{"location":"examples/ambassador-ingress/","text":"Install Ambassador Gateway on k0s # You can configure k0s with the Ambassador API Gateway and a MetalLB service loadbalancer . To do this you leverage Helm's extensible bootstrapping functionality to add the correct extensions to the k0s.yaml file during cluster configuration. Use Docker for non-native k0s platforms # With Docker you can run k0s on platforms that the distribution does not natively support (refer to Run k0s in Docker ). Skip this section if you are on a platform that k0s natively supports. As you need to create a custom configuration file to install Ambassador Gateway, you will first need to map that file into the k0s container and to expose the ports Ambassador needs for outside access. Run k0s under Docker: docker run -d --name k0s --hostname k0s --privileged -v /var/lib/k0s -p 6443 :6443 docker.io/k0sproject/k0s:latest Export the default k0s configuration file: docker exec k0s k0s default-config > k0s.yaml Export the cluster config, so you can access it using kubectl: docker exec k0s cat /var/lib/k0s/pki/admin.conf > k0s-cluster.conf export KUBECONFIG = $KUBECONFIG :<absolute path to k0s-cluster.conf> Configure k0s.yaml for Ambassador Gateway # Open the k0s.yml file and append the following extensions at the end: extensions : helm : repositories : - name : datawire url : https://www.getambassador.io - name : bitnami url : https://charts.bitnami.com/bitnami charts : - name : ambassador chartname : datawire/ambassador version : \"6.5.13\" namespace : ambassador values : |2 service: externalIPs: - 172.17.0.2 - name : metallb chartname : bitnami/metallb version : \"1.0.1\" namespace : default values : |2 configInline: address-pools: - name: generic-cluster-pool protocol: layer2 addresses: - 172.17.0.2 Note : It may be necessary to replace the 172.17.0.2 IP with your local IP address. This action adds both Ambassador and MetalLB (required for LoadBalancers) with the corresponding repositories and (minimal) configurations. Be aware that the provided example illustrates the use of your local network and that you will want to provide a range of IPs for MetalLB that are addressable on your LAN to access these services from anywhere on your network. Stop/remove your k0s container: docker stop k0s docker rm k0s Retart your k0s container, this time with additional ports and the above config file mapped into it: docker run --name k0s --hostname k0s --privileged -v /var/lib/k0s -v <path to k0s.yaml file>:/k0s.yaml -p 6443 :6443 -p 80 :80 -p 443 :443 -p 8080 :8080 docker.io/k0sproject/k0s:latest After some time, you will be able to list the Ambassador Services: kubectl get services -n ambassador Output : NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE ambassador-1611224811 LoadBalancer 10 .99.84.151 172 .17.0.2 80 :30327/TCP,443:30355/TCP 2m11s ambassador-1611224811-admin ClusterIP 10 .96.79.130 <none> 8877 /TCP 2m11s ambassador-1611224811-redis ClusterIP 10 .110.33.229 <none> 6379 /TCP 2m11s Install the Ambassador edgectl tool and run the login command: edgectl login --namespace = ambassador localhost Your browser will open and deeliver you to the Ambassador Console . Deploy / Map a Service # Create a YAML file for the service (for example purposes, create a Swagger Petstore service using a petstore.YAML file): --- apiVersion : v1 kind : Service metadata : name : petstore namespace : ambassador spec : ports : - name : http port : 80 targetPort : 8080 selector : app : petstore --- apiVersion : apps/v1 kind : Deployment metadata : name : petstore namespace : ambassador spec : replicas : 1 selector : matchLabels : app : petstore strategy : type : RollingUpdate template : metadata : labels : app : petstore spec : containers : - name : petstore-backend image : docker.io/swaggerapi/petstore3:unstable ports : - name : http containerPort : 8080 --- apiVersion : getambassador.io/v2 kind : Mapping metadata : name : petstore namespace : ambassador spec : prefix : /petstore/ service : petstore Apply the YAML file: kubectl apply -f petstore.yaml Output : service/petstore created deployment.apps/petstore created mapping.getambassador.io/petstore created Validate that the service is running. In the terminal using curl: curl -k 'https://localhost/petstore/api/v3/pet/findByStatus?status=available' Output : [{ \"id\" : 1 , \"category\" :{ \"id\" : 2 , \"name\" : \"Cats\" }, \"name\" : \"Cat 1\" , \"photoUrls\" :[ \"url1\" , \"url2\" ], \"tags\" :[{ \"id\" : 1 , \"name\" : \"tag1\" },{ \"id\" : 2 , \"name\" : \"tag2\" }], \"status\" : \"available\" },{ \"id\" : 2 , \"category\" :{ \"id\" : 2 , \"name\" : \"Cats\" }, \"name\" : \"Cat 2\" , \"photoUrls\" :[ \"url1\" , \"url2\" ], \"tags\" :[{ \"id\" : 1 , \"name\" : \"tag2\" },{ \"id\" : 2 , \"name\" : \"tag3\" }], \"status\" : \"available\" },{ \"id\" : 4 , \"category\" :{ \"id\" : 1 , \"name\" : \"Dogs\" }, \"name\" : \"Dog 1\" , \"photoUrls\" :[ \"url1\" , \"url2\" ], \"tags\" :[{ \"id\" : 1 , \"name\" : \"tag1\" },{ \"id\" : 2 , \"name\" : \"tag2\" }], \"status\" : \"available\" },{ \"id\" : 7 , \"category\" :{ \"id\" : 4 , \"name\" : \"Lions\" }, \"name\" : \"Lion 1\" , \"photoUrls\" :[ \"url1\" , \"url2\" ], \"tags\" :[{ \"id\" : 1 , \"name\" : \"tag1\" },{ \"id\" : 2 , \"name\" : \"tag2\" }], \"status\" : \"available\" },{ \"id\" : 8 , \"category\" :{ \"id\" : 4 , \"name\" : \"Lions\" }, \"name\" : \"Lion 2\" , \"photoUrls\" :[ \"url1\" , \"url2\" ], \"tags\" :[{ \"id\" : 1 , \"name\" : \"tag2\" },{ \"id\" : 2 , \"name\" : \"tag3\" }], \"status\" : \"available\" },{ \"id\" : 9 , \"category\" :{ \"id\" : 4 , \"name\" : \"Lions\" }, \"name\" : \"Lion 3\" , \"photoUrls\" :[ \"url1\" , \"url2\" ], \"tags\" :[{ \"id\" : 1 , \"name\" : \"tag3\" },{ \"id\" : 2 , \"name\" : \"tag4\" }], \"status\" : \"available\" },{ \"id\" : 10 , \"category\" :{ \"id\" : 3 , \"name\" : \"Rabbits\" }, \"name\" : \"Rabbit 1\" , \"photoUrls\" :[ \"url1\" , \"url2\" ], \"tags\" :[{ \"id\" : 1 , \"name\" : \"tag3\" },{ \"id\" : 2 , \"name\" : \"tag4\" }], \"status\" : \"available\" }] Or by way of your browser: Open https://localhost/petstore/ in your browser and change the URL in the field at the top of the page to https://localhost/petstore/api/v3/openapi.json (as it is mapped to the /petstore prefix) and click Explore . Navigate to the Mappings area in the Ambassador Console to view the corresponding PetStore mapping as configured.","title":"Ambassador Gateway"},{"location":"examples/ambassador-ingress/#install-ambassador-gateway-on-k0s","text":"You can configure k0s with the Ambassador API Gateway and a MetalLB service loadbalancer . To do this you leverage Helm's extensible bootstrapping functionality to add the correct extensions to the k0s.yaml file during cluster configuration.","title":"Install Ambassador Gateway on k0s"},{"location":"examples/ambassador-ingress/#use-docker-for-non-native-k0s-platforms","text":"With Docker you can run k0s on platforms that the distribution does not natively support (refer to Run k0s in Docker ). Skip this section if you are on a platform that k0s natively supports. As you need to create a custom configuration file to install Ambassador Gateway, you will first need to map that file into the k0s container and to expose the ports Ambassador needs for outside access. Run k0s under Docker: docker run -d --name k0s --hostname k0s --privileged -v /var/lib/k0s -p 6443 :6443 docker.io/k0sproject/k0s:latest Export the default k0s configuration file: docker exec k0s k0s default-config > k0s.yaml Export the cluster config, so you can access it using kubectl: docker exec k0s cat /var/lib/k0s/pki/admin.conf > k0s-cluster.conf export KUBECONFIG = $KUBECONFIG :<absolute path to k0s-cluster.conf>","title":"Use Docker for non-native k0s platforms"},{"location":"examples/ambassador-ingress/#configure-k0syaml-for-ambassador-gateway","text":"Open the k0s.yml file and append the following extensions at the end: extensions : helm : repositories : - name : datawire url : https://www.getambassador.io - name : bitnami url : https://charts.bitnami.com/bitnami charts : - name : ambassador chartname : datawire/ambassador version : \"6.5.13\" namespace : ambassador values : |2 service: externalIPs: - 172.17.0.2 - name : metallb chartname : bitnami/metallb version : \"1.0.1\" namespace : default values : |2 configInline: address-pools: - name: generic-cluster-pool protocol: layer2 addresses: - 172.17.0.2 Note : It may be necessary to replace the 172.17.0.2 IP with your local IP address. This action adds both Ambassador and MetalLB (required for LoadBalancers) with the corresponding repositories and (minimal) configurations. Be aware that the provided example illustrates the use of your local network and that you will want to provide a range of IPs for MetalLB that are addressable on your LAN to access these services from anywhere on your network. Stop/remove your k0s container: docker stop k0s docker rm k0s Retart your k0s container, this time with additional ports and the above config file mapped into it: docker run --name k0s --hostname k0s --privileged -v /var/lib/k0s -v <path to k0s.yaml file>:/k0s.yaml -p 6443 :6443 -p 80 :80 -p 443 :443 -p 8080 :8080 docker.io/k0sproject/k0s:latest After some time, you will be able to list the Ambassador Services: kubectl get services -n ambassador Output : NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE ambassador-1611224811 LoadBalancer 10 .99.84.151 172 .17.0.2 80 :30327/TCP,443:30355/TCP 2m11s ambassador-1611224811-admin ClusterIP 10 .96.79.130 <none> 8877 /TCP 2m11s ambassador-1611224811-redis ClusterIP 10 .110.33.229 <none> 6379 /TCP 2m11s Install the Ambassador edgectl tool and run the login command: edgectl login --namespace = ambassador localhost Your browser will open and deeliver you to the Ambassador Console .","title":"Configure k0s.yaml for Ambassador Gateway"},{"location":"examples/ambassador-ingress/#deploy-map-a-service","text":"Create a YAML file for the service (for example purposes, create a Swagger Petstore service using a petstore.YAML file): --- apiVersion : v1 kind : Service metadata : name : petstore namespace : ambassador spec : ports : - name : http port : 80 targetPort : 8080 selector : app : petstore --- apiVersion : apps/v1 kind : Deployment metadata : name : petstore namespace : ambassador spec : replicas : 1 selector : matchLabels : app : petstore strategy : type : RollingUpdate template : metadata : labels : app : petstore spec : containers : - name : petstore-backend image : docker.io/swaggerapi/petstore3:unstable ports : - name : http containerPort : 8080 --- apiVersion : getambassador.io/v2 kind : Mapping metadata : name : petstore namespace : ambassador spec : prefix : /petstore/ service : petstore Apply the YAML file: kubectl apply -f petstore.yaml Output : service/petstore created deployment.apps/petstore created mapping.getambassador.io/petstore created Validate that the service is running. In the terminal using curl: curl -k 'https://localhost/petstore/api/v3/pet/findByStatus?status=available' Output : [{ \"id\" : 1 , \"category\" :{ \"id\" : 2 , \"name\" : \"Cats\" }, \"name\" : \"Cat 1\" , \"photoUrls\" :[ \"url1\" , \"url2\" ], \"tags\" :[{ \"id\" : 1 , \"name\" : \"tag1\" },{ \"id\" : 2 , \"name\" : \"tag2\" }], \"status\" : \"available\" },{ \"id\" : 2 , \"category\" :{ \"id\" : 2 , \"name\" : \"Cats\" }, \"name\" : \"Cat 2\" , \"photoUrls\" :[ \"url1\" , \"url2\" ], \"tags\" :[{ \"id\" : 1 , \"name\" : \"tag2\" },{ \"id\" : 2 , \"name\" : \"tag3\" }], \"status\" : \"available\" },{ \"id\" : 4 , \"category\" :{ \"id\" : 1 , \"name\" : \"Dogs\" }, \"name\" : \"Dog 1\" , \"photoUrls\" :[ \"url1\" , \"url2\" ], \"tags\" :[{ \"id\" : 1 , \"name\" : \"tag1\" },{ \"id\" : 2 , \"name\" : \"tag2\" }], \"status\" : \"available\" },{ \"id\" : 7 , \"category\" :{ \"id\" : 4 , \"name\" : \"Lions\" }, \"name\" : \"Lion 1\" , \"photoUrls\" :[ \"url1\" , \"url2\" ], \"tags\" :[{ \"id\" : 1 , \"name\" : \"tag1\" },{ \"id\" : 2 , \"name\" : \"tag2\" }], \"status\" : \"available\" },{ \"id\" : 8 , \"category\" :{ \"id\" : 4 , \"name\" : \"Lions\" }, \"name\" : \"Lion 2\" , \"photoUrls\" :[ \"url1\" , \"url2\" ], \"tags\" :[{ \"id\" : 1 , \"name\" : \"tag2\" },{ \"id\" : 2 , \"name\" : \"tag3\" }], \"status\" : \"available\" },{ \"id\" : 9 , \"category\" :{ \"id\" : 4 , \"name\" : \"Lions\" }, \"name\" : \"Lion 3\" , \"photoUrls\" :[ \"url1\" , \"url2\" ], \"tags\" :[{ \"id\" : 1 , \"name\" : \"tag3\" },{ \"id\" : 2 , \"name\" : \"tag4\" }], \"status\" : \"available\" },{ \"id\" : 10 , \"category\" :{ \"id\" : 3 , \"name\" : \"Rabbits\" }, \"name\" : \"Rabbit 1\" , \"photoUrls\" :[ \"url1\" , \"url2\" ], \"tags\" :[{ \"id\" : 1 , \"name\" : \"tag3\" },{ \"id\" : 2 , \"name\" : \"tag4\" }], \"status\" : \"available\" }] Or by way of your browser: Open https://localhost/petstore/ in your browser and change the URL in the field at the top of the page to https://localhost/petstore/api/v3/openapi.json (as it is mapped to the /petstore prefix) and click Explore . Navigate to the Mappings area in the Ambassador Console to view the corresponding PetStore mapping as configured.","title":"Deploy / Map a Service"},{"location":"examples/ansible-playbook/","text":"Creating a cluster with an Ansible Playbook # Ansible is a popular infrastructure-as-code tool that can use to automate tasks for the purpose of achieving the desired state in a system. With Ansible (and the k0s-Ansible playbook) you can quickly install a multi-node Kubernetes Cluster. Note : Before using Ansible to create a cluster, you should have a general understanding of Ansible (refer to the official Ansible User Guide . Prerequisites # You will require the following tools to install k0s on local virtual machines: Tool Detail multipass A lightweight VM manager that uses KVM on Linux, Hyper-V on Windows, and hypervisor.framework on macOS. Installation information ansible An infrastructure as code tool. Installation Guide kubectl Command line tool for running commands against Kubernetes clusters. Kubernetes Install Tools Create the cluster # Download k0s-ansible Clone the k0s-ansible repository on your local machine: git clone https://github.com/movd/k0s-ansible.git cd k0s-ansible Create virtual machines Note : Though multipass is the VM manager in use here, there is no interdependence. Create a number of virtual machines. For the automation to work, each instance must have passwordless SSH access. To achieve this, provision each instance with a cloud-init manifest that imports your current users' public SSH key and into a user k0s (refer to the bash script below). This creates 7 virtual machines: $ ./tools/multipass_create_instances.sh 7 Create cloud-init to import ssh key... [ 1 /7 ] Creating instance k0s-1 with multipass... Launched: k0s-1 [ 2 /7 ] Creating instance k0s-2 with multipass... Launched: k0s-2 [ 3 /7 ] Creating instance k0s-3 with multipass... Launched: k0s-3 [ 4 /7 ] Creating instance k0s-4 with multipass... Launched: k0s-4 [ 5 /7 ] Creating instance k0s-5 with multipass... Launched: k0s-5 [ 6 /7 ] Creating instance k0s-6 with multipass... Launched: k0s-6 [ 7 /7 ] Creating instance k0s-7 with multipass... Launched: k0s-7 Name State IPv4 Image k0s-1 Running 192 .168.64.32 Ubuntu 20 .04 LTS k0s-2 Running 192 .168.64.33 Ubuntu 20 .04 LTS k0s-3 Running 192 .168.64.56 Ubuntu 20 .04 LTS k0s-4 Running 192 .168.64.57 Ubuntu 20 .04 LTS k0s-5 Running 192 .168.64.58 Ubuntu 20 .04 LTS k0s-6 Running 192 .168.64.60 Ubuntu 20 .04 LTS k0s-7 Running 192 .168.64.61 Ubuntu 20 .04 LTS Create Ansible inventory Copy the sample to create the inventory directory: cp -rfp inventory/sample inventory/multipass Create the inventory. Assign the virtual machines to the different host groups, as required by the playbook logic. Host group Detail initial_controller Must contain a single node that creates the worker and controller tokens needed by the other nodes controller Can contain nodes that, together with the host from initial_controller , form a highly available isolated control plane worker Must contain at least one node, to allow for the deployment of Kubernetes objects Fill in inventory/multipass/inventory.yml . This can be done by direct entry using the metadata provided by multipass list, , or you can use the following Python script multipass_generate_inventory.py : $ ./tools/multipass_generate_inventory.py Designate first three instances as control plane Created Ansible Inventory at: /Users/dev/k0s-ansible/tools/inventory.yml $ cp tools/inventory.yml inventory/multipass/inventory.yml Your inventory/multipass/inventory.yml should resemble the example below: --- all : children : initial_controller : hosts : k0s-1 : controller : hosts : k0s-2 : k0s-3 : worker : hosts : k0s-4 : k0s-5 : k0s-6 : k0s-7 : hosts : k0s-1 : ansible_host : 192.168.64.32 k0s-2 : ansible_host : 192.168.64.33 k0s-3 : ansible_host : 192.168.64.56 k0s-4 : ansible_host : 192.168.64.57 k0s-5 : ansible_host : 192.168.64.58 k0s-6 : ansible_host : 192.168.64.60 k0s-7 : ansible_host : 192.168.64.61 vars : ansible_user : k0s Test the virtual machine connections Run the following command to test the connection to your hosts: $ ansible -i inventory/multipass/inventory.yml -m ping k0s-4 | SUCCESS = > { \"ansible_facts\" : { \"discovered_interpreter_python\" : \"/usr/bin/python3\" } , \"changed\" : false, \"ping\" : \"pong\" } ... If the test result indicates success, you can proceed. Provision the cluster with Ansible Applying the playbook, k0s download and be set up on all nodes, tokens will be exchanged, and a kubeconfig will be dumped to your local deployment environment. $ ansible-playbook site.yml -i inventory/multipass/inventory.yml TASK [ k0s/initial_controller : print kubeconfig command ] ******************************************************* Tuesday 22 December 2020 17 :43:20 +0100 ( 0 :00:00.257 ) 0 :00:41.287 ****** ok: [ k0s-1 ] = > { \"msg\" : \"To use Cluster: export KUBECONFIG=/Users/dev/k0s-ansible/inventory/multipass/artifacts/k0s-kubeconfig.yml\" } ... PLAY RECAP ***************************************************************************************************** k0s-1 : ok = 21 changed = 11 unreachable = 0 failed = 0 skipped = 1 rescued = 0 ignored = 0 k0s-2 : ok = 10 changed = 5 unreachable = 0 failed = 0 skipped = 1 rescued = 0 ignored = 0 k0s-3 : ok = 10 changed = 5 unreachable = 0 failed = 0 skipped = 1 rescued = 0 ignored = 0 k0s-4 : ok = 9 changed = 5 unreachable = 0 failed = 0 skipped = 1 rescued = 0 ignored = 0 k0s-5 : ok = 9 changed = 5 unreachable = 0 failed = 0 skipped = 1 rescued = 0 ignored = 0 k0s-6 : ok = 9 changed = 5 unreachable = 0 failed = 0 skipped = 1 rescued = 0 ignored = 0 k0s-7 : ok = 9 changed = 5 unreachable = 0 failed = 0 skipped = 1 rescued = 0 ignored = 0 Tuesday 22 December 2020 17 :43:36 +0100 ( 0 :00:01.204 ) 0 :00:57.478 ****** =============================================================================== prereq : Install apt packages -------------------------------------------------------------------------- 22 .70s k0s/controller : Wait for k8s apiserver ----------------------------------------------------------------- 4 .30s k0s/initial_controller : Create worker join token ------------------------------------------------------- 3 .38s k0s/initial_controller : Wait for k8s apiserver --------------------------------------------------------- 3 .36s download : Download k0s binary k0s-v0.9.0-rc1-amd64 ----------------------------------------------------- 3 .11s Gathering Facts ----------------------------------------------------------------------------------------- 2 .85s Gathering Facts ----------------------------------------------------------------------------------------- 1 .95s prereq : Create k0s Directories ------------------------------------------------------------------------- 1 .53s k0s/worker : Enable and check k0s service --------------------------------------------------------------- 1 .20s prereq : Write the k0s config file ---------------------------------------------------------------------- 1 .09s k0s/initial_controller : Enable and check k0s service --------------------------------------------------- 0 .94s k0s/controller : Enable and check k0s service ----------------------------------------------------------- 0 .73s Gathering Facts ----------------------------------------------------------------------------------------- 0 .71s Gathering Facts ----------------------------------------------------------------------------------------- 0 .66s Gathering Facts ----------------------------------------------------------------------------------------- 0 .64s k0s/worker : Write the k0s token file on worker --------------------------------------------------------- 0 .64s k0s/worker : Copy k0s service file ---------------------------------------------------------------------- 0 .53s k0s/controller : Write the k0s token file on controller ------------------------------------------------- 0 .41s k0s/controller : Copy k0s service file ------------------------------------------------------------------ 0 .40s k0s/initial_controller : Copy k0s service file ---------------------------------------------------------- 0 .36s Use the cluster with kubectl # A kubeconfig was copied to your local machine while the playbook was running which you can use to gain access to your new Kubernetes cluster: $ export KUBECONFIG = /Users/dev/k0s-ansible/inventory/multipass/artifacts/k0s-kubeconfig.yml $ kubectl cluster-info Kubernetes control plane is running at https://192.168.64.32:6443 CoreDNS is running at https://192.168.64.32:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://192.168.64.32:6443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k0s-4 Ready <none> 21s v1.20.1-k0s1 192 .168.64.57 <none> Ubuntu 20 .04.1 LTS 5 .4.0-54-generic containerd://1.4.3 k0s-5 Ready <none> 21s v1.20.1-k0s1 192 .168.64.58 <none> Ubuntu 20 .04.1 LTS 5 .4.0-54-generic containerd://1.4.3 k0s-6 NotReady <none> 21s v1.20.1-k0s1 192 .168.64.60 <none> Ubuntu 20 .04.1 LTS 5 .4.0-54-generic containerd://1.4.3 k0s-7 NotReady <none> 21s v1.20.1-k0s1 192 .168.64.61 <none> Ubuntu 20 .04.1 LTS 5 .4.0-54-generic containerd://1.4.3 Note : The first three control plane nodes will not display, as the control plane is fully isolated. To check on the distributed etcd cluster, you can use ssh to securely log a controller node, or you can run the following ad-hoc command: $ ansible k0s-1 -a \"k0s etcd member-list -c /etc/k0s/k0s.yaml\" -i inventory/multipass/inventory.yml | tail -1 | jq { \"level\" : \"info\" , \"members\" : { \"k0s-1\" : \"https://192.168.64.32:2380\" , \"k0s-2\" : \"https://192.168.64.33:2380\" , \"k0s-3\" : \"https://192.168.64.56:2380\" } , \"msg\" : \"done\" , \"time\" : \"2020-12-23T00:21:22+01:00\" } Once all worker nodes are at Ready state you can use the cluster. You can test the cluster state by creating a simple nginx deployment. $ kubectl create deployment nginx --image = gcr.io/google-containers/nginx --replicas = 5 deployment.apps/nginx created $ kubectl expose deployment nginx --target-port = 80 --port = 8100 service/nginx exposed $ kubectl run hello-k0s --image = quay.io/prometheus/busybox --rm -it --restart = Never --command -- wget -qO- nginx:8100 <!DOCTYPE html> <html> <head> <title>Welcome to nginx on Debian!</title> ... pod \"hello-k0s\" deleted Note : k0s users are the developers of k0s-ansible. Please send your feedback, bug reports, and pull requests to github.com/movd/k0s-ansible ._","title":"Ansible Playbook"},{"location":"examples/ansible-playbook/#creating-a-cluster-with-an-ansible-playbook","text":"Ansible is a popular infrastructure-as-code tool that can use to automate tasks for the purpose of achieving the desired state in a system. With Ansible (and the k0s-Ansible playbook) you can quickly install a multi-node Kubernetes Cluster. Note : Before using Ansible to create a cluster, you should have a general understanding of Ansible (refer to the official Ansible User Guide .","title":"Creating a cluster with an Ansible Playbook"},{"location":"examples/ansible-playbook/#prerequisites","text":"You will require the following tools to install k0s on local virtual machines: Tool Detail multipass A lightweight VM manager that uses KVM on Linux, Hyper-V on Windows, and hypervisor.framework on macOS. Installation information ansible An infrastructure as code tool. Installation Guide kubectl Command line tool for running commands against Kubernetes clusters. Kubernetes Install Tools","title":"Prerequisites"},{"location":"examples/ansible-playbook/#create-the-cluster","text":"Download k0s-ansible Clone the k0s-ansible repository on your local machine: git clone https://github.com/movd/k0s-ansible.git cd k0s-ansible Create virtual machines Note : Though multipass is the VM manager in use here, there is no interdependence. Create a number of virtual machines. For the automation to work, each instance must have passwordless SSH access. To achieve this, provision each instance with a cloud-init manifest that imports your current users' public SSH key and into a user k0s (refer to the bash script below). This creates 7 virtual machines: $ ./tools/multipass_create_instances.sh 7 Create cloud-init to import ssh key... [ 1 /7 ] Creating instance k0s-1 with multipass... Launched: k0s-1 [ 2 /7 ] Creating instance k0s-2 with multipass... Launched: k0s-2 [ 3 /7 ] Creating instance k0s-3 with multipass... Launched: k0s-3 [ 4 /7 ] Creating instance k0s-4 with multipass... Launched: k0s-4 [ 5 /7 ] Creating instance k0s-5 with multipass... Launched: k0s-5 [ 6 /7 ] Creating instance k0s-6 with multipass... Launched: k0s-6 [ 7 /7 ] Creating instance k0s-7 with multipass... Launched: k0s-7 Name State IPv4 Image k0s-1 Running 192 .168.64.32 Ubuntu 20 .04 LTS k0s-2 Running 192 .168.64.33 Ubuntu 20 .04 LTS k0s-3 Running 192 .168.64.56 Ubuntu 20 .04 LTS k0s-4 Running 192 .168.64.57 Ubuntu 20 .04 LTS k0s-5 Running 192 .168.64.58 Ubuntu 20 .04 LTS k0s-6 Running 192 .168.64.60 Ubuntu 20 .04 LTS k0s-7 Running 192 .168.64.61 Ubuntu 20 .04 LTS Create Ansible inventory Copy the sample to create the inventory directory: cp -rfp inventory/sample inventory/multipass Create the inventory. Assign the virtual machines to the different host groups, as required by the playbook logic. Host group Detail initial_controller Must contain a single node that creates the worker and controller tokens needed by the other nodes controller Can contain nodes that, together with the host from initial_controller , form a highly available isolated control plane worker Must contain at least one node, to allow for the deployment of Kubernetes objects Fill in inventory/multipass/inventory.yml . This can be done by direct entry using the metadata provided by multipass list, , or you can use the following Python script multipass_generate_inventory.py : $ ./tools/multipass_generate_inventory.py Designate first three instances as control plane Created Ansible Inventory at: /Users/dev/k0s-ansible/tools/inventory.yml $ cp tools/inventory.yml inventory/multipass/inventory.yml Your inventory/multipass/inventory.yml should resemble the example below: --- all : children : initial_controller : hosts : k0s-1 : controller : hosts : k0s-2 : k0s-3 : worker : hosts : k0s-4 : k0s-5 : k0s-6 : k0s-7 : hosts : k0s-1 : ansible_host : 192.168.64.32 k0s-2 : ansible_host : 192.168.64.33 k0s-3 : ansible_host : 192.168.64.56 k0s-4 : ansible_host : 192.168.64.57 k0s-5 : ansible_host : 192.168.64.58 k0s-6 : ansible_host : 192.168.64.60 k0s-7 : ansible_host : 192.168.64.61 vars : ansible_user : k0s Test the virtual machine connections Run the following command to test the connection to your hosts: $ ansible -i inventory/multipass/inventory.yml -m ping k0s-4 | SUCCESS = > { \"ansible_facts\" : { \"discovered_interpreter_python\" : \"/usr/bin/python3\" } , \"changed\" : false, \"ping\" : \"pong\" } ... If the test result indicates success, you can proceed. Provision the cluster with Ansible Applying the playbook, k0s download and be set up on all nodes, tokens will be exchanged, and a kubeconfig will be dumped to your local deployment environment. $ ansible-playbook site.yml -i inventory/multipass/inventory.yml TASK [ k0s/initial_controller : print kubeconfig command ] ******************************************************* Tuesday 22 December 2020 17 :43:20 +0100 ( 0 :00:00.257 ) 0 :00:41.287 ****** ok: [ k0s-1 ] = > { \"msg\" : \"To use Cluster: export KUBECONFIG=/Users/dev/k0s-ansible/inventory/multipass/artifacts/k0s-kubeconfig.yml\" } ... PLAY RECAP ***************************************************************************************************** k0s-1 : ok = 21 changed = 11 unreachable = 0 failed = 0 skipped = 1 rescued = 0 ignored = 0 k0s-2 : ok = 10 changed = 5 unreachable = 0 failed = 0 skipped = 1 rescued = 0 ignored = 0 k0s-3 : ok = 10 changed = 5 unreachable = 0 failed = 0 skipped = 1 rescued = 0 ignored = 0 k0s-4 : ok = 9 changed = 5 unreachable = 0 failed = 0 skipped = 1 rescued = 0 ignored = 0 k0s-5 : ok = 9 changed = 5 unreachable = 0 failed = 0 skipped = 1 rescued = 0 ignored = 0 k0s-6 : ok = 9 changed = 5 unreachable = 0 failed = 0 skipped = 1 rescued = 0 ignored = 0 k0s-7 : ok = 9 changed = 5 unreachable = 0 failed = 0 skipped = 1 rescued = 0 ignored = 0 Tuesday 22 December 2020 17 :43:36 +0100 ( 0 :00:01.204 ) 0 :00:57.478 ****** =============================================================================== prereq : Install apt packages -------------------------------------------------------------------------- 22 .70s k0s/controller : Wait for k8s apiserver ----------------------------------------------------------------- 4 .30s k0s/initial_controller : Create worker join token ------------------------------------------------------- 3 .38s k0s/initial_controller : Wait for k8s apiserver --------------------------------------------------------- 3 .36s download : Download k0s binary k0s-v0.9.0-rc1-amd64 ----------------------------------------------------- 3 .11s Gathering Facts ----------------------------------------------------------------------------------------- 2 .85s Gathering Facts ----------------------------------------------------------------------------------------- 1 .95s prereq : Create k0s Directories ------------------------------------------------------------------------- 1 .53s k0s/worker : Enable and check k0s service --------------------------------------------------------------- 1 .20s prereq : Write the k0s config file ---------------------------------------------------------------------- 1 .09s k0s/initial_controller : Enable and check k0s service --------------------------------------------------- 0 .94s k0s/controller : Enable and check k0s service ----------------------------------------------------------- 0 .73s Gathering Facts ----------------------------------------------------------------------------------------- 0 .71s Gathering Facts ----------------------------------------------------------------------------------------- 0 .66s Gathering Facts ----------------------------------------------------------------------------------------- 0 .64s k0s/worker : Write the k0s token file on worker --------------------------------------------------------- 0 .64s k0s/worker : Copy k0s service file ---------------------------------------------------------------------- 0 .53s k0s/controller : Write the k0s token file on controller ------------------------------------------------- 0 .41s k0s/controller : Copy k0s service file ------------------------------------------------------------------ 0 .40s k0s/initial_controller : Copy k0s service file ---------------------------------------------------------- 0 .36s","title":"Create the cluster"},{"location":"examples/ansible-playbook/#use-the-cluster-with-kubectl","text":"A kubeconfig was copied to your local machine while the playbook was running which you can use to gain access to your new Kubernetes cluster: $ export KUBECONFIG = /Users/dev/k0s-ansible/inventory/multipass/artifacts/k0s-kubeconfig.yml $ kubectl cluster-info Kubernetes control plane is running at https://192.168.64.32:6443 CoreDNS is running at https://192.168.64.32:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://192.168.64.32:6443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k0s-4 Ready <none> 21s v1.20.1-k0s1 192 .168.64.57 <none> Ubuntu 20 .04.1 LTS 5 .4.0-54-generic containerd://1.4.3 k0s-5 Ready <none> 21s v1.20.1-k0s1 192 .168.64.58 <none> Ubuntu 20 .04.1 LTS 5 .4.0-54-generic containerd://1.4.3 k0s-6 NotReady <none> 21s v1.20.1-k0s1 192 .168.64.60 <none> Ubuntu 20 .04.1 LTS 5 .4.0-54-generic containerd://1.4.3 k0s-7 NotReady <none> 21s v1.20.1-k0s1 192 .168.64.61 <none> Ubuntu 20 .04.1 LTS 5 .4.0-54-generic containerd://1.4.3 Note : The first three control plane nodes will not display, as the control plane is fully isolated. To check on the distributed etcd cluster, you can use ssh to securely log a controller node, or you can run the following ad-hoc command: $ ansible k0s-1 -a \"k0s etcd member-list -c /etc/k0s/k0s.yaml\" -i inventory/multipass/inventory.yml | tail -1 | jq { \"level\" : \"info\" , \"members\" : { \"k0s-1\" : \"https://192.168.64.32:2380\" , \"k0s-2\" : \"https://192.168.64.33:2380\" , \"k0s-3\" : \"https://192.168.64.56:2380\" } , \"msg\" : \"done\" , \"time\" : \"2020-12-23T00:21:22+01:00\" } Once all worker nodes are at Ready state you can use the cluster. You can test the cluster state by creating a simple nginx deployment. $ kubectl create deployment nginx --image = gcr.io/google-containers/nginx --replicas = 5 deployment.apps/nginx created $ kubectl expose deployment nginx --target-port = 80 --port = 8100 service/nginx exposed $ kubectl run hello-k0s --image = quay.io/prometheus/busybox --rm -it --restart = Never --command -- wget -qO- nginx:8100 <!DOCTYPE html> <html> <head> <title>Welcome to nginx on Debian!</title> ... pod \"hello-k0s\" deleted Note : k0s users are the developers of k0s-ansible. Please send your feedback, bug reports, and pull requests to github.com/movd/k0s-ansible ._","title":"Use the cluster with kubectl"},{"location":"examples/rook-ceph/","text":"Installing Ceph Storage with Rook # In this tutorial you'll create a Ceph storage for k0s. Ceph is a highly scalable, distributed storage solution. It offers object, block, and file storage, and it's designed to run on any common hardware. Ceph implements data replication into multiple volumes that makes it fault-tolerant. Another clear advantage of Ceph in Kubernetes is the dynamic provisioning. This means that applications just need to request the storage (persistent volume claim) and Ceph will automatically provision the requested storage without a manual creation of the persistent volume each time. Unfortunately, the Ceph deployment as such can be considered a bit complex. To make the deployment easier, we'll use Rook operator. Rook is a CNCF project and it's dedicated to storage orchestration. Rook supports several storage solutions, but in this tutorial we will use it to manage Ceph. This tutorial uses three worker nodes and one controller. It's possible to use less nodes, but using three worker nodes makes it a good example for deploying a high-available storage cluster. We use external storage partitions, which are assigned to the worker nodes to be used by Ceph. After the Ceph deployment we'll deploy a sample application (MongoDB) to use the storage in practice. Prerequisites # Linux OS GitHub access AWS account Terraform Deployment steps # 1. Preparations # In this example we'll use Terraform to create four Ubuntu VMs on AWS. Using Terraform makes the VM deployment fast and repeatable. You can avoid manually setting up everything in the AWS GUI. Moreover, when you have finished with the tutorial, it's very easy to tear down the VMs with Terraform (with one command). However, you can set up the nodes in many different ways and it doesn't make a difference in the following steps. We will use k0sctl to create the k0s cluster. k0sctl repo also includes a ready-made Terraform configuration to create the VMs on AWS. We'll use that. Let's start be cloning the k0sctl repo. git clone git@github.com:k0sproject/k0sctl.git Take a look at the Terraform files cd k0sctl/examples/aws-tf ls -l Open variables.tf and set the number of controller and worker nodes like this: variable \"cluster_name\" { type = string default = \"k0sctl\" } variable \"controller_count\" { type = number default = 1 } variable \"worker_count\" { type = number default = 3 } variable \"cluster_flavor\" { type = string default = \"t3.small\" } You can also configure a different name to your cluster and change the default VM type. t3.small (2 vCPUs, 2 GB RAM) runs just fine for this tutorial. 2. Create the VMs # For AWS, you need an account. Terraform will use the following environment variable: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN . You can easily copy-paste them from the AWS portal. For more information, see the AWS documentation . When the environment variables are set, you can proceed with Terraform and deploy the VMs. terraform init terraform apply If you decide to create the VMs manually using AWS GUI, you need to disable source / destination checking. This needs to be disbled always for multi-node Kubernetes clusters in order to get the node-to-node communication working due to Network Address Translation. For Terraform this is already taken care of in the default configuration. 3. Create and attach the volumes # Ceph requires one of the following storage options for storing the data: Raw devices (no partitions or formatted filesystems) Raw partitions (no formatted filesystem) PVs available from a storage class in block mode We will be using raw partititions (AWS EBS volumes), which can be easily attached to the worker node VMs. They are automatically detected by Ceph with its default configuration. Deploy AWS EBS volumes, one for each worker node. You can manually create three EBS volumes (for example 10 GB each) using the AWS GUI and attach those to your worker nodes. Formatting shouldn't be done. Instead, Ceph handles that part automatically. After you have attached the EBS volumes to the worker nodes, log in to one of the workers and check the available block devices: $ lsblk -f NAME FSTYPE LABEL UUID FSAVAIL FSUSE% MOUNTPOINT loop0 squashfs 0 100 % /snap/amazon-ssm-agent/3552 loop1 squashfs 0 100 % /snap/core18/1997 loop2 squashfs 0 100 % /snap/snapd/11588 loop3 squashfs 0 100 % /snap/lxd/19647 nvme0n1 \u2514\u2500nvme0n1p1 ext4 cloudimg-rootfs e8070c31-bfee-4314-a151-d1332dc23486 5 .1G 33 % / nvme1n1 The last line (nvme1n1) in this example printout corresponds to the attached EBS volume. Note that it doesn't have any filesystem (FSTYPE is empty). This meets the Ceph storage requirements and you are good to proceed. 4. Install k0s using k0sctl # You can use terraform to automatically output a config file for k0sctl with the ip addresses and access details. terraform output -raw k0s_cluster > k0sctl.yaml After that deploying k0s becomes very easy with the ready-made configuration. k0sctl apply --config k0sctl.yaml It might take around 2-3 minutes for k0sctl to connect each node, install k0s and connect the nodes together to form a cluster. 5. Access k0s cluster # To access your new cluster remotely, you can use k0sctl to fetch kubeconfig and use that with kubectl or Lens. k0sctl kubeconfig --config k0sctl.yaml > kubeconfig export KUBECONFIG = $PWD /kubeconfig kubectl get nodes The other option is to login to your controller node and use the k0s in-built kubectl to access the cluster. Then you don't need to worry about kubeconfig (k0s takes care of that automatically). ssh -i aws.pem <username>@<ip-address> sudo k0s kubectl get nodes 6. Deploy Rook # To get started with Rook, let's first clone the Rook GitHub repo: git clone --single-branch --branch v1.6.0 https://github.com/rook/rook.git cd rook/cluster/examples/kubernetes/ceph We will use mostly the default Rook configuration. However, k0s kubelet drectory must be configured in operator.yaml like this ROOK_CSI_KUBELET_DIR_PATH : \"/var/lib/k0s/kubelet\" To create the resources, which are needed by the Rook\u2019s Ceph operator, run kubectl apply -f crds.yaml -f common.yaml -f operator.yaml Now you should see the operator running. Check them with kubectl get pods -n rook-ceph 7. Deploy Ceph Cluster # Then you can proceed to create a Ceph cluster. Ceph will use the three EBS volumes attached to the worker nodes: kubectl apply -f cluster.yaml It takes some minutes to prepare the volumes and create the cluster. Once this is completed you should see the following output: $ kubectl get pods -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-nhxc8 3 /3 Running 0 2m48s csi-cephfsplugin-provisioner-db45f85f5-ldhjp 6 /6 Running 0 2m48s csi-cephfsplugin-provisioner-db45f85f5-sxfm8 6 /6 Running 0 2m48s csi-cephfsplugin-tj2bh 3 /3 Running 0 2m48s csi-cephfsplugin-z2rrl 3 /3 Running 0 2m48s csi-rbdplugin-5q7gq 3 /3 Running 0 2m49s csi-rbdplugin-8sfpd 3 /3 Running 0 2m49s csi-rbdplugin-f2xdz 3 /3 Running 0 2m49s csi-rbdplugin-provisioner-d85cbdb48-g6vck 6 /6 Running 0 2m49s csi-rbdplugin-provisioner-d85cbdb48-zpmvr 6 /6 Running 0 2m49s rook-ceph-crashcollector-ip-172-31-0-76-64cb4c7775-m55x2 1 /1 Running 0 45s rook-ceph-crashcollector-ip-172-31-13-183-654b46588d-djqsd 1 /1 Running 0 2m57s rook-ceph-crashcollector-ip-172-31-15-5-67b68698f-gcjb7 1 /1 Running 0 2m46s rook-ceph-mgr-a-5ffc65c874-8pxgv 1 /1 Running 0 58s rook-ceph-mon-a-ffcd85c5f-z89tb 1 /1 Running 0 2m59s rook-ceph-mon-b-fc8f59464-lgczk 1 /1 Running 0 2m46s rook-ceph-mon-c-69bd87b558-kl4nl 1 /1 Running 0 91s rook-ceph-operator-54cf7487d4-pl66p 1 /1 Running 0 4m57s rook-ceph-osd-0-dd4fd8f6-g6s9m 1 /1 Running 0 48s rook-ceph-osd-1-7c478c49c4-gkqml 1 /1 Running 0 47s rook-ceph-osd-2-5b887995fd-26492 1 /1 Running 0 46s rook-ceph-osd-prepare-ip-172-31-0-76-6b5fw 0 /1 Completed 0 28s rook-ceph-osd-prepare-ip-172-31-13-183-cnkf9 0 /1 Completed 0 25s rook-ceph-osd-prepare-ip-172-31-15-5-qc6pt 0 /1 Completed 0 23s 8. Configure Ceph block storage # Before Ceph can provide storage to your cluster, you need to create a ReplicaPool and a StorageClass. In this example, we use the default configuration to create the block storage. kubectl apply -f ./csi/rbd/storageclass.yaml 9. Request storage # Create a new manifest file mongo-pvc.yaml with the following content: apiVersion : v1 kind : PersistentVolumeClaim metadata : name : mongo-pvc spec : storageClassName : rook-ceph-block accessModes : - ReadWriteOnce resources : requests : storage : 2Gi This will create Persistent Volume Claim (PVC) to request a 2 GB block storage from Ceph. Provioning will be done dynamically. You can define the block size freely as long as it fits to the available storage size. kubectl apply -f mongo-pvc.yaml You can now check the status of your PVC: kubectl get pvc When the PVC gets the requested volume reserved (bound), it should look like this: $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mongo-pvc Bound pvc-08337736-65dd-49d2-938c-8197a8871739 3Gi RWO rook-ceph-block 6s 10. Deploy an example application # Let's deploy a Mongo database to verify the Ceph storage. Create a new file mongo.yaml with the following content: apiVersion : apps/v1 kind : Deployment metadata : name : mongo spec : selector : matchLabels : app : mongo template : metadata : labels : app : mongo spec : containers : - image : mongo:4.0 name : mongo ports : - containerPort : 27017 name : mongo volumeMounts : - name : mongo-persistent-storage mountPath : /data/db volumes : - name : mongo-persistent-storage persistentVolumeClaim : claimName : mongo-pvc Deploy the database: kubectl apply -f mongo.yaml 11. Access the application # Open the MongoDB shell using the mongo pod: $ kubectl get pods NAME READY STATUS RESTARTS AGE mongo-b87cbd5cc-4wx8t 1 /1 Running 0 76s $ kubectl exec -it mongo-b87cbd5cc-4wx8t -- mongo Create a DB and insert some data: > use testDB switched to db testDB > db.testDB.insertOne( {name: \"abc\", number: 123 }) { \"acknowledged\" : true, \"insertedId\" : ObjectId(\"60815690a709d344f83b651d\") } > db.testDB.insertOne( {name: \"bcd\", number: 234 }) { \"acknowledged\" : true, \"insertedId\" : ObjectId(\"6081569da709d344f83b651e\") } Read the data: > db.getCollection(\"testDB\").find() { \"_id\" : ObjectId(\"60815690a709d344f83b651d\"), \"name\" : \"abc\", \"number\" : 123 } { \"_id\" : ObjectId(\"6081569da709d344f83b651e\"), \"name\" : \"bcd\", \"number\" : 234 } > You can also try to restart the mongo pod or restart the worker nodes to verity that the storage is persistent. 12. Clean-up # You can use Terraform to take down the VMs: terraform destroy Remember to delete the EBS volumes separately. Conclusions # You have now created a replicated Ceph storage for k0s. All you data is stored to multiple disks at the same time so you have a fault-tolerant solution. You also have enabled dynamic provisioning. Your applications can request the available storage without a manual creation of the persistent volumes each time. This was just one example to deploy distributed storage to k0s cluster using an operator. You can easily use different Kubernetes storage solutions with k0s.","title":"Ceph Storage with Rook"},{"location":"examples/rook-ceph/#installing-ceph-storage-with-rook","text":"In this tutorial you'll create a Ceph storage for k0s. Ceph is a highly scalable, distributed storage solution. It offers object, block, and file storage, and it's designed to run on any common hardware. Ceph implements data replication into multiple volumes that makes it fault-tolerant. Another clear advantage of Ceph in Kubernetes is the dynamic provisioning. This means that applications just need to request the storage (persistent volume claim) and Ceph will automatically provision the requested storage without a manual creation of the persistent volume each time. Unfortunately, the Ceph deployment as such can be considered a bit complex. To make the deployment easier, we'll use Rook operator. Rook is a CNCF project and it's dedicated to storage orchestration. Rook supports several storage solutions, but in this tutorial we will use it to manage Ceph. This tutorial uses three worker nodes and one controller. It's possible to use less nodes, but using three worker nodes makes it a good example for deploying a high-available storage cluster. We use external storage partitions, which are assigned to the worker nodes to be used by Ceph. After the Ceph deployment we'll deploy a sample application (MongoDB) to use the storage in practice.","title":"Installing Ceph Storage with Rook"},{"location":"examples/rook-ceph/#prerequisites","text":"Linux OS GitHub access AWS account Terraform","title":"Prerequisites"},{"location":"examples/rook-ceph/#deployment-steps","text":"","title":"Deployment steps"},{"location":"examples/rook-ceph/#1-preparations","text":"In this example we'll use Terraform to create four Ubuntu VMs on AWS. Using Terraform makes the VM deployment fast and repeatable. You can avoid manually setting up everything in the AWS GUI. Moreover, when you have finished with the tutorial, it's very easy to tear down the VMs with Terraform (with one command). However, you can set up the nodes in many different ways and it doesn't make a difference in the following steps. We will use k0sctl to create the k0s cluster. k0sctl repo also includes a ready-made Terraform configuration to create the VMs on AWS. We'll use that. Let's start be cloning the k0sctl repo. git clone git@github.com:k0sproject/k0sctl.git Take a look at the Terraform files cd k0sctl/examples/aws-tf ls -l Open variables.tf and set the number of controller and worker nodes like this: variable \"cluster_name\" { type = string default = \"k0sctl\" } variable \"controller_count\" { type = number default = 1 } variable \"worker_count\" { type = number default = 3 } variable \"cluster_flavor\" { type = string default = \"t3.small\" } You can also configure a different name to your cluster and change the default VM type. t3.small (2 vCPUs, 2 GB RAM) runs just fine for this tutorial.","title":"1. Preparations"},{"location":"examples/rook-ceph/#2-create-the-vms","text":"For AWS, you need an account. Terraform will use the following environment variable: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN . You can easily copy-paste them from the AWS portal. For more information, see the AWS documentation . When the environment variables are set, you can proceed with Terraform and deploy the VMs. terraform init terraform apply If you decide to create the VMs manually using AWS GUI, you need to disable source / destination checking. This needs to be disbled always for multi-node Kubernetes clusters in order to get the node-to-node communication working due to Network Address Translation. For Terraform this is already taken care of in the default configuration.","title":"2. Create the VMs"},{"location":"examples/rook-ceph/#3-create-and-attach-the-volumes","text":"Ceph requires one of the following storage options for storing the data: Raw devices (no partitions or formatted filesystems) Raw partitions (no formatted filesystem) PVs available from a storage class in block mode We will be using raw partititions (AWS EBS volumes), which can be easily attached to the worker node VMs. They are automatically detected by Ceph with its default configuration. Deploy AWS EBS volumes, one for each worker node. You can manually create three EBS volumes (for example 10 GB each) using the AWS GUI and attach those to your worker nodes. Formatting shouldn't be done. Instead, Ceph handles that part automatically. After you have attached the EBS volumes to the worker nodes, log in to one of the workers and check the available block devices: $ lsblk -f NAME FSTYPE LABEL UUID FSAVAIL FSUSE% MOUNTPOINT loop0 squashfs 0 100 % /snap/amazon-ssm-agent/3552 loop1 squashfs 0 100 % /snap/core18/1997 loop2 squashfs 0 100 % /snap/snapd/11588 loop3 squashfs 0 100 % /snap/lxd/19647 nvme0n1 \u2514\u2500nvme0n1p1 ext4 cloudimg-rootfs e8070c31-bfee-4314-a151-d1332dc23486 5 .1G 33 % / nvme1n1 The last line (nvme1n1) in this example printout corresponds to the attached EBS volume. Note that it doesn't have any filesystem (FSTYPE is empty). This meets the Ceph storage requirements and you are good to proceed.","title":"3. Create and attach the volumes"},{"location":"examples/rook-ceph/#4-install-k0s-using-k0sctl","text":"You can use terraform to automatically output a config file for k0sctl with the ip addresses and access details. terraform output -raw k0s_cluster > k0sctl.yaml After that deploying k0s becomes very easy with the ready-made configuration. k0sctl apply --config k0sctl.yaml It might take around 2-3 minutes for k0sctl to connect each node, install k0s and connect the nodes together to form a cluster.","title":"4. Install k0s using k0sctl"},{"location":"examples/rook-ceph/#5-access-k0s-cluster","text":"To access your new cluster remotely, you can use k0sctl to fetch kubeconfig and use that with kubectl or Lens. k0sctl kubeconfig --config k0sctl.yaml > kubeconfig export KUBECONFIG = $PWD /kubeconfig kubectl get nodes The other option is to login to your controller node and use the k0s in-built kubectl to access the cluster. Then you don't need to worry about kubeconfig (k0s takes care of that automatically). ssh -i aws.pem <username>@<ip-address> sudo k0s kubectl get nodes","title":"5. Access k0s cluster"},{"location":"examples/rook-ceph/#6-deploy-rook","text":"To get started with Rook, let's first clone the Rook GitHub repo: git clone --single-branch --branch v1.6.0 https://github.com/rook/rook.git cd rook/cluster/examples/kubernetes/ceph We will use mostly the default Rook configuration. However, k0s kubelet drectory must be configured in operator.yaml like this ROOK_CSI_KUBELET_DIR_PATH : \"/var/lib/k0s/kubelet\" To create the resources, which are needed by the Rook\u2019s Ceph operator, run kubectl apply -f crds.yaml -f common.yaml -f operator.yaml Now you should see the operator running. Check them with kubectl get pods -n rook-ceph","title":"6. Deploy Rook"},{"location":"examples/rook-ceph/#7-deploy-ceph-cluster","text":"Then you can proceed to create a Ceph cluster. Ceph will use the three EBS volumes attached to the worker nodes: kubectl apply -f cluster.yaml It takes some minutes to prepare the volumes and create the cluster. Once this is completed you should see the following output: $ kubectl get pods -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-nhxc8 3 /3 Running 0 2m48s csi-cephfsplugin-provisioner-db45f85f5-ldhjp 6 /6 Running 0 2m48s csi-cephfsplugin-provisioner-db45f85f5-sxfm8 6 /6 Running 0 2m48s csi-cephfsplugin-tj2bh 3 /3 Running 0 2m48s csi-cephfsplugin-z2rrl 3 /3 Running 0 2m48s csi-rbdplugin-5q7gq 3 /3 Running 0 2m49s csi-rbdplugin-8sfpd 3 /3 Running 0 2m49s csi-rbdplugin-f2xdz 3 /3 Running 0 2m49s csi-rbdplugin-provisioner-d85cbdb48-g6vck 6 /6 Running 0 2m49s csi-rbdplugin-provisioner-d85cbdb48-zpmvr 6 /6 Running 0 2m49s rook-ceph-crashcollector-ip-172-31-0-76-64cb4c7775-m55x2 1 /1 Running 0 45s rook-ceph-crashcollector-ip-172-31-13-183-654b46588d-djqsd 1 /1 Running 0 2m57s rook-ceph-crashcollector-ip-172-31-15-5-67b68698f-gcjb7 1 /1 Running 0 2m46s rook-ceph-mgr-a-5ffc65c874-8pxgv 1 /1 Running 0 58s rook-ceph-mon-a-ffcd85c5f-z89tb 1 /1 Running 0 2m59s rook-ceph-mon-b-fc8f59464-lgczk 1 /1 Running 0 2m46s rook-ceph-mon-c-69bd87b558-kl4nl 1 /1 Running 0 91s rook-ceph-operator-54cf7487d4-pl66p 1 /1 Running 0 4m57s rook-ceph-osd-0-dd4fd8f6-g6s9m 1 /1 Running 0 48s rook-ceph-osd-1-7c478c49c4-gkqml 1 /1 Running 0 47s rook-ceph-osd-2-5b887995fd-26492 1 /1 Running 0 46s rook-ceph-osd-prepare-ip-172-31-0-76-6b5fw 0 /1 Completed 0 28s rook-ceph-osd-prepare-ip-172-31-13-183-cnkf9 0 /1 Completed 0 25s rook-ceph-osd-prepare-ip-172-31-15-5-qc6pt 0 /1 Completed 0 23s","title":"7. Deploy Ceph Cluster"},{"location":"examples/rook-ceph/#8-configure-ceph-block-storage","text":"Before Ceph can provide storage to your cluster, you need to create a ReplicaPool and a StorageClass. In this example, we use the default configuration to create the block storage. kubectl apply -f ./csi/rbd/storageclass.yaml","title":"8. Configure Ceph block storage"},{"location":"examples/rook-ceph/#9-request-storage","text":"Create a new manifest file mongo-pvc.yaml with the following content: apiVersion : v1 kind : PersistentVolumeClaim metadata : name : mongo-pvc spec : storageClassName : rook-ceph-block accessModes : - ReadWriteOnce resources : requests : storage : 2Gi This will create Persistent Volume Claim (PVC) to request a 2 GB block storage from Ceph. Provioning will be done dynamically. You can define the block size freely as long as it fits to the available storage size. kubectl apply -f mongo-pvc.yaml You can now check the status of your PVC: kubectl get pvc When the PVC gets the requested volume reserved (bound), it should look like this: $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mongo-pvc Bound pvc-08337736-65dd-49d2-938c-8197a8871739 3Gi RWO rook-ceph-block 6s","title":"9. Request storage"},{"location":"examples/rook-ceph/#10-deploy-an-example-application","text":"Let's deploy a Mongo database to verify the Ceph storage. Create a new file mongo.yaml with the following content: apiVersion : apps/v1 kind : Deployment metadata : name : mongo spec : selector : matchLabels : app : mongo template : metadata : labels : app : mongo spec : containers : - image : mongo:4.0 name : mongo ports : - containerPort : 27017 name : mongo volumeMounts : - name : mongo-persistent-storage mountPath : /data/db volumes : - name : mongo-persistent-storage persistentVolumeClaim : claimName : mongo-pvc Deploy the database: kubectl apply -f mongo.yaml","title":"10. Deploy an example application"},{"location":"examples/rook-ceph/#11-access-the-application","text":"Open the MongoDB shell using the mongo pod: $ kubectl get pods NAME READY STATUS RESTARTS AGE mongo-b87cbd5cc-4wx8t 1 /1 Running 0 76s $ kubectl exec -it mongo-b87cbd5cc-4wx8t -- mongo Create a DB and insert some data: > use testDB switched to db testDB > db.testDB.insertOne( {name: \"abc\", number: 123 }) { \"acknowledged\" : true, \"insertedId\" : ObjectId(\"60815690a709d344f83b651d\") } > db.testDB.insertOne( {name: \"bcd\", number: 234 }) { \"acknowledged\" : true, \"insertedId\" : ObjectId(\"6081569da709d344f83b651e\") } Read the data: > db.getCollection(\"testDB\").find() { \"_id\" : ObjectId(\"60815690a709d344f83b651d\"), \"name\" : \"abc\", \"number\" : 123 } { \"_id\" : ObjectId(\"6081569da709d344f83b651e\"), \"name\" : \"bcd\", \"number\" : 234 } > You can also try to restart the mongo pod or restart the worker nodes to verity that the storage is persistent.","title":"11. Access the application"},{"location":"examples/rook-ceph/#12-clean-up","text":"You can use Terraform to take down the VMs: terraform destroy Remember to delete the EBS volumes separately.","title":"12. Clean-up"},{"location":"examples/rook-ceph/#conclusions","text":"You have now created a replicated Ceph storage for k0s. All you data is stored to multiple disks at the same time so you have a fault-tolerant solution. You also have enabled dynamic provisioning. Your applications can request the available storage without a manual creation of the persistent volumes each time. This was just one example to deploy distributed storage to k0s cluster using an operator. You can easily use different Kubernetes storage solutions with k0s.","title":"Conclusions"},{"location":"examples/traefik-ingress/","text":"Install the Traefik Ingress Controller on k0s # You can configure k0s with the Traefik ingress controller , a MetalLB service loadbalancer , and deploy the Traefik Dashboard using a service sample. To do this you leverage Helm's extensible bootstrapping functionality to add the correct extensions to the k0s.yaml file during cluster configuration. 1. Configure k0s.yaml # Configure k0s to install Traefik and MetalLB during cluster bootstrapping by adding their Helm charts as extensions in the k0s configuration file ( k0s.yaml ). Note: A good practice is to have a small range of IP addresses that are addressable on your network, preferably outside the assignment pool your DHCP server allocates (though any valid IP range should work locally on your machine). Providing an addressable range allows you to access your load balancer and Ingress services from anywhere on your local network. extensions : helm : repositories : - name : traefik url : https://helm.traefik.io/traefik - name : bitnami url : https://charts.bitnami.com/bitnami charts : - name : traefik chartname : traefik/traefik version : \"9.11.0\" namespace : default - name : metallb chartname : bitnami/metallb version : \"1.0.1\" namespace : default values : |2 configInline: address-pools: - name: generic-cluster-pool protocol: layer2 addresses: - 192.168.0.5-192.168.0.10 2. Retrieve the Load Balancer IP # After you start your cluster, run kubectl get all to confirm the deployment of Traefik and MetalLB. The command should return a response with the metallb and traefik resources, along with a service load balancer that has an assigned EXTERNAL-IP . kubectl get all Output : NAME READY STATUS RESTARTS AGE pod/metallb-1607085578-controller-864c9757f6-bpx6r 1 /1 Running 0 81s pod/metallb-1607085578-speaker-245c2 1 /1 Running 0 60s pod/traefik-1607085579-77bbc57699-b2f2t 1 /1 Running 0 81s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 96s service/traefik-1607085579 LoadBalancer 10 .105.119.102 192 .168.0.5 80 :32153/TCP,443:30791/TCP 84s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/metallb-1607085578-speaker 1 1 1 1 1 kubernetes.io/os = linux 87s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/metallb-1607085578-controller 1 /1 1 1 87s deployment.apps/traefik-1607085579 1 /1 1 1 84s NAME DESIRED CURRENT READY AGE replicaset.apps/metallb-1607085578-controller-864c9757f6 1 1 1 81s replicaset.apps/traefik-1607085579-77bbc57699 1 1 1 81s Take note of the EXTERNAL-IP given to the service/traefik-n load balancer. In this example, 192.168.0.5 has been assigned and can be used to access services via the Ingress proxy: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/traefik-1607085579 LoadBalancer 10 .105.119.102 192 .168.0.5 80 :32153/TCP,443:30791/TCP 84s Receiving a 404 response here is normal, as you've not configured any Ingress resources to respond yet: $ curl http://192.168.0.5 404 page not found 3. Deploy and access the Traefik Dashboard # With an available and addressable load balancer present on your cluster, now you can quickly deploy the Traefik dashboard and access it from anywhere on your LAN (assuming that MetalLB is configured with an addressable range). Create the Traefik Dashboard IngressRoute in a YAML file: apiVersion : traefik.containo.us/v1alpha1 kind : IngressRoute metadata : name : dashboard spec : entryPoints : - web routes : - match : PathPrefix(`/dashboard`) || PathPrefix(`/api`) kind : Rule services : - name : api@internal kind : TraefikService Deploy the resource: root@k0s-host \u279c kubectl apply -f traefik-dashboard.yaml Output : ingressroute.traefik.containo.us/dashboard created At this point you should be able to access the dashboard using the EXTERNAL-IP that you noted above by visiting http://192.168.0.5 in your browser: Create a simple whoami Deployment, Service, and Ingress manifest: apiVersion : apps/v1 kind : Deployment metadata : name : whoami-deployment spec : replicas : 1 selector : matchLabels : app : whoami template : metadata : labels : app : whoami spec : containers : - name : whoami-container image : containous/whoami --- apiVersion : v1 kind : Service metadata : name : whoami-service spec : ports : - name : http targetPort : 80 port : 80 selector : app : whoami --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : whoami-ingress spec : rules : - http : paths : - path : /whoami pathType : Exact backend : service : name : whoami-service port : number : 80 Apply the manifests: root@k0s-host \u279c kubectl apply -f whoami.yaml Output : deployment.apps/whoami-deployment created service/whoami-service created ingress.networking.k8s.io/whoami-ingress created Test the ingress and service: curl http://192.168.0.5/whoami Output : Hostname: whoami-deployment-85bfbd48f-7l77c IP: 127 .0.0.1 IP: ::1 IP: 10 .244.214.198 IP: fe80::b049:f8ff:fe77:3e64 RemoteAddr: 10 .244.214.196:34858 GET /whoami HTTP/1.1 Host: 192 .168.0.5 User-Agent: curl/7.68.0 Accept: */* Accept-Encoding: gzip X-Forwarded-For: 192 .168.0.82 X-Forwarded-Host: 192 .168.0.5 X-Forwarded-Port: 80 X-Forwarded-Proto: http X-Forwarded-Server: traefik-1607085579-77bbc57699-b2f2t X-Real-Ip: 192 .168.0.82 Further details # With the Traefik Ingress Controller it is possible to use 3rd party tools, such as ngrok , to go further and expose your load balancer to the world. In doing this you enable dynamic certificate provisioning through Let's Encrypt , using either cert-manager or Traefik's own built-in ACME provider .","title":"Traefik Ingress Controller"},{"location":"examples/traefik-ingress/#install-the-traefik-ingress-controller-on-k0s","text":"You can configure k0s with the Traefik ingress controller , a MetalLB service loadbalancer , and deploy the Traefik Dashboard using a service sample. To do this you leverage Helm's extensible bootstrapping functionality to add the correct extensions to the k0s.yaml file during cluster configuration.","title":"Install the Traefik Ingress Controller on k0s"},{"location":"examples/traefik-ingress/#1-configure-k0syaml","text":"Configure k0s to install Traefik and MetalLB during cluster bootstrapping by adding their Helm charts as extensions in the k0s configuration file ( k0s.yaml ). Note: A good practice is to have a small range of IP addresses that are addressable on your network, preferably outside the assignment pool your DHCP server allocates (though any valid IP range should work locally on your machine). Providing an addressable range allows you to access your load balancer and Ingress services from anywhere on your local network. extensions : helm : repositories : - name : traefik url : https://helm.traefik.io/traefik - name : bitnami url : https://charts.bitnami.com/bitnami charts : - name : traefik chartname : traefik/traefik version : \"9.11.0\" namespace : default - name : metallb chartname : bitnami/metallb version : \"1.0.1\" namespace : default values : |2 configInline: address-pools: - name: generic-cluster-pool protocol: layer2 addresses: - 192.168.0.5-192.168.0.10","title":"1. Configure k0s.yaml"},{"location":"examples/traefik-ingress/#2-retrieve-the-load-balancer-ip","text":"After you start your cluster, run kubectl get all to confirm the deployment of Traefik and MetalLB. The command should return a response with the metallb and traefik resources, along with a service load balancer that has an assigned EXTERNAL-IP . kubectl get all Output : NAME READY STATUS RESTARTS AGE pod/metallb-1607085578-controller-864c9757f6-bpx6r 1 /1 Running 0 81s pod/metallb-1607085578-speaker-245c2 1 /1 Running 0 60s pod/traefik-1607085579-77bbc57699-b2f2t 1 /1 Running 0 81s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 96s service/traefik-1607085579 LoadBalancer 10 .105.119.102 192 .168.0.5 80 :32153/TCP,443:30791/TCP 84s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/metallb-1607085578-speaker 1 1 1 1 1 kubernetes.io/os = linux 87s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/metallb-1607085578-controller 1 /1 1 1 87s deployment.apps/traefik-1607085579 1 /1 1 1 84s NAME DESIRED CURRENT READY AGE replicaset.apps/metallb-1607085578-controller-864c9757f6 1 1 1 81s replicaset.apps/traefik-1607085579-77bbc57699 1 1 1 81s Take note of the EXTERNAL-IP given to the service/traefik-n load balancer. In this example, 192.168.0.5 has been assigned and can be used to access services via the Ingress proxy: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/traefik-1607085579 LoadBalancer 10 .105.119.102 192 .168.0.5 80 :32153/TCP,443:30791/TCP 84s Receiving a 404 response here is normal, as you've not configured any Ingress resources to respond yet: $ curl http://192.168.0.5 404 page not found","title":"2. Retrieve the Load Balancer IP"},{"location":"examples/traefik-ingress/#3-deploy-and-access-the-traefik-dashboard","text":"With an available and addressable load balancer present on your cluster, now you can quickly deploy the Traefik dashboard and access it from anywhere on your LAN (assuming that MetalLB is configured with an addressable range). Create the Traefik Dashboard IngressRoute in a YAML file: apiVersion : traefik.containo.us/v1alpha1 kind : IngressRoute metadata : name : dashboard spec : entryPoints : - web routes : - match : PathPrefix(`/dashboard`) || PathPrefix(`/api`) kind : Rule services : - name : api@internal kind : TraefikService Deploy the resource: root@k0s-host \u279c kubectl apply -f traefik-dashboard.yaml Output : ingressroute.traefik.containo.us/dashboard created At this point you should be able to access the dashboard using the EXTERNAL-IP that you noted above by visiting http://192.168.0.5 in your browser: Create a simple whoami Deployment, Service, and Ingress manifest: apiVersion : apps/v1 kind : Deployment metadata : name : whoami-deployment spec : replicas : 1 selector : matchLabels : app : whoami template : metadata : labels : app : whoami spec : containers : - name : whoami-container image : containous/whoami --- apiVersion : v1 kind : Service metadata : name : whoami-service spec : ports : - name : http targetPort : 80 port : 80 selector : app : whoami --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : whoami-ingress spec : rules : - http : paths : - path : /whoami pathType : Exact backend : service : name : whoami-service port : number : 80 Apply the manifests: root@k0s-host \u279c kubectl apply -f whoami.yaml Output : deployment.apps/whoami-deployment created service/whoami-service created ingress.networking.k8s.io/whoami-ingress created Test the ingress and service: curl http://192.168.0.5/whoami Output : Hostname: whoami-deployment-85bfbd48f-7l77c IP: 127 .0.0.1 IP: ::1 IP: 10 .244.214.198 IP: fe80::b049:f8ff:fe77:3e64 RemoteAddr: 10 .244.214.196:34858 GET /whoami HTTP/1.1 Host: 192 .168.0.5 User-Agent: curl/7.68.0 Accept: */* Accept-Encoding: gzip X-Forwarded-For: 192 .168.0.82 X-Forwarded-Host: 192 .168.0.5 X-Forwarded-Port: 80 X-Forwarded-Proto: http X-Forwarded-Server: traefik-1607085579-77bbc57699-b2f2t X-Real-Ip: 192 .168.0.82","title":"3. Deploy and access the Traefik Dashboard"},{"location":"examples/traefik-ingress/#further-details","text":"With the Traefik Ingress Controller it is possible to use 3rd party tools, such as ngrok , to go further and expose your load balancer to the world. In doing this you enable dynamic certificate provisioning through Let's Encrypt , using either cert-manager or Traefik's own built-in ACME provider .","title":"Further details"},{"location":"internal/host-dependencies/","text":"Host Dependencies # The goal of k0s is to only depend on kernel functionality. However, the kubelet currently has several host dependencies. Some of these may need to be bundled, but we should prefer upstream contributions to remove these dependencies. List of hard dependencies # find -- PR by @ncopa to resolve this: https://github.com/kubernetes/kubernetes/pull/95189 du -- PR by @ncopa to resolve this: https://github.com/kubernetes/kubernetes/pull/95178 -- note that du dependency remains, but using POSIX-compliant argument nice iptables -- as documented in https://github.com/k0sproject/k0s/issues/176 it is unclear whether iptables is needed. It appears to come from the portmap plugin, but the most robust solution may be to simply bundle iptables with k0s.","title":"Host Dependencies"},{"location":"internal/host-dependencies/#host-dependencies","text":"The goal of k0s is to only depend on kernel functionality. However, the kubelet currently has several host dependencies. Some of these may need to be bundled, but we should prefer upstream contributions to remove these dependencies.","title":"Host Dependencies"},{"location":"internal/host-dependencies/#list-of-hard-dependencies","text":"find -- PR by @ncopa to resolve this: https://github.com/kubernetes/kubernetes/pull/95189 du -- PR by @ncopa to resolve this: https://github.com/kubernetes/kubernetes/pull/95178 -- note that du dependency remains, but using POSIX-compliant argument nice iptables -- as documented in https://github.com/k0sproject/k0s/issues/176 it is unclear whether iptables is needed. It appears to come from the portmap plugin, but the most robust solution may be to simply bundle iptables with k0s.","title":"List of hard dependencies"},{"location":"internal/publishing_docs_using_mkdocs/","text":"Publishing Docs # We use mkdocs and mike for publishing docs to docs.k0sproject.io . This guide will provide a simple how-to on how to configure and deploy newly added docs to our website. Requirements # Install mike: https://github.com/jimporter/mike#installation Adding A New link to the Navigation # All docs must live under the docs directory (I.E., changes to the main README.md are not reflected in the website). Add a new link under nav in the main mkdocs.yml file: nav : - Overview : README.md - Creating A Cluster : - Quick Start Guide : create-cluster.md - Run in Docker : k0s-in-docker.md - Single node set-up : k0s-single-node.md - Configuration Reference : - Architecture : architecture.md - Networking : networking.md - Configuration Options : configuration.md - Using Cloud Providers : cloud-providers.md - Running k0s with Traefik : examples/traefik-ingress.md - Running k0s as a service : install.md - k0s CLI Help Pages : cli/k0s.md - Deploying Manifests : manifests.md - FAQ : FAQ.md - Troubleshooting : troubleshooting.md - Contributing : - Overview : contributors/overview.md - Workflow : contributors/github_workflow.md - Testing : contributors/testing.md Once your changes are pushed to main , the \"Publish Docs\" jos will start running: https://github.com/k0sproject/k0s/actions?query=workflow%3A%22Publish+docs+via+GitHub+Pages%22 You should see the deployment outcome in the gh-pages deployment page: https://github.com/k0sproject/k0s/deployments/activity_log?environment=github-pages Testing docs locally # We've got a dockerized setup for easily testing docs in local environment. Simply run docker-compose up in the docs root folder. The docs will be available on localhost:80 . Note If you have something already running locally on port 80 you need to change the mapped port on the docker-compose.yml file.","title":"Documentation"},{"location":"internal/publishing_docs_using_mkdocs/#publishing-docs","text":"We use mkdocs and mike for publishing docs to docs.k0sproject.io . This guide will provide a simple how-to on how to configure and deploy newly added docs to our website.","title":"Publishing Docs"},{"location":"internal/publishing_docs_using_mkdocs/#requirements","text":"Install mike: https://github.com/jimporter/mike#installation","title":"Requirements"},{"location":"internal/publishing_docs_using_mkdocs/#adding-a-new-link-to-the-navigation","text":"All docs must live under the docs directory (I.E., changes to the main README.md are not reflected in the website). Add a new link under nav in the main mkdocs.yml file: nav : - Overview : README.md - Creating A Cluster : - Quick Start Guide : create-cluster.md - Run in Docker : k0s-in-docker.md - Single node set-up : k0s-single-node.md - Configuration Reference : - Architecture : architecture.md - Networking : networking.md - Configuration Options : configuration.md - Using Cloud Providers : cloud-providers.md - Running k0s with Traefik : examples/traefik-ingress.md - Running k0s as a service : install.md - k0s CLI Help Pages : cli/k0s.md - Deploying Manifests : manifests.md - FAQ : FAQ.md - Troubleshooting : troubleshooting.md - Contributing : - Overview : contributors/overview.md - Workflow : contributors/github_workflow.md - Testing : contributors/testing.md Once your changes are pushed to main , the \"Publish Docs\" jos will start running: https://github.com/k0sproject/k0s/actions?query=workflow%3A%22Publish+docs+via+GitHub+Pages%22 You should see the deployment outcome in the gh-pages deployment page: https://github.com/k0sproject/k0s/deployments/activity_log?environment=github-pages","title":"Adding A New link to the Navigation"},{"location":"internal/publishing_docs_using_mkdocs/#testing-docs-locally","text":"We've got a dockerized setup for easily testing docs in local environment. Simply run docker-compose up in the docs root folder. The docs will be available on localhost:80 . Note If you have something already running locally on port 80 you need to change the mapped port on the docker-compose.yml file.","title":"Testing docs locally"},{"location":"internal/upgrading-calico/","text":"Upgrading Calico # k0s bundles Kubernetes manifests for Calico. The manifests are retrieved from the official Calico docs . As fetching and modifying the entire multi-thousand line file is error-prone, you may follow these steps to upgrade Calico to the latest version: run ./get-calico.sh check the git diff to see if it looks sensible re-apply our manual adjustments (documented below) run make bindata-manifests compile, pray, and test commit and create a PR Manual Adjustments # Note: All manual adjustments should be fairly obvious from the git diff. This section attempts to provide a sanity checklist to go through and make sure we still have those changes applied. The code blocks in this section are our modifications , not the calico originals. static/manifests/calico/DaemonSet/calico-node.yaml : variable-based support for both vxlan and ipip (search for ipip to find): {{ - if eq .Mode \"ipip\" }} # Enable IPIP - name : CALICO_IPV4POOL_IPIP value : {{ .Overlay }} # Enable or Disable VXLAN on the default IP pool. - name : CALICO_IPV4POOL_VXLAN value : \"Never\" {{ - else if eq .Mode \"vxlan\" }} # Disable IPIP - name : CALICO_IPV4POOL_IPIP value : \"Never\" # Enable VXLAN on the default IP pool. - name : CALICO_IPV4POOL_VXLAN value : {{ .Overlay }} - name : FELIX_VXLANPORT value : \"{{ .VxlanPort }}\" - name : FELIX_VXLANVNI value : \"{{ .VxlanVNI }}\" {{ - end }} iptables auto detect: # Auto detect the iptables backend - name : FELIX_IPTABLESBACKEND value : \"auto\" variable-based WireGuard support: {{ - if .EnableWireguard }} - name : FELIX_WIREGUARDENABLED value : \"true\" {{ - end }} variable-based cluster CIDR: - name : CALICO_IPV4POOL_CIDR value : \"{{ .ClusterCIDR }}\" custom backend and MTU # calico-config.yaml calico_backend : \"{{ .Mode }}\" veth_mtu : \"{{ .MTU }}\" remove bgp from CLUSTER_TYPE - name : CLUSTER_TYPE value : \"k8s\" disable BIRD checks on liveness and readiness as we don't support BGP by removing -bird-ready and -bird-live from the readiness and liveness probes respectively Container image names # Instead of hardcoded image names and versions use placeholders to support configuration level settings. Following placeholders are used: CalicoCNIImage for calico/cni CalicoNodeImage for calico/node CalicoKubeControllersImage for calico/kube-controllers Also, all containers in manifests were modified to have 'imagePullPolicy' field: imagePullPolicy : {{ .PullPolicy }} Example: # calico-node.yaml image : {{ .CalicoCNIImage }}","title":"Upgrading Calico"},{"location":"internal/upgrading-calico/#upgrading-calico","text":"k0s bundles Kubernetes manifests for Calico. The manifests are retrieved from the official Calico docs . As fetching and modifying the entire multi-thousand line file is error-prone, you may follow these steps to upgrade Calico to the latest version: run ./get-calico.sh check the git diff to see if it looks sensible re-apply our manual adjustments (documented below) run make bindata-manifests compile, pray, and test commit and create a PR","title":"Upgrading Calico"},{"location":"internal/upgrading-calico/#manual-adjustments","text":"Note: All manual adjustments should be fairly obvious from the git diff. This section attempts to provide a sanity checklist to go through and make sure we still have those changes applied. The code blocks in this section are our modifications , not the calico originals. static/manifests/calico/DaemonSet/calico-node.yaml : variable-based support for both vxlan and ipip (search for ipip to find): {{ - if eq .Mode \"ipip\" }} # Enable IPIP - name : CALICO_IPV4POOL_IPIP value : {{ .Overlay }} # Enable or Disable VXLAN on the default IP pool. - name : CALICO_IPV4POOL_VXLAN value : \"Never\" {{ - else if eq .Mode \"vxlan\" }} # Disable IPIP - name : CALICO_IPV4POOL_IPIP value : \"Never\" # Enable VXLAN on the default IP pool. - name : CALICO_IPV4POOL_VXLAN value : {{ .Overlay }} - name : FELIX_VXLANPORT value : \"{{ .VxlanPort }}\" - name : FELIX_VXLANVNI value : \"{{ .VxlanVNI }}\" {{ - end }} iptables auto detect: # Auto detect the iptables backend - name : FELIX_IPTABLESBACKEND value : \"auto\" variable-based WireGuard support: {{ - if .EnableWireguard }} - name : FELIX_WIREGUARDENABLED value : \"true\" {{ - end }} variable-based cluster CIDR: - name : CALICO_IPV4POOL_CIDR value : \"{{ .ClusterCIDR }}\" custom backend and MTU # calico-config.yaml calico_backend : \"{{ .Mode }}\" veth_mtu : \"{{ .MTU }}\" remove bgp from CLUSTER_TYPE - name : CLUSTER_TYPE value : \"k8s\" disable BIRD checks on liveness and readiness as we don't support BGP by removing -bird-ready and -bird-live from the readiness and liveness probes respectively","title":"Manual Adjustments"},{"location":"internal/upgrading-calico/#container-image-names","text":"Instead of hardcoded image names and versions use placeholders to support configuration level settings. Following placeholders are used: CalicoCNIImage for calico/cni CalicoNodeImage for calico/node CalicoKubeControllersImage for calico/kube-controllers Also, all containers in manifests were modified to have 'imagePullPolicy' field: imagePullPolicy : {{ .PullPolicy }} Example: # calico-node.yaml image : {{ .CalicoCNIImage }}","title":"Container image names"}]}