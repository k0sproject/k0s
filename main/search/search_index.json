{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview # k0s is an all-inclusive Kubernetes distribution with all the required bells and whistles preconfigured to make building a Kubernetes clusters a matter of just copying an executable to every host and running it. Key Features # Packaged as a single static binary Self-hosted, isolated control plane Variety of storage backends: etcd, SQLite, MySQL (or any compatible), PostgreSQL Elastic control-plane Vanilla upstream Kubernetes Supports custom container runtimes (containerd is the default) Supports custom Container Network Interface (CNI) plugins (calico is the default) Supports x86_64 and arm64 Join the Community # If you'd like to help build k0s, please check out our guide to Contributing and our Code of Conduct . Demo # Downloading k0s # Download k0s for linux amd64 and arm64 architectures. Quick start # Creating A k0s Cluster","title":"Overview"},{"location":"#overview","text":"k0s is an all-inclusive Kubernetes distribution with all the required bells and whistles preconfigured to make building a Kubernetes clusters a matter of just copying an executable to every host and running it.","title":"Overview"},{"location":"#key-features","text":"Packaged as a single static binary Self-hosted, isolated control plane Variety of storage backends: etcd, SQLite, MySQL (or any compatible), PostgreSQL Elastic control-plane Vanilla upstream Kubernetes Supports custom container runtimes (containerd is the default) Supports custom Container Network Interface (CNI) plugins (calico is the default) Supports x86_64 and arm64","title":"Key Features"},{"location":"#join-the-community","text":"If you'd like to help build k0s, please check out our guide to Contributing and our Code of Conduct .","title":"Join the Community"},{"location":"#demo","text":"","title":"Demo"},{"location":"#downloading-k0s","text":"Download k0s for linux amd64 and arm64 architectures.","title":"Downloading k0s"},{"location":"#quick-start","text":"Creating A k0s Cluster","title":"Quick start"},{"location":"CODE_OF_CONDUCT/","text":"K0s Community Code Of Conduct # Please refer to our contributor code of conduct .","title":"K0s Community Code Of Conduct"},{"location":"CODE_OF_CONDUCT/#k0s-community-code-of-conduct","text":"Please refer to our contributor code of conduct .","title":"K0s Community Code Of Conduct"},{"location":"FAQ/","text":"Frequently asked questions # How is k0s pronounced? # kay-zero-ess How do I run a single node cluster? # k0s server --enable-worker How do I connect to the cluster? # You find the config in ${DATADIR}/pki/admin.conf (default: /var/lib/k0s/pki/admin.conf ). Copy this file, and change the localhost entry to the public ip of the controller. Use the modified config to connect with kubectl: export KUBECONFIG=/path/to/admin.conf kubectl ... Why doesn't kubectl get nodes list the k0s server? # As a default, the control plane does not run kubelet at all, and will not accept any workloads, so the server will not show up on the node list in kubectl. If you want your server to accept workloads and run pods, you do so with: k0s server --enable-worker (recommended only as test/dev/POC environments).","title":"FAQ"},{"location":"FAQ/#frequently-asked-questions","text":"","title":"Frequently asked questions"},{"location":"FAQ/#how-is-k0s-pronounced","text":"kay-zero-ess","title":"How is k0s pronounced?"},{"location":"FAQ/#how-do-i-run-a-single-node-cluster","text":"k0s server --enable-worker","title":"How do I run a single node cluster?"},{"location":"FAQ/#how-do-i-connect-to-the-cluster","text":"You find the config in ${DATADIR}/pki/admin.conf (default: /var/lib/k0s/pki/admin.conf ). Copy this file, and change the localhost entry to the public ip of the controller. Use the modified config to connect with kubectl: export KUBECONFIG=/path/to/admin.conf kubectl ...","title":"How do I connect to the cluster?"},{"location":"FAQ/#why-doesnt-kubectl-get-nodes-list-the-k0s-server","text":"As a default, the control plane does not run kubelet at all, and will not accept any workloads, so the server will not show up on the node list in kubectl. If you want your server to accept workloads and run pods, you do so with: k0s server --enable-worker (recommended only as test/dev/POC environments).","title":"Why doesn't kubectl get nodes list the k0s server?"},{"location":"architecture/","text":"Architecture # Note: As with any young project, things change rapidly. Thus all the details in this architecture documentation may not be always up-to-date, but the high level concepts and patterns should still apply. Packaging # k0s is packaged as single, self-extracting binary which embeds Kubernetes binaries. This has many benefits: - Everything can be, and is, statically compiled - No OS level deps - No RPMs, dep's, snaps or any other OS specific packaging needed. Single \"package\" for all OSes - We can fully control the versions of each and every dependency Control plane # k0s as a single binary acts as the process supervisor for all other control plane components. This means there's no container engine or kubelet running on controllers (by default). Which means there is no way for a cluster user to schedule workloads onto controller nodes. k0s creates, manages and configures each of the components. k0s runs all control plane components as \"naked\" processes. So on the controller node there's no container engine running. Storage # Typically Kubernetes control plane supports only etcd as the datastore. In addition to etcd, k0s supports many other datastore options. This is achieved by including kine . Kine allows wide variety of backend data stores to be used such as MySQL, PostgreSQL, SQLite and dqlite. See more in storage documentation In case of k0s managed etcd, k0s manages the full lifecycle of the etcd cluster. This means for example that by joining a new controller node with k0s server \"long-join-token\" k0s will automatically adjust the etcd cluster membership info to allow the new member to join the cluster. Note: Currently k0s cannot shrink the etcd cluster. For now user needs to manually remove the etcd member and only after that shutdown the k0s controller on the removed node. Worker plane # Like for the control plane, k0s creates and manages the core worker components as naked processes on the worker node. Currently we support only containerd as the container engine.","title":"Architecture"},{"location":"architecture/#architecture","text":"Note: As with any young project, things change rapidly. Thus all the details in this architecture documentation may not be always up-to-date, but the high level concepts and patterns should still apply.","title":"Architecture"},{"location":"architecture/#packaging","text":"k0s is packaged as single, self-extracting binary which embeds Kubernetes binaries. This has many benefits: - Everything can be, and is, statically compiled - No OS level deps - No RPMs, dep's, snaps or any other OS specific packaging needed. Single \"package\" for all OSes - We can fully control the versions of each and every dependency","title":"Packaging"},{"location":"architecture/#control-plane","text":"k0s as a single binary acts as the process supervisor for all other control plane components. This means there's no container engine or kubelet running on controllers (by default). Which means there is no way for a cluster user to schedule workloads onto controller nodes. k0s creates, manages and configures each of the components. k0s runs all control plane components as \"naked\" processes. So on the controller node there's no container engine running.","title":"Control plane"},{"location":"architecture/#storage","text":"Typically Kubernetes control plane supports only etcd as the datastore. In addition to etcd, k0s supports many other datastore options. This is achieved by including kine . Kine allows wide variety of backend data stores to be used such as MySQL, PostgreSQL, SQLite and dqlite. See more in storage documentation In case of k0s managed etcd, k0s manages the full lifecycle of the etcd cluster. This means for example that by joining a new controller node with k0s server \"long-join-token\" k0s will automatically adjust the etcd cluster membership info to allow the new member to join the cluster. Note: Currently k0s cannot shrink the etcd cluster. For now user needs to manually remove the etcd member and only after that shutdown the k0s controller on the removed node.","title":"Storage"},{"location":"architecture/#worker-plane","text":"Like for the control plane, k0s creates and manages the core worker components as naked processes on the worker node. Currently we support only containerd as the container engine.","title":"Worker plane"},{"location":"cloud-providers/","text":"Using cloud providers # k0s builds Kubernetes components in \"providerless\" mode. This means that there is no cloud providers built into k0s managed Kubernetes components. This means the cloud providers have to be configured \"externally\". The following steps outline how to enable cloud providers support in your k0s cluster. For more information on running Kubernetes with cloud providers see the official documentation . Enabling cloud provider support in kubelet # Even when all components are built with \"providerless\" mode, we need to be able to enable cloud provider \"mode\" for kubelet. This is done by running the workers with --enable-cloud-provider=true . This enables --cloud-provider=external on kubelet process. Deploying the actual cloud provider # From Kubernetes point of view, it does not really matter how and where the cloud providers controller(s) are running. Of course the easiest way is to deploy them on the cluster itself. To deploy your cloud provider as k0s managed stack you can use the built-in manifest deployer . Simply drop all the needed manifests under e.g. /var/lib/k0s/manifests/aws/ directory and k0s will deploy everything. Some cloud providers do need some configuration files to be present on all the nodes or some other pre-requisites. Consult your cloud providers documentation for needed steps.","title":"Using Cloud Providers"},{"location":"cloud-providers/#using-cloud-providers","text":"k0s builds Kubernetes components in \"providerless\" mode. This means that there is no cloud providers built into k0s managed Kubernetes components. This means the cloud providers have to be configured \"externally\". The following steps outline how to enable cloud providers support in your k0s cluster. For more information on running Kubernetes with cloud providers see the official documentation .","title":"Using cloud providers"},{"location":"cloud-providers/#enabling-cloud-provider-support-in-kubelet","text":"Even when all components are built with \"providerless\" mode, we need to be able to enable cloud provider \"mode\" for kubelet. This is done by running the workers with --enable-cloud-provider=true . This enables --cloud-provider=external on kubelet process.","title":"Enabling cloud provider support in kubelet"},{"location":"cloud-providers/#deploying-the-actual-cloud-provider","text":"From Kubernetes point of view, it does not really matter how and where the cloud providers controller(s) are running. Of course the easiest way is to deploy them on the cluster itself. To deploy your cloud provider as k0s managed stack you can use the built-in manifest deployer . Simply drop all the needed manifests under e.g. /var/lib/k0s/manifests/aws/ directory and k0s will deploy everything. Some cloud providers do need some configuration files to be present on all the nodes or some other pre-requisites. Consult your cloud providers documentation for needed steps.","title":"Deploying the actual cloud provider"},{"location":"configuration/","text":"k0s configuration # Control plane # k0s Control plane can be configured via a YAML config file. By default k0s server command reads a file called k0s.yaml but can be told to read any yaml file via --config option. An example config file with the most common options users should configure: apiVersion : k0s.k0sproject.io/v1beta1 kind : Cluster metadata : name : k0s spec : api : address : 192.168.68.106 sans : - my-k0s-control.my-domain.com network : podCIDR : 10.244.0.0/16 serviceCIDR : 10.96.0.0/12 extensions : helm : repositories : - name : prometheus-community url : https://prometheus-community.github.io/helm-charts charts : - name : prometheus-stack chartname : prometheus-community/prometheus version : \"11.16.8\" namespace : default spec.api # address : The local address to bind API on. Also used as one of the addresses pushed on the k0s create service certificate on the API. Defaults to first non-local address found on the node. sans : List of additional addresses to push to API servers serving certificate spec.network # podCIDR : Pod network CIDR to be used in the cluster serviceCIDR : Network CIDR to be used for cluster VIP services. extensions.helm # List of Helm repositories and charts to deploy during cluster bootstrap. This example configures Prometheus from \"stable\" Helms chart repository. Configuring multi-node controlplane # When configuring an elastic/HA controlplane one must use same configuration options on each node for the cluster level options. Following options need to match on each node, otherwise the control plane components will end up in very unknown states: - network - storage : Needless to say, one cannot create a clustered controlplane with each node only storing data locally on SQLite. Full config reference # Note: Many of the options configure things deep down in the \"stack\" on various components. So please make sure you understand what is being configured and whether or not it works in your specific environment. A full config file with defaults generated by the k0s default-config command: apiVersion : k0s.k0sproject.io/v1beta1 kind : Cluster metadata : name : k0s spec : api : externalAddress : my-lb-address.example.com address : 192.168.68.106 sans : - 192.168.68.106 extraArgs : {} controllerManager : extraArgs : {} scheduler : extraArgs : {} storage : type : etcd etcd : peerAddress : 192.168.68.106 network : podCIDR : 10.244.0.0/16 serviceCIDR : 10.96.0.0/12 provider : calico calico : mode : vxlan vxlanPort : 4789 vxlanVNI : 4096 mtu : 1450 wireguard : false flexVolumeDriverPath : /usr/libexec/k0s/kubelet-plugins/volume/exec/nodeagent~uds podSecurityPolicy : defaultPolicy : 00-k0s-privileged workerProfiles : [] images : konnectivity : image : us.gcr.io/k8s-artifacts-prod/kas-network-proxy/proxy-agent version : v0.0.13 metricsserver : image : gcr.io/k8s-staging-metrics-server/metrics-server version : v0.3.7 kubeproxy : image : k8s.gcr.io/kube-proxy version : v1.20.1 coredns : image : docker.io/coredns/coredns version : 1.7.0 calico : cni : image : calico/cni version : v3.16.2 flexvolume : image : calico/pod2daemon-flexvol version : v3.16.2 node : image : calico/node version : v3.16.2 kubecontrollers : image : calico/kube-controllers version : v3.16.2 repository : \"\" telemetry : interval : 10m0s enabled : true extensions : helm : repositories : - name : stable url : https://charts.helm.sh/stable - name : prometheus-community url : https://prometheus-community.github.io/helm-charts charts : - name : prometheus-stack chartname : prometheus-community/prometheus version : \"11.16.8\" values : | server: podDisruptionBudget: enabled: false namespace : default spec.api # externalAddress : If k0s controllers are running behind a loadbalancer provide the loadbalancer address here. This will configure all cluster components to connect to this address and also configures this address to be used when joining new nodes into the cluster. address : The local address to bind API on. Also used as one of the addresses pushed on the k0s create service certificate on the API. Defaults to first non-local address found on the node. sans : List of additional addresses to push to API servers serving certificate extraArgs : Map of key-values (strings) for any extra arguments you wish to pass down to Kubernetes api-server process spec.controllerManager # extraArgs : Map of key-values (strings) for any extra arguments you wish to pass down to Kubernetes controller manager process spec.scheduler # extraArgs : Map of key-values (strings) for any extra arguments you wish to pass down to Kubernetes scheduler process spec.storage # type : Type of the data store, either etcd or kine . etcd.peerAddress : Nodes address to be used for etcd cluster peering. kine.dataSource : kine datasource URL. Using type etcd will make k0s to create and manage an elastic etcd cluster within the controller nodes. spec.network # provider : Network provider, either calico or custom . In case of custom user can push any network provider. podCIDR : Pod network CIDR to be used in the cluster serviceCIDR : Network CIDR to be used for cluster VIP services. spec.network.calico mode : vxlan (default) or ipip vxlanPort : The UDP port to use for VXLAN (default 4789 ) vxlanVNI : The virtual network ID to use for VXLAN. (default: 4096 ) mtu : MTU to use for overlay network (default 1450 ) wireguard : enable wireguard based encryption (default false ). Your host system must be wireguard ready. See https://docs.projectcalico.org/security/encrypt-cluster-pod-traffic for details. flexVolumeDriverPath : The host path to use for Calicos flex-volume-driver (default: /usr/libexec/k0s/kubelet-plugins/volume/exec/nodeagent~uds ). This should only need to be changed if the default path is unwriteable. See https://github.com/projectcalico/calico/issues/2712 for details. This option should ideally be paired with a custom volumePluginDir in the profile used on your worker nodes. spec.podSecurityPolicy # Configures the default psp to be set. k0s creates two PSPs out of box: 00-k0s-privileged (default): no restrictions, always also used for Kubernetes/k0s level system pods 99-k0s-restricted : no host namespaces or root users allowed, no bind mounts from host As a user you can of course create any supplemental PSPs and bind them to users / access accounts as you need. spec.workerProfiles # Array of spec.workerProfiles.workerProfile Each element has following properties: - name : string, name, used as profile selector for the worker process - values : mapping object For each profile the control plane will create separate ConfigMap with kubelet-config yaml. Based on the --profile argument given to the k0s worker the corresponding ConfigMap would be used to extract kubelet-config.yaml from. values are recursively merged with default kubelet-config.yaml There are a few fields that cannot be overridden: - clusterDNS - clusterDomain - apiVersion - kind Example: workerProfiles: - name: custom-role values: key: value mapping: innerKey: innerValue Custom volumePluginDir: workerProfiles: - name: custom-role values: volumePluginDir: /var/libexec/k0s/kubelet-plugins/volume/exec images # Each node under the images key has the same structure images: konnectivity: image: calico/kube-controllers version: v3.16.2 Following keys are available: images.konnectivity images.metricsserver images.kubeproxy images.coredns images.calico.cni images.calico.flexvolume images.calico.node images.calico.kubecontrollers images.repository If images.repository is set and not empty, every image name will be prefixed with the value of images.repository Example: images: repository: \"my.own.repo\" konnectivity: image: calico/kube-controllers version: v3.16.2 In the runtime the image name will be calculated as my.own.repo/calico/kube-controllers:v3.16.2 . This only affects the location where images are getting pulled, omitting an image specification here will not disable the component from being deployed. Extensions # As stated in the project scope we intent to keep the scope of k0s quite small and not build gazillions of extensions into the product itself. To run k0s easily with your preferred extensions you have two options. Dump all needed extension manifest under /var/lib/k0s/manifests/my-extension . Read more on this approach here . Define your extensions as Helm charts : extensions: helm: repositories: - name: stable url: https://charts.helm.sh/stable - name: prometheus-community url: https://prometheus-community.github.io/helm-charts charts: - name: prometheus-stack chartname: prometheus-community/prometheus version: \"11.16.8\" values: | storageSpec: emptyDir: medium: Memory namespace: default This way you get a declarative way to configure the cluster and k0s controller manages the setup of the defined extension Helm charts as part of the cluster bootstrap process. Some examples what you could use as extension charts: - Ingress controllers: Nginx ingress , Traefix ingress ( tutorial ), - Volume storage providers: OpenEBS , Rook , Longhorn - Monitoring: Prometheus , Grafana Telemetry # To build better end user experience we collect and send telemetry data from clusters. It is enabled by default and can be disabled by settings corresponding option as false The default interval is 10 minutes, any valid value for time.Duration string representation can be used as a value. Example telemetry: interval: 2m0s enabled: true","title":"Configuration Options"},{"location":"configuration/#k0s-configuration","text":"","title":"k0s configuration"},{"location":"configuration/#control-plane","text":"k0s Control plane can be configured via a YAML config file. By default k0s server command reads a file called k0s.yaml but can be told to read any yaml file via --config option. An example config file with the most common options users should configure: apiVersion : k0s.k0sproject.io/v1beta1 kind : Cluster metadata : name : k0s spec : api : address : 192.168.68.106 sans : - my-k0s-control.my-domain.com network : podCIDR : 10.244.0.0/16 serviceCIDR : 10.96.0.0/12 extensions : helm : repositories : - name : prometheus-community url : https://prometheus-community.github.io/helm-charts charts : - name : prometheus-stack chartname : prometheus-community/prometheus version : \"11.16.8\" namespace : default","title":"Control plane"},{"location":"configuration/#specapi","text":"address : The local address to bind API on. Also used as one of the addresses pushed on the k0s create service certificate on the API. Defaults to first non-local address found on the node. sans : List of additional addresses to push to API servers serving certificate","title":"spec.api"},{"location":"configuration/#specnetwork","text":"podCIDR : Pod network CIDR to be used in the cluster serviceCIDR : Network CIDR to be used for cluster VIP services.","title":"spec.network"},{"location":"configuration/#extensionshelm","text":"List of Helm repositories and charts to deploy during cluster bootstrap. This example configures Prometheus from \"stable\" Helms chart repository.","title":"extensions.helm"},{"location":"configuration/#configuring-multi-node-controlplane","text":"When configuring an elastic/HA controlplane one must use same configuration options on each node for the cluster level options. Following options need to match on each node, otherwise the control plane components will end up in very unknown states: - network - storage : Needless to say, one cannot create a clustered controlplane with each node only storing data locally on SQLite.","title":"Configuring multi-node controlplane"},{"location":"configuration/#full-config-reference","text":"Note: Many of the options configure things deep down in the \"stack\" on various components. So please make sure you understand what is being configured and whether or not it works in your specific environment. A full config file with defaults generated by the k0s default-config command: apiVersion : k0s.k0sproject.io/v1beta1 kind : Cluster metadata : name : k0s spec : api : externalAddress : my-lb-address.example.com address : 192.168.68.106 sans : - 192.168.68.106 extraArgs : {} controllerManager : extraArgs : {} scheduler : extraArgs : {} storage : type : etcd etcd : peerAddress : 192.168.68.106 network : podCIDR : 10.244.0.0/16 serviceCIDR : 10.96.0.0/12 provider : calico calico : mode : vxlan vxlanPort : 4789 vxlanVNI : 4096 mtu : 1450 wireguard : false flexVolumeDriverPath : /usr/libexec/k0s/kubelet-plugins/volume/exec/nodeagent~uds podSecurityPolicy : defaultPolicy : 00-k0s-privileged workerProfiles : [] images : konnectivity : image : us.gcr.io/k8s-artifacts-prod/kas-network-proxy/proxy-agent version : v0.0.13 metricsserver : image : gcr.io/k8s-staging-metrics-server/metrics-server version : v0.3.7 kubeproxy : image : k8s.gcr.io/kube-proxy version : v1.20.1 coredns : image : docker.io/coredns/coredns version : 1.7.0 calico : cni : image : calico/cni version : v3.16.2 flexvolume : image : calico/pod2daemon-flexvol version : v3.16.2 node : image : calico/node version : v3.16.2 kubecontrollers : image : calico/kube-controllers version : v3.16.2 repository : \"\" telemetry : interval : 10m0s enabled : true extensions : helm : repositories : - name : stable url : https://charts.helm.sh/stable - name : prometheus-community url : https://prometheus-community.github.io/helm-charts charts : - name : prometheus-stack chartname : prometheus-community/prometheus version : \"11.16.8\" values : | server: podDisruptionBudget: enabled: false namespace : default","title":"Full config reference"},{"location":"configuration/#specapi_1","text":"externalAddress : If k0s controllers are running behind a loadbalancer provide the loadbalancer address here. This will configure all cluster components to connect to this address and also configures this address to be used when joining new nodes into the cluster. address : The local address to bind API on. Also used as one of the addresses pushed on the k0s create service certificate on the API. Defaults to first non-local address found on the node. sans : List of additional addresses to push to API servers serving certificate extraArgs : Map of key-values (strings) for any extra arguments you wish to pass down to Kubernetes api-server process","title":"spec.api"},{"location":"configuration/#speccontrollermanager","text":"extraArgs : Map of key-values (strings) for any extra arguments you wish to pass down to Kubernetes controller manager process","title":"spec.controllerManager"},{"location":"configuration/#specscheduler","text":"extraArgs : Map of key-values (strings) for any extra arguments you wish to pass down to Kubernetes scheduler process","title":"spec.scheduler"},{"location":"configuration/#specstorage","text":"type : Type of the data store, either etcd or kine . etcd.peerAddress : Nodes address to be used for etcd cluster peering. kine.dataSource : kine datasource URL. Using type etcd will make k0s to create and manage an elastic etcd cluster within the controller nodes.","title":"spec.storage"},{"location":"configuration/#specnetwork_1","text":"provider : Network provider, either calico or custom . In case of custom user can push any network provider. podCIDR : Pod network CIDR to be used in the cluster serviceCIDR : Network CIDR to be used for cluster VIP services.","title":"spec.network"},{"location":"configuration/#specpodsecuritypolicy","text":"Configures the default psp to be set. k0s creates two PSPs out of box: 00-k0s-privileged (default): no restrictions, always also used for Kubernetes/k0s level system pods 99-k0s-restricted : no host namespaces or root users allowed, no bind mounts from host As a user you can of course create any supplemental PSPs and bind them to users / access accounts as you need.","title":"spec.podSecurityPolicy"},{"location":"configuration/#specworkerprofiles","text":"Array of spec.workerProfiles.workerProfile Each element has following properties: - name : string, name, used as profile selector for the worker process - values : mapping object For each profile the control plane will create separate ConfigMap with kubelet-config yaml. Based on the --profile argument given to the k0s worker the corresponding ConfigMap would be used to extract kubelet-config.yaml from. values are recursively merged with default kubelet-config.yaml There are a few fields that cannot be overridden: - clusterDNS - clusterDomain - apiVersion - kind Example: workerProfiles: - name: custom-role values: key: value mapping: innerKey: innerValue Custom volumePluginDir: workerProfiles: - name: custom-role values: volumePluginDir: /var/libexec/k0s/kubelet-plugins/volume/exec","title":"spec.workerProfiles"},{"location":"configuration/#images","text":"Each node under the images key has the same structure images: konnectivity: image: calico/kube-controllers version: v3.16.2 Following keys are available: images.konnectivity images.metricsserver images.kubeproxy images.coredns images.calico.cni images.calico.flexvolume images.calico.node images.calico.kubecontrollers images.repository If images.repository is set and not empty, every image name will be prefixed with the value of images.repository Example: images: repository: \"my.own.repo\" konnectivity: image: calico/kube-controllers version: v3.16.2 In the runtime the image name will be calculated as my.own.repo/calico/kube-controllers:v3.16.2 . This only affects the location where images are getting pulled, omitting an image specification here will not disable the component from being deployed.","title":"images"},{"location":"configuration/#extensions","text":"As stated in the project scope we intent to keep the scope of k0s quite small and not build gazillions of extensions into the product itself. To run k0s easily with your preferred extensions you have two options. Dump all needed extension manifest under /var/lib/k0s/manifests/my-extension . Read more on this approach here . Define your extensions as Helm charts : extensions: helm: repositories: - name: stable url: https://charts.helm.sh/stable - name: prometheus-community url: https://prometheus-community.github.io/helm-charts charts: - name: prometheus-stack chartname: prometheus-community/prometheus version: \"11.16.8\" values: | storageSpec: emptyDir: medium: Memory namespace: default This way you get a declarative way to configure the cluster and k0s controller manages the setup of the defined extension Helm charts as part of the cluster bootstrap process. Some examples what you could use as extension charts: - Ingress controllers: Nginx ingress , Traefix ingress ( tutorial ), - Volume storage providers: OpenEBS , Rook , Longhorn - Monitoring: Prometheus , Grafana","title":"Extensions"},{"location":"configuration/#telemetry","text":"To build better end user experience we collect and send telemetry data from clusters. It is enabled by default and can be disabled by settings corresponding option as false The default interval is 10 minutes, any valid value for time.Duration string representation can be used as a value. Example telemetry: interval: 2m0s enabled: true","title":"Telemetry"},{"location":"conformance-testing/","text":"Kubernetes conformance testing for k0s # We run the conformance testing for the last RC build for a release. Follow the instructions as the conformance testing repository. In a nutshell, you need to: - Setup k0s on some VMs/bare metal boxes - Download, if you do not already have, sonobuoy tool - Run the conformance tests with something like sonobuoy run --mode=certified-conformance - Wait for couple hours - Collect results","title":"Kubernetes conformance testing for k0s"},{"location":"conformance-testing/#kubernetes-conformance-testing-for-k0s","text":"We run the conformance testing for the last RC build for a release. Follow the instructions as the conformance testing repository. In a nutshell, you need to: - Setup k0s on some VMs/bare metal boxes - Download, if you do not already have, sonobuoy tool - Run the conformance tests with something like sonobuoy run --mode=certified-conformance - Wait for couple hours - Collect results","title":"Kubernetes conformance testing for k0s"},{"location":"containerd_config/","text":"containerd configuration # containerd is an industry-standard container runtime. NOTE: In most use cases changes to the containerd configuration will not be required. In order to make changes to containerd configuration first you need to generate a default containerd configuration by running: containerd config default > /etc/k0s/containerd.toml This command will set the default values to /etc/k0s/containerd.toml . k0s runs containerd with the following default values: /var/lib/k0s/bin/containerd \\ --root=/var/lib/k0s/containerd \\ --state=/var/lib/k0s/run/containerd \\ --address=/var/lib/k0s/run/containerd.sock \\ --config=/etc/k0s/containerd.toml Before proceeding further, add the following default values to the configuration file: version = 2 root = \"/var/lib/k0s/containerd\" state = \"/var/lib/k0s/run/containerd\" ... [grpc] address = \"/var/lib/k0s/run/containerd.sock\" Next if you want to change CRI look into this section [plugins.\"io.containerd.runtime.v1.linux\"] shim = \"containerd-shim\" runtime = \"runc\" Using gVisor # gVisor is an application kernel, written in Go, that implements a substantial portion of the Linux system call interface. It provides an additional layer of isolation between running applications and the host operating system. First you must install the needed gVisor binaries into the host. ( set -e URL = https://storage.googleapis.com/gvisor/releases/release/latest wget ${ URL } /runsc ${ URL } /runsc.sha512 \\ ${ URL } /gvisor-containerd-shim ${ URL } /gvisor-containerd-shim.sha512 \\ ${ URL } /containerd-shim-runsc-v1 ${ URL } /containerd-shim-runsc-v1.sha512 sha512sum -c runsc.sha512 \\ -c gvisor-containerd-shim.sha512 \\ -c containerd-shim-runsc-v1.sha512 rm -f *.sha512 chmod a+rx runsc gvisor-containerd-shim containerd-shim-runsc-v1 sudo mv runsc gvisor-containerd-shim containerd-shim-runsc-v1 /usr/local/bin ) See gVisor install docs Next we need to prepare the config for k0s managed containerD to utilize gVisor as additional runtime: cat <<EOF | sudo tee /etc/k0s/containerd.toml disabled_plugins = [\"restart\"] [plugins.linux] shim_debug = true [plugins.cri.containerd.runtimes.runsc] runtime_type = \"io.containerd.runsc.v1\" EOF Then we can start and join the worker as normally into the cluster: k0s worker $token By default containerd uses nromal runc as the runtime. To make gVisor runtime usable for workloads we must register it to Kubernetes side: cat <<EOF | kubectl apply -f - apiVersion: node.k8s.io/v1beta1 kind: RuntimeClass metadata: name: gvisor handler: runsc EOF After this we can use it for our workloads: apiVersion : v1 kind : Pod metadata : name : nginx-gvisor spec : runtimeClassName : gvisor containers : - name : nginx image : nginx We can verify the created nginx pod is actually running under gVisor runtime: # kubectl exec nginx-gvisor -- dmesg | grep -i gvisor [ 0.000000] Starting gVisor... Using custom nvidia-container-runtime # By default CRI is set tu runC and if you want to configure Nvidia GPU support you will have to replace runc with nvidia-container-runtime as shown below: [plugins.\"io.containerd.runtime.v1.linux\"] shim = \"containerd-shim\" runtime = \"nvidia-container-runtime\" Note To run nvidia-container-runtime on your node please look here for detailed instructions. After changes to the configuration, restart k0s and in this case containerd will be using newly configured runtime.","title":"Configuring Containerd"},{"location":"containerd_config/#containerd-configuration","text":"containerd is an industry-standard container runtime. NOTE: In most use cases changes to the containerd configuration will not be required. In order to make changes to containerd configuration first you need to generate a default containerd configuration by running: containerd config default > /etc/k0s/containerd.toml This command will set the default values to /etc/k0s/containerd.toml . k0s runs containerd with the following default values: /var/lib/k0s/bin/containerd \\ --root=/var/lib/k0s/containerd \\ --state=/var/lib/k0s/run/containerd \\ --address=/var/lib/k0s/run/containerd.sock \\ --config=/etc/k0s/containerd.toml Before proceeding further, add the following default values to the configuration file: version = 2 root = \"/var/lib/k0s/containerd\" state = \"/var/lib/k0s/run/containerd\" ... [grpc] address = \"/var/lib/k0s/run/containerd.sock\" Next if you want to change CRI look into this section [plugins.\"io.containerd.runtime.v1.linux\"] shim = \"containerd-shim\" runtime = \"runc\"","title":"containerd configuration"},{"location":"containerd_config/#using-gvisor","text":"gVisor is an application kernel, written in Go, that implements a substantial portion of the Linux system call interface. It provides an additional layer of isolation between running applications and the host operating system. First you must install the needed gVisor binaries into the host. ( set -e URL = https://storage.googleapis.com/gvisor/releases/release/latest wget ${ URL } /runsc ${ URL } /runsc.sha512 \\ ${ URL } /gvisor-containerd-shim ${ URL } /gvisor-containerd-shim.sha512 \\ ${ URL } /containerd-shim-runsc-v1 ${ URL } /containerd-shim-runsc-v1.sha512 sha512sum -c runsc.sha512 \\ -c gvisor-containerd-shim.sha512 \\ -c containerd-shim-runsc-v1.sha512 rm -f *.sha512 chmod a+rx runsc gvisor-containerd-shim containerd-shim-runsc-v1 sudo mv runsc gvisor-containerd-shim containerd-shim-runsc-v1 /usr/local/bin ) See gVisor install docs Next we need to prepare the config for k0s managed containerD to utilize gVisor as additional runtime: cat <<EOF | sudo tee /etc/k0s/containerd.toml disabled_plugins = [\"restart\"] [plugins.linux] shim_debug = true [plugins.cri.containerd.runtimes.runsc] runtime_type = \"io.containerd.runsc.v1\" EOF Then we can start and join the worker as normally into the cluster: k0s worker $token By default containerd uses nromal runc as the runtime. To make gVisor runtime usable for workloads we must register it to Kubernetes side: cat <<EOF | kubectl apply -f - apiVersion: node.k8s.io/v1beta1 kind: RuntimeClass metadata: name: gvisor handler: runsc EOF After this we can use it for our workloads: apiVersion : v1 kind : Pod metadata : name : nginx-gvisor spec : runtimeClassName : gvisor containers : - name : nginx image : nginx We can verify the created nginx pod is actually running under gVisor runtime: # kubectl exec nginx-gvisor -- dmesg | grep -i gvisor [ 0.000000] Starting gVisor...","title":"Using gVisor"},{"location":"containerd_config/#using-custom-nvidia-container-runtime","text":"By default CRI is set tu runC and if you want to configure Nvidia GPU support you will have to replace runc with nvidia-container-runtime as shown below: [plugins.\"io.containerd.runtime.v1.linux\"] shim = \"containerd-shim\" runtime = \"nvidia-container-runtime\" Note To run nvidia-container-runtime on your node please look here for detailed instructions. After changes to the configuration, restart k0s and in this case containerd will be using newly configured runtime.","title":"Using custom nvidia-container-runtime"},{"location":"create-cluster/","text":"Creating A cluster with k0s # As k0s binary has everything it needs packaged into a single binary, it makes it super easy to spin up Kubernetes clusters. Pre-requisites # Download k0s binary from releases and push it to all the nodes you wish to connect to the cluster. That's it, really. Bootstrapping controller node # Create a configuration file if you wish to tune some of the settings. $ k0s server -c k0s.yaml That's it, really. k0s process will act as a \"supervisor\" for all the control plane components. In few seconds you'll have the control plane up-and-running. Naturally, to make k0s boot up the control plane when the node itself reboots you should really make the k0s process to be supervised by systemd or some other init system. Create join token # To be able to join workers into the cluster we need a token. The token embeds information with which we can enable mutual trust between the worker and controller(s) and allow the node to join the cluster as worker. To get a token run the following on one of the existing controller nodes: k0s token create --role = worker This will output a long token which we will use to join the worker. To enhance security, we can also set an expiration time on the tokens by using: k0s token create --role = worker --expiry = \"100h\" Joining worker(s) to cluster # To join the worker we need to run k0s in worker mode with the token from previous step: $ k0s worker \"long-join-token\" That's it, really. Naturally, to make k0s boot up the worker components when the node itself reboots you should really make the k0s process to be supervised by systemd or some other init system. Tokens # The tokens are actually base64 encoded kubeconfigs . Why: - well defined structure - can be used directly as bootstrap auth configs for kubelet - embeds CA info for mutual trust The actual bearer token embedded in the kubeconfig is a bootstrap token . For controller join token and for worker join token we use different usage attributes so we can make sure we can validate the token role on the controller side. Join controller node # To be able to join a new controller node into the cluster you must be using either etcd or some externalized data store (MySQL or Postgres) via kine. Also make sure the configurations match for the data storage on all controller nodes. To create a join token for the new controller, run the following on existing controller node: k0s token create --role = controller --expiry = 1h On the new controller, run: k0s server \"long-join-token\" Adding a Cluster User # To add a user to cluster, use the kubeconfig create command. This will output a kubeconfig for the user, which can be used for authentication. On the controller, run the following to generate a kubeconfig for a user: k0s kubeconfig create [ username ] Enabling Access to Cluster Resources # To allow the user access to the cluster, the user needs to be created with the system:masters group: clusterUser = \"testUser\" k0s kubeconfig create --groups \"system:masters\" $clusterUser > ~/.kube/config Create the proper roleBinding, to allow the user access to the resources: kubectl create clusterrolebinding $clusterUser -admin-binding --clusterrole = admin --user = $clusterUser Service and Log Setup # k0s install sub-command was created as a helper command to allow users to easily install k0s as a service. For more information, read here . Enabling Shell Completion # The k0s completion script for Bash, zsh, fish and powershell can be generated with the command k0s completion < shell > . Sourcing the completion script in your shell enables k0s autocompletion. Bash # echo 'source <(k0s completion bash)' >>~/.bashrc # To load completions for each session, execute once: $ k0s completion bash > /etc/bash_completion.d/k0s Zsh # If shell completion is not already enabled in your environment you will need to enable it. You can execute the following once: $ echo \"autoload -U compinit; compinit\" >> ~/.zshrc # To load completions for each session, execute once: $ k0s completion zsh > \" ${ fpath [1] } /_k0s\" You will need to start a new shell for this setup to take effect. Fish # $ k0s completion fish | source # To load completions for each session, execute once: $ k0s completion fish > ~/.config/fish/completions/k0s.fish","title":"Quick Start Guide"},{"location":"create-cluster/#creating-a-cluster-with-k0s","text":"As k0s binary has everything it needs packaged into a single binary, it makes it super easy to spin up Kubernetes clusters.","title":"Creating A cluster with k0s"},{"location":"create-cluster/#pre-requisites","text":"Download k0s binary from releases and push it to all the nodes you wish to connect to the cluster. That's it, really.","title":"Pre-requisites"},{"location":"create-cluster/#bootstrapping-controller-node","text":"Create a configuration file if you wish to tune some of the settings. $ k0s server -c k0s.yaml That's it, really. k0s process will act as a \"supervisor\" for all the control plane components. In few seconds you'll have the control plane up-and-running. Naturally, to make k0s boot up the control plane when the node itself reboots you should really make the k0s process to be supervised by systemd or some other init system.","title":"Bootstrapping controller node"},{"location":"create-cluster/#create-join-token","text":"To be able to join workers into the cluster we need a token. The token embeds information with which we can enable mutual trust between the worker and controller(s) and allow the node to join the cluster as worker. To get a token run the following on one of the existing controller nodes: k0s token create --role = worker This will output a long token which we will use to join the worker. To enhance security, we can also set an expiration time on the tokens by using: k0s token create --role = worker --expiry = \"100h\"","title":"Create join token"},{"location":"create-cluster/#joining-workers-to-cluster","text":"To join the worker we need to run k0s in worker mode with the token from previous step: $ k0s worker \"long-join-token\" That's it, really. Naturally, to make k0s boot up the worker components when the node itself reboots you should really make the k0s process to be supervised by systemd or some other init system.","title":"Joining worker(s) to cluster"},{"location":"create-cluster/#tokens","text":"The tokens are actually base64 encoded kubeconfigs . Why: - well defined structure - can be used directly as bootstrap auth configs for kubelet - embeds CA info for mutual trust The actual bearer token embedded in the kubeconfig is a bootstrap token . For controller join token and for worker join token we use different usage attributes so we can make sure we can validate the token role on the controller side.","title":"Tokens"},{"location":"create-cluster/#join-controller-node","text":"To be able to join a new controller node into the cluster you must be using either etcd or some externalized data store (MySQL or Postgres) via kine. Also make sure the configurations match for the data storage on all controller nodes. To create a join token for the new controller, run the following on existing controller node: k0s token create --role = controller --expiry = 1h On the new controller, run: k0s server \"long-join-token\"","title":"Join controller node"},{"location":"create-cluster/#adding-a-cluster-user","text":"To add a user to cluster, use the kubeconfig create command. This will output a kubeconfig for the user, which can be used for authentication. On the controller, run the following to generate a kubeconfig for a user: k0s kubeconfig create [ username ]","title":"Adding a Cluster User"},{"location":"create-cluster/#enabling-access-to-cluster-resources","text":"To allow the user access to the cluster, the user needs to be created with the system:masters group: clusterUser = \"testUser\" k0s kubeconfig create --groups \"system:masters\" $clusterUser > ~/.kube/config Create the proper roleBinding, to allow the user access to the resources: kubectl create clusterrolebinding $clusterUser -admin-binding --clusterrole = admin --user = $clusterUser","title":"Enabling Access to Cluster Resources"},{"location":"create-cluster/#service-and-log-setup","text":"k0s install sub-command was created as a helper command to allow users to easily install k0s as a service. For more information, read here .","title":"Service and Log Setup"},{"location":"create-cluster/#enabling-shell-completion","text":"The k0s completion script for Bash, zsh, fish and powershell can be generated with the command k0s completion < shell > . Sourcing the completion script in your shell enables k0s autocompletion.","title":"Enabling Shell Completion"},{"location":"create-cluster/#bash","text":"echo 'source <(k0s completion bash)' >>~/.bashrc # To load completions for each session, execute once: $ k0s completion bash > /etc/bash_completion.d/k0s","title":"Bash"},{"location":"create-cluster/#zsh","text":"If shell completion is not already enabled in your environment you will need to enable it. You can execute the following once: $ echo \"autoload -U compinit; compinit\" >> ~/.zshrc # To load completions for each session, execute once: $ k0s completion zsh > \" ${ fpath [1] } /_k0s\" You will need to start a new shell for this setup to take effect.","title":"Zsh"},{"location":"create-cluster/#fish","text":"$ k0s completion fish | source # To load completions for each session, execute once: $ k0s completion fish > ~/.config/fish/completions/k0s.fish","title":"Fish"},{"location":"custom-cri-runtime/","text":"Custom CRI runtime # k0s supports users bringing their own CRI runtime (for example, docker). In which case, k0s will not start nor manage the runtime, and it is fully up to the user to configure it properly. To run a k0s worker with a custom CRI runtime use the option --cri-socket . It takes input in the form of <type>:<socket> where: type : Either remote or docker . Use docker for pure docker setup, remote for anything else. socket : Path to the socket, examples: unix:///var/run/docker.sock To run k0s with pre-existing docker setup run the worker with k0s worker --cri-socket docker:unix:///var/run/docker.sock <token> . When docker is used as a runtime, k0s will configure kubelet to create the dockershim socket at /var/run/dockershim.sock .","title":"Using A Custom CRI"},{"location":"custom-cri-runtime/#custom-cri-runtime","text":"k0s supports users bringing their own CRI runtime (for example, docker). In which case, k0s will not start nor manage the runtime, and it is fully up to the user to configure it properly. To run a k0s worker with a custom CRI runtime use the option --cri-socket . It takes input in the form of <type>:<socket> where: type : Either remote or docker . Use docker for pure docker setup, remote for anything else. socket : Path to the socket, examples: unix:///var/run/docker.sock To run k0s with pre-existing docker setup run the worker with k0s worker --cri-socket docker:unix:///var/run/docker.sock <token> . When docker is used as a runtime, k0s will configure kubelet to create the dockershim socket at /var/run/dockershim.sock .","title":"Custom CRI runtime"},{"location":"experimental-windows/","text":"Windows support # Experimental status # Windows support feature is under active development and MUST BE considered as highly experemential. Build # make clean k0s.exe This should create k0s.exe with staged kubelet.exe and kube-proxy.exe Description # the k0s.exe supervises kubelet.exe and kube-proxy.exe During the first run calico install script created as C:\\bootstrap.ps1 The bootstrap script downloads the calico binaries, builds pause container and set ups vSwitch settings. Running # It is expected to have docker EE installed on the windows node (we need it during the initial calico set up) C:\\>k0s.exe worker --cri-socket=docker:tcp://127.0.0.1:2375 --cidr-range=<cidr_range> --cluster-dns=<clusterdns> --api-server=<k0s api> <token> Cluster MUST have at least one linux worker node. Cluster control plane must be inited with proper config (see section below) Configuration # Strict-affinity # To run windows node we need to have strict affinity enabled. There is a configuration field spec.network.calico.withWindowsNodes , equals false by default. If set to the true, the additional calico related manifest /var/lib/k0s/manifests/calico/calico-IPAMConfig-ipamconfig.yaml would be created with the following values --- apiVersion: crd.projectcalico.org/v1 kind: IPAMConfig metadata: name: default spec: strictAffinity: true Another way is to use calicoctl manually: calicoctl ipam configure --strictaffinity=true Network connectivity in AWS # The network interface attached to your EC2 instance MUST have disabled \u201cChange Source/Dest. Check\u201d option. In AWS console option can be found on the Actions menu for a selected network interface. Hacks # We need to figure out proper way to pass cluster settings from controller plane to worker. While we don't have it, there are CLI arguments: - cidr-range - cluster-dns - api-server Some useful commands # Run pod with cmd.exe shell kubectl run win --image=hello-world:nanoserver --command=true -i --attach=true -- cmd.exe Manifest for pod with IIS web-server apiVersion: v1 kind: Pod metadata: name: iis spec: containers: - name: iis image: mcr.microsoft.com/windows/servercore/iis imagePullPolicy: IfNotPresent","title":"K0s in Windows (experimental)"},{"location":"experimental-windows/#windows-support","text":"","title":"Windows support"},{"location":"experimental-windows/#experimental-status","text":"Windows support feature is under active development and MUST BE considered as highly experemential.","title":"Experimental status"},{"location":"experimental-windows/#build","text":"make clean k0s.exe This should create k0s.exe with staged kubelet.exe and kube-proxy.exe","title":"Build"},{"location":"experimental-windows/#description","text":"the k0s.exe supervises kubelet.exe and kube-proxy.exe During the first run calico install script created as C:\\bootstrap.ps1 The bootstrap script downloads the calico binaries, builds pause container and set ups vSwitch settings.","title":"Description"},{"location":"experimental-windows/#running","text":"It is expected to have docker EE installed on the windows node (we need it during the initial calico set up) C:\\>k0s.exe worker --cri-socket=docker:tcp://127.0.0.1:2375 --cidr-range=<cidr_range> --cluster-dns=<clusterdns> --api-server=<k0s api> <token> Cluster MUST have at least one linux worker node. Cluster control plane must be inited with proper config (see section below)","title":"Running"},{"location":"experimental-windows/#configuration","text":"","title":"Configuration"},{"location":"experimental-windows/#strict-affinity","text":"To run windows node we need to have strict affinity enabled. There is a configuration field spec.network.calico.withWindowsNodes , equals false by default. If set to the true, the additional calico related manifest /var/lib/k0s/manifests/calico/calico-IPAMConfig-ipamconfig.yaml would be created with the following values --- apiVersion: crd.projectcalico.org/v1 kind: IPAMConfig metadata: name: default spec: strictAffinity: true Another way is to use calicoctl manually: calicoctl ipam configure --strictaffinity=true","title":"Strict-affinity"},{"location":"experimental-windows/#network-connectivity-in-aws","text":"The network interface attached to your EC2 instance MUST have disabled \u201cChange Source/Dest. Check\u201d option. In AWS console option can be found on the Actions menu for a selected network interface.","title":"Network connectivity in AWS"},{"location":"experimental-windows/#hacks","text":"We need to figure out proper way to pass cluster settings from controller plane to worker. While we don't have it, there are CLI arguments: - cidr-range - cluster-dns - api-server","title":"Hacks"},{"location":"experimental-windows/#some-useful-commands","text":"Run pod with cmd.exe shell kubectl run win --image=hello-world:nanoserver --command=true -i --attach=true -- cmd.exe Manifest for pod with IIS web-server apiVersion: v1 kind: Pod metadata: name: iis spec: containers: - name: iis image: mcr.microsoft.com/windows/servercore/iis imagePullPolicy: IfNotPresent","title":"Some useful commands"},{"location":"extensions/","text":"Cluster extensions # k0s allows users to use extensions to extend cluster functionality. At the moment the only supported type of extensions is helm based charts. The default configuration has no extensions. Helm based extensions # Configuration # Example. helm: repositories: - name: stable url: https://charts.helm.sh/stable - name: prometheus-community url: https://prometheus-community.github.io/helm-charts charts: - name: prometheus-stack chartname: prometheus-community/prometheus version: \"11.16.8\" values: | storageSpec: emptyDir: medium: Memory namespace: default By using the configuration above, the cluster would: - add stable and prometheus-community chart repositories - install the `prometheus-community/prometheus` chart of the specified version to the `default` namespace. The chart installation is implemented by using CRD `helm.k0sproject.io/Chart`. For every given helm extension the cluster creates a Chart CRD instance. The cluster has a controller which monitors for the Chart CRDs, supporting the following operations: - install - upgrade - delete For security reasons, the cluster operates only on Chart CRDs instantiated in the `kube-system` namespace, however, the target namespace could be any. #### CRD definition apiVersion: helm.k0sproject.io/v1beta1 kind: Chart metadata: creationTimestamp: \"2020-11-10T14:17:53Z\" generation: 2 labels: k0s.k0sproject.io/stack: helm name: k0s-addon-chart-test-addon namespace: kube-system resourceVersion: \"627\" selfLink: /apis/helm.k0sproject.io/v1beta1/namespaces/kube-system/charts/k0s-addon-chart-test-addon uid: ebe59ed4-1ff8-4d41-8e33-005b183651ed spec: chartName: prometheus-community/prometheus namespace: default values: | storageSpec: emptyDir: medium: Memory version: 11.16.8 status: appVersion: 2.21.0 namespace: default releaseName: prometheus-1605017878 revision: 2 updated: 2020-11-10 14:18:08.235656 +0000 UTC m=+41.871656901 version: 11.16.8 The Chart.spec defines the chart information. The Chart.status keeps the information about the last operation performed by the operator.","title":"Cluster extensions"},{"location":"extensions/#cluster-extensions","text":"k0s allows users to use extensions to extend cluster functionality. At the moment the only supported type of extensions is helm based charts. The default configuration has no extensions.","title":"Cluster extensions"},{"location":"extensions/#helm-based-extensions","text":"","title":"Helm based extensions"},{"location":"extensions/#configuration","text":"Example. helm: repositories: - name: stable url: https://charts.helm.sh/stable - name: prometheus-community url: https://prometheus-community.github.io/helm-charts charts: - name: prometheus-stack chartname: prometheus-community/prometheus version: \"11.16.8\" values: | storageSpec: emptyDir: medium: Memory namespace: default By using the configuration above, the cluster would: - add stable and prometheus-community chart repositories - install the `prometheus-community/prometheus` chart of the specified version to the `default` namespace. The chart installation is implemented by using CRD `helm.k0sproject.io/Chart`. For every given helm extension the cluster creates a Chart CRD instance. The cluster has a controller which monitors for the Chart CRDs, supporting the following operations: - install - upgrade - delete For security reasons, the cluster operates only on Chart CRDs instantiated in the `kube-system` namespace, however, the target namespace could be any. #### CRD definition apiVersion: helm.k0sproject.io/v1beta1 kind: Chart metadata: creationTimestamp: \"2020-11-10T14:17:53Z\" generation: 2 labels: k0s.k0sproject.io/stack: helm name: k0s-addon-chart-test-addon namespace: kube-system resourceVersion: \"627\" selfLink: /apis/helm.k0sproject.io/v1beta1/namespaces/kube-system/charts/k0s-addon-chart-test-addon uid: ebe59ed4-1ff8-4d41-8e33-005b183651ed spec: chartName: prometheus-community/prometheus namespace: default values: | storageSpec: emptyDir: medium: Memory version: 11.16.8 status: appVersion: 2.21.0 namespace: default releaseName: prometheus-1605017878 revision: 2 updated: 2020-11-10 14:18:08.235656 +0000 UTC m=+41.871656901 version: 11.16.8 The Chart.spec defines the chart information. The Chart.status keeps the information about the last operation performed by the operator.","title":"Configuration"},{"location":"install/","text":"Service and Logging # We've created the subcommand k0s install to allow users to easily install k0s as a service, and define its logging. This is an alpha state feature. Caveats # This command is strictly a helper command. It is not meant to provide a fully-automated solution, since you can run k0s in multiple, very different ways. It configures your service set-up as either a worker or a server, and will have different tasks, depending on the role you pick. Supported services: OpenRC & Systemd Server setup # This is the default mode of operation. When a server role is picked, the installer will do the following: Create user accounts for the different components (see https://github.com/k0sproject/k0s/blob/main/pkg/apis/v1beta1/system.go#L6) Create a service file (OpenRC/Systemd) and redirects logging to /var/log/k0s.log . If the --debug flag is used, it will also pass this flag along to the service file. enable-worker (single-node) setup is not supported. If you would like to run your service in that way, a possible solution would be to run cmd install as worker, and edit the startup command by hand. Worker Setup # A worker cannot run with any other user, other than root , so no special users will be created. The service file will include the --token-file flag, with a value that needs to be manually changed. If the --debug flag is used, it will also pass this flag along to the service file. Additional Documentation # see: k0s install","title":"Running k0s as a service"},{"location":"install/#service-and-logging","text":"We've created the subcommand k0s install to allow users to easily install k0s as a service, and define its logging. This is an alpha state feature.","title":"Service and Logging"},{"location":"install/#caveats","text":"This command is strictly a helper command. It is not meant to provide a fully-automated solution, since you can run k0s in multiple, very different ways. It configures your service set-up as either a worker or a server, and will have different tasks, depending on the role you pick. Supported services: OpenRC & Systemd","title":"Caveats"},{"location":"install/#server-setup","text":"This is the default mode of operation. When a server role is picked, the installer will do the following: Create user accounts for the different components (see https://github.com/k0sproject/k0s/blob/main/pkg/apis/v1beta1/system.go#L6) Create a service file (OpenRC/Systemd) and redirects logging to /var/log/k0s.log . If the --debug flag is used, it will also pass this flag along to the service file. enable-worker (single-node) setup is not supported. If you would like to run your service in that way, a possible solution would be to run cmd install as worker, and edit the startup command by hand.","title":"Server setup"},{"location":"install/#worker-setup","text":"A worker cannot run with any other user, other than root , so no special users will be created. The service file will include the --token-file flag, with a value that needs to be manually changed. If the --debug flag is used, it will also pass this flag along to the service file.","title":"Worker Setup"},{"location":"install/#additional-documentation","text":"see: k0s install","title":"Additional Documentation"},{"location":"k0s-in-docker/","text":"k0s in Docker # We publish a k0s container image with every release. By default, we run both controller and worker in the same container to provide an easy local testing \"cluster\". You can run your own k0s-in-docker easily with: docker run -d --name k0s --hostname k0s --privileged -v /var/lib/k0s -p 6443 :6443 docker.pkg.github.com/k0sproject/k0s/k0s:<version> Just grab the kubeconfig file with docker exec k0s cat /var/lib/k0s/pki/admin.conf and paste e.g. into Lens . Running workers # If you want to attach multiple workers nodes into the cluster you can run separate containers for each worker. First, we need a join token for the worker: token = $( docker exec -t -i k0s k0s token create --role = worker ) Then join a new worker by running the container with: docker run -d --name k0s-worker1 --hostname k0s-worker1 --privileged -v /var/lib/k0s docker.pkg.github.com/k0sproject/k0s/k0s:<version> k0s worker $token Repeat for as many workers you need, and have resources for. :) Docker Compose # You can also run k0s with Docker Compose: version : \"3.9\" services : k0s : container_name : k0s image : docker.pkg.github.com/k0sproject/k0s/k0s:<version> hostname : k0s privileged : true volumes : - \"/var/lib/k0s\" ports : - \"6443:6443\" network_mode : \"bridge\" Known limitations # No custom Docker networks # Currently, we cannot run k0s nodes if the containers are configured to use custom networks e.g. with --net my-net . This is caused by the fact that Docker sets up a custom DNS service within the network and that messes up CoreDNS. We know that there are some workarounds possible, but they are bit hackish. And on the other hand, running k0s cluster(s) in bridge network should not cause issues.","title":"Run in Docker"},{"location":"k0s-in-docker/#k0s-in-docker","text":"We publish a k0s container image with every release. By default, we run both controller and worker in the same container to provide an easy local testing \"cluster\". You can run your own k0s-in-docker easily with: docker run -d --name k0s --hostname k0s --privileged -v /var/lib/k0s -p 6443 :6443 docker.pkg.github.com/k0sproject/k0s/k0s:<version> Just grab the kubeconfig file with docker exec k0s cat /var/lib/k0s/pki/admin.conf and paste e.g. into Lens .","title":"k0s in Docker"},{"location":"k0s-in-docker/#running-workers","text":"If you want to attach multiple workers nodes into the cluster you can run separate containers for each worker. First, we need a join token for the worker: token = $( docker exec -t -i k0s k0s token create --role = worker ) Then join a new worker by running the container with: docker run -d --name k0s-worker1 --hostname k0s-worker1 --privileged -v /var/lib/k0s docker.pkg.github.com/k0sproject/k0s/k0s:<version> k0s worker $token Repeat for as many workers you need, and have resources for. :)","title":"Running workers"},{"location":"k0s-in-docker/#docker-compose","text":"You can also run k0s with Docker Compose: version : \"3.9\" services : k0s : container_name : k0s image : docker.pkg.github.com/k0sproject/k0s/k0s:<version> hostname : k0s privileged : true volumes : - \"/var/lib/k0s\" ports : - \"6443:6443\" network_mode : \"bridge\"","title":"Docker Compose"},{"location":"k0s-in-docker/#known-limitations","text":"","title":"Known limitations"},{"location":"k0s-in-docker/#no-custom-docker-networks","text":"Currently, we cannot run k0s nodes if the containers are configured to use custom networks e.g. with --net my-net . This is caused by the fact that Docker sets up a custom DNS service within the network and that messes up CoreDNS. We know that there are some workarounds possible, but they are bit hackish. And on the other hand, running k0s cluster(s) in bridge network should not cause issues.","title":"No custom Docker networks"},{"location":"k0s-single-node/","text":"k0s Single Node Quick Start # These instructions outline a quick method for running a local k0s master and worker in a single node. NOTE: This method of running k0s is only recommended for dev, test or POC environments. Prepare Dependencies # 1. Download the k0s binary curl -sSLf https://get.k0s.sh | sh 2. Download the kubectl binary sudo curl --output /usr/local/sbin/kubectl -L \"https://storage.googleapis.com/kubernetes-release/release/ $( curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt ) /bin/linux/amd64/kubectl\" 3. Make both binaries executable sudo chmod +x /usr/local/sbin/kubectl sudo chmod +x /usr/bin/k0s Start k0s # 1. Create the k0s config directory mkdir -p ${ HOME } /.k0s 2. Generate a default cluster configuration k0s default-config | tee ${ HOME } /.k0s/k0s.yaml 3. Start k0s sudo k0s server -c ${ HOME } /.k0s/k0s.yaml --enable-worker & Use kubectl to access k0s # 1. Save kubeconfig for user sudo cat /var/lib/k0s/pki/admin.conf | tee ~/.k0s/kubeconfig 2. Set the KUBECONFIG environment variable export KUBECONFIG = \" ${ HOME } /.k0s/kubeconfig\" 3. Monitor cluster startup kubectl get pods --all-namespaces","title":"Single node set-up"},{"location":"k0s-single-node/#k0s-single-node-quick-start","text":"These instructions outline a quick method for running a local k0s master and worker in a single node. NOTE: This method of running k0s is only recommended for dev, test or POC environments.","title":"k0s Single Node Quick Start"},{"location":"k0s-single-node/#prepare-dependencies","text":"","title":"Prepare Dependencies"},{"location":"k0s-single-node/#start-k0s","text":"","title":"Start k0s"},{"location":"k0s-single-node/#use-kubectl-to-access-k0s","text":"","title":"Use kubectl to access k0s"},{"location":"manifests/","text":"Manifest deployer # k0s embeds a manifest deployer on controllers which provides an easy way to deploy manifests automatically. By default k0s reads all manifests under ${DATADIR}/manifests (default: /var/lib/k0s/manifests ) and ensures their state matches the cluster state. When you remove a manifest file, k0s will automatically prune all the resources associated with it. Each directory that is a direct descendant of ${DATADIR}/manifests is considered as its own \"stack\", but nested directories will be excluded from the stack mechanism. Note: k0s uses this mechanism for some of its internal in-cluster components and other resources. Make sure you only touch the manifests not managed by k0s. Future # We may in the future support nested directories, but those will not be considered stacks , but rather sub-resources of a parent stacks. Stacks are exclusively top-level.","title":"Deploying Manifests"},{"location":"manifests/#manifest-deployer","text":"k0s embeds a manifest deployer on controllers which provides an easy way to deploy manifests automatically. By default k0s reads all manifests under ${DATADIR}/manifests (default: /var/lib/k0s/manifests ) and ensures their state matches the cluster state. When you remove a manifest file, k0s will automatically prune all the resources associated with it. Each directory that is a direct descendant of ${DATADIR}/manifests is considered as its own \"stack\", but nested directories will be excluded from the stack mechanism. Note: k0s uses this mechanism for some of its internal in-cluster components and other resources. Make sure you only touch the manifests not managed by k0s.","title":"Manifest deployer"},{"location":"manifests/#future","text":"We may in the future support nested directories, but those will not be considered stacks , but rather sub-resources of a parent stacks. Stacks are exclusively top-level.","title":"Future"},{"location":"networking/","text":"k0s Networking # In-cluster networking # k0s supports currently only Calico as the built-in in-cluster overlay network provider. A user can however opt-out of k0s managing the network setup by using a custom as the network type. Using custom network provider it is expected that the user sets up the networking. This can be achieved e.g. by pushing network provider manifests into /var/lib/k0s/manifests from where k0s controllers will pick them up and deploy into the cluster. More on the automatic manifest handling here . Controller(s) - Worker communication # As one of the goals of k0s is to allow deployment of totally isolated control plane we cannot rely on the fact that there is an IP route between controller nodes and the pod overlay network. To enable this communication path, which is mandated by conformance tests, we use Egress service and konnectivity proxy to proxy the traffic from API server into worker nodes. This ansures that we can always fulfill all the Kubernetes API functionalities but still operate the control plane in total isolation from the workers. Needed open ports & protocols # Protocol Port Service Direction Notes TCP 2380 etcd peers controller <-> controller TCP 6443 kube-apiserver Worker, CLI => controller authenticated kube API using kube TLS client certs, ServiceAccount tokens with RBAC UDP 4789 Calico worker <-> worker Calico VXLAN overlay TCP 10250 kubelet Master, Worker => Host * authenticated kubelet API for the master node kube-apiserver (and heapster / metrics-server addons) using TLS client certs TCP 9443 k0s-api controller <-> controller k0s controller join API, TLS with token auth TCP 8132,8133 konnectivity server worker <-> controller konnectivity is used as \"reverse\" tunnel between kube-apiserver and worker kubelets","title":"Networking"},{"location":"networking/#k0s-networking","text":"","title":"k0s Networking"},{"location":"networking/#in-cluster-networking","text":"k0s supports currently only Calico as the built-in in-cluster overlay network provider. A user can however opt-out of k0s managing the network setup by using a custom as the network type. Using custom network provider it is expected that the user sets up the networking. This can be achieved e.g. by pushing network provider manifests into /var/lib/k0s/manifests from where k0s controllers will pick them up and deploy into the cluster. More on the automatic manifest handling here .","title":"In-cluster networking"},{"location":"networking/#controllers-worker-communication","text":"As one of the goals of k0s is to allow deployment of totally isolated control plane we cannot rely on the fact that there is an IP route between controller nodes and the pod overlay network. To enable this communication path, which is mandated by conformance tests, we use Egress service and konnectivity proxy to proxy the traffic from API server into worker nodes. This ansures that we can always fulfill all the Kubernetes API functionalities but still operate the control plane in total isolation from the workers.","title":"Controller(s) - Worker communication"},{"location":"networking/#needed-open-ports-protocols","text":"Protocol Port Service Direction Notes TCP 2380 etcd peers controller <-> controller TCP 6443 kube-apiserver Worker, CLI => controller authenticated kube API using kube TLS client certs, ServiceAccount tokens with RBAC UDP 4789 Calico worker <-> worker Calico VXLAN overlay TCP 10250 kubelet Master, Worker => Host * authenticated kubelet API for the master node kube-apiserver (and heapster / metrics-server addons) using TLS client certs TCP 9443 k0s-api controller <-> controller k0s controller join API, TLS with token auth TCP 8132,8133 konnectivity server worker <-> controller konnectivity is used as \"reverse\" tunnel between kube-apiserver and worker kubelets","title":"Needed open ports &amp; protocols"},{"location":"troubleshooting/","text":"Troubleshooting # There are few common cases we've seen where k0s fails to run properly. CoreDNS in crashloop # The most common case we've encountered so far has been CoreDNS getting into crashloop on the node(s). With kubectl you see something like this: $ kubectl get pod --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-5f6546844f-25px6 1 /1 Running 0 167m kube-system calico-node-fwjx5 1 /1 Running 0 164m kube-system calico-node-t4tx5 1 /1 Running 0 164m kube-system calico-node-whwsg 1 /1 Running 0 164m kube-system coredns-5c98d7d4d8-tfs4q 1 /1 Error 17 167m kube-system konnectivity-agent-9jkfd 1 /1 Running 0 164m kube-system konnectivity-agent-bvhdb 1 /1 Running 0 164m kube-system konnectivity-agent-r6mzj 1 /1 Running 0 164m kube-system kube-proxy-kr2r9 1 /1 Running 0 164m kube-system kube-proxy-tbljr 1 /1 Running 0 164m kube-system kube-proxy-xbw7p 1 /1 Running 0 164m kube-system metrics-server-7d4bcb75dd-pqkrs 1 /1 Running 0 167m When you check the logs, it'll show something like this: $ kubectl -n kube-system logs coredns-5c98d7d4d8-tfs4q plugin/loop: Loop (127.0.0.1:55953 -> :1053) detected for zone \".\", see https://coredns.io/plugins/loop#troubleshooting. Query: \"HINFO 4547991504243258144.3688648895315093531.\" This is most often caused by systemd-resolved stub (or something similar) running locally and CoreDNS detects a possible loop with DNS queries. The easiest but most crude way to workaround is to disable the systemd-resolved stub and revert the hosts /etc/resolv.conf to original Read more at CoreDNS troubleshooting docs . k0s server fails on ARM boxes # In the logs you probably see ETCD not starting up properly. Etcd is not fully supported on ARM architecture, thus you need to run k0s server and thus also etcd process with env ETCD_UNSUPPORTED_ARCH=arm64 . As Etcd is not fully supported on ARM architecture it also means that k0s controlplane with etcd itself is not fully supported on ARM either. Pods pending when using cloud providers # Once we enable cloud provider support on kubelet on worker nodes, kubelet will automatically add a taint node.cloudprovider.kubernetes.io/uninitialized for the node. This tain will prevent normal workloads to be scheduled on the node until the cloud provider controller actually runs second initialization on the node and removes the taint. This means that these nodes are not available for scheduling until the cloud provider controller is actually successfully running on the cluster. For troubleshooting your specific cloud provider see its documentation. k0s not working with read only /usr # By default k0s does not run on nodes where /usr is read only. This can be fixed by changing the default path for volumePluginDir in your k0s config. You will need to change to values, one for the kubelet itself, and one for Calico. Here is a snippet of an example config with the default values changed: spec : controllerManager : extraArgs : flex-volume-plugin-dir : \"/etc/kubernetes/kubelet-plugins/volume/exec\" network : calico : flexVolumeDriverPath : /etc/k0s/kubelet-plugins/volume/exec/nodeagent~uds workerProfiles : - name : coreos values : volumePluginDir : /etc/k0s/kubelet-plugins/volume/exec/ With this config you can start your server as usual. Any workers will need to be started with k0s worker --profile coreos [TOKEN]","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"There are few common cases we've seen where k0s fails to run properly.","title":"Troubleshooting"},{"location":"troubleshooting/#coredns-in-crashloop","text":"The most common case we've encountered so far has been CoreDNS getting into crashloop on the node(s). With kubectl you see something like this: $ kubectl get pod --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-5f6546844f-25px6 1 /1 Running 0 167m kube-system calico-node-fwjx5 1 /1 Running 0 164m kube-system calico-node-t4tx5 1 /1 Running 0 164m kube-system calico-node-whwsg 1 /1 Running 0 164m kube-system coredns-5c98d7d4d8-tfs4q 1 /1 Error 17 167m kube-system konnectivity-agent-9jkfd 1 /1 Running 0 164m kube-system konnectivity-agent-bvhdb 1 /1 Running 0 164m kube-system konnectivity-agent-r6mzj 1 /1 Running 0 164m kube-system kube-proxy-kr2r9 1 /1 Running 0 164m kube-system kube-proxy-tbljr 1 /1 Running 0 164m kube-system kube-proxy-xbw7p 1 /1 Running 0 164m kube-system metrics-server-7d4bcb75dd-pqkrs 1 /1 Running 0 167m When you check the logs, it'll show something like this: $ kubectl -n kube-system logs coredns-5c98d7d4d8-tfs4q plugin/loop: Loop (127.0.0.1:55953 -> :1053) detected for zone \".\", see https://coredns.io/plugins/loop#troubleshooting. Query: \"HINFO 4547991504243258144.3688648895315093531.\" This is most often caused by systemd-resolved stub (or something similar) running locally and CoreDNS detects a possible loop with DNS queries. The easiest but most crude way to workaround is to disable the systemd-resolved stub and revert the hosts /etc/resolv.conf to original Read more at CoreDNS troubleshooting docs .","title":"CoreDNS in crashloop"},{"location":"troubleshooting/#k0s-server-fails-on-arm-boxes","text":"In the logs you probably see ETCD not starting up properly. Etcd is not fully supported on ARM architecture, thus you need to run k0s server and thus also etcd process with env ETCD_UNSUPPORTED_ARCH=arm64 . As Etcd is not fully supported on ARM architecture it also means that k0s controlplane with etcd itself is not fully supported on ARM either.","title":"k0s server fails on ARM boxes"},{"location":"troubleshooting/#pods-pending-when-using-cloud-providers","text":"Once we enable cloud provider support on kubelet on worker nodes, kubelet will automatically add a taint node.cloudprovider.kubernetes.io/uninitialized for the node. This tain will prevent normal workloads to be scheduled on the node until the cloud provider controller actually runs second initialization on the node and removes the taint. This means that these nodes are not available for scheduling until the cloud provider controller is actually successfully running on the cluster. For troubleshooting your specific cloud provider see its documentation.","title":"Pods pending when using cloud providers"},{"location":"troubleshooting/#k0s-not-working-with-read-only-usr","text":"By default k0s does not run on nodes where /usr is read only. This can be fixed by changing the default path for volumePluginDir in your k0s config. You will need to change to values, one for the kubelet itself, and one for Calico. Here is a snippet of an example config with the default values changed: spec : controllerManager : extraArgs : flex-volume-plugin-dir : \"/etc/kubernetes/kubelet-plugins/volume/exec\" network : calico : flexVolumeDriverPath : /etc/k0s/kubelet-plugins/volume/exec/nodeagent~uds workerProfiles : - name : coreos values : volumePluginDir : /etc/k0s/kubelet-plugins/volume/exec/ With this config you can start your server as usual. Any workers will need to be started with k0s worker --profile coreos [TOKEN]","title":"k0s not working with read only /usr"},{"location":"cli/","text":"k0s # k0s - Zero Friction Kubernetes Synopsis # k0s - The zero friction Kubernetes - https://k0sproject.io Options # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -h, --help help for k0s -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1]) SEE ALSO # k0s api - Run the controller api k0s completion - Generate completion script k0s default-config - Output the default k0s configuration yaml to stdout k0s docs - Generate Markdown docs for the k0s binary k0s etcd - Manage etcd cluster k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo) k0s kubeconfig - Create a kubeconfig file for a specified user k0s server - Run server k0s token - Manage join tokens k0s version - Print the k0s version k0s worker - Run worker","title":"Index"},{"location":"cli/#k0s","text":"k0s - Zero Friction Kubernetes","title":"k0s"},{"location":"cli/#synopsis","text":"k0s - The zero friction Kubernetes - https://k0sproject.io","title":"Synopsis"},{"location":"cli/#options","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -h, --help help for k0s -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])","title":"Options"},{"location":"cli/#see-also","text":"k0s api - Run the controller api k0s completion - Generate completion script k0s default-config - Output the default k0s configuration yaml to stdout k0s docs - Generate Markdown docs for the k0s binary k0s etcd - Manage etcd cluster k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo) k0s kubeconfig - Create a kubeconfig file for a specified user k0s server - Run server k0s token - Manage join tokens k0s version - Print the k0s version k0s worker - Run worker","title":"SEE ALSO"},{"location":"cli/k0s/","text":"k0s # k0s - Zero Friction Kubernetes Synopsis # k0s - The zero friction Kubernetes - https://k0sproject.io Options # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -h, --help help for k0s -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1]) SEE ALSO # k0s api - Run the controller api k0s completion - Generate completion script k0s default-config - Output the default k0s configuration yaml to stdout k0s docs - Generate Markdown docs for the k0s binary k0s etcd - Manage etcd cluster k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo) k0s kubeconfig - Create a kubeconfig file for a specified user k0s server - Run server k0s token - Manage join tokens k0s version - Print the k0s version k0s worker - Run worker","title":"k0s CLI Help Pages"},{"location":"cli/k0s/#k0s","text":"k0s - Zero Friction Kubernetes","title":"k0s"},{"location":"cli/k0s/#synopsis","text":"k0s - The zero friction Kubernetes - https://k0sproject.io","title":"Synopsis"},{"location":"cli/k0s/#options","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -h, --help help for k0s -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])","title":"Options"},{"location":"cli/k0s/#see-also","text":"k0s api - Run the controller api k0s completion - Generate completion script k0s default-config - Output the default k0s configuration yaml to stdout k0s docs - Generate Markdown docs for the k0s binary k0s etcd - Manage etcd cluster k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo) k0s kubeconfig - Create a kubeconfig file for a specified user k0s server - Run server k0s token - Manage join tokens k0s version - Print the k0s version k0s worker - Run worker","title":"SEE ALSO"},{"location":"cli/k0s_api/","text":"k0s api # Run the controller api k0s api [flags] Options # -h, --help help for api Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s api"},{"location":"cli/k0s_api/#k0s-api","text":"Run the controller api k0s api [flags]","title":"k0s api"},{"location":"cli/k0s_api/#options","text":"-h, --help help for api","title":"Options"},{"location":"cli/k0s_api/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_api/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_completion/","text":"k0s completion # Generate completion script Synopsis # To load completions: Bash: $ source <(k0s completion bash) To load completions for each session, execute once: # $ k0s completion bash > /etc/bash_completion.d/k0s Zsh: If shell completion is not already enabled in your environment you will need # to enable it. You can execute the following once: # $ echo \"autoload -U compinit; compinit\" >> ~/.zshrc To load completions for each session, execute once: # $ k0s completion zsh > \"${fpath[1]}/_k0s\" You will need to start a new shell for this setup to take effect. # Fish: $ k0s completion fish | source To load completions for each session, execute once: # $ k0s completion fish > ~/.config/fish/completions/k0s.fish k0s completion [bash|zsh|fish|powershell] Options # -h, --help help for completion Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s completion"},{"location":"cli/k0s_completion/#k0s-completion","text":"Generate completion script","title":"k0s completion"},{"location":"cli/k0s_completion/#synopsis","text":"To load completions: Bash: $ source <(k0s completion bash)","title":"Synopsis"},{"location":"cli/k0s_completion/#to-load-completions-for-each-session-execute-once","text":"$ k0s completion bash > /etc/bash_completion.d/k0s Zsh:","title":"To load completions for each session, execute once:"},{"location":"cli/k0s_completion/#if-shell-completion-is-not-already-enabled-in-your-environment-you-will-need","text":"","title":"If shell completion is not already enabled in your environment you will need"},{"location":"cli/k0s_completion/#to-enable-it-you-can-execute-the-following-once","text":"$ echo \"autoload -U compinit; compinit\" >> ~/.zshrc","title":"to enable it.  You can execute the following once:"},{"location":"cli/k0s_completion/#to-load-completions-for-each-session-execute-once_1","text":"$ k0s completion zsh > \"${fpath[1]}/_k0s\"","title":"To load completions for each session, execute once:"},{"location":"cli/k0s_completion/#you-will-need-to-start-a-new-shell-for-this-setup-to-take-effect","text":"Fish: $ k0s completion fish | source","title":"You will need to start a new shell for this setup to take effect."},{"location":"cli/k0s_completion/#to-load-completions-for-each-session-execute-once_2","text":"$ k0s completion fish > ~/.config/fish/completions/k0s.fish k0s completion [bash|zsh|fish|powershell]","title":"To load completions for each session, execute once:"},{"location":"cli/k0s_completion/#options","text":"-h, --help help for completion","title":"Options"},{"location":"cli/k0s_completion/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_completion/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_default-config/","text":"k0s default-config # Output the default k0s configuration yaml to stdout k0s default-config [flags] Options # -h, --help help for default-config Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s default config"},{"location":"cli/k0s_default-config/#k0s-default-config","text":"Output the default k0s configuration yaml to stdout k0s default-config [flags]","title":"k0s default-config"},{"location":"cli/k0s_default-config/#options","text":"-h, --help help for default-config","title":"Options"},{"location":"cli/k0s_default-config/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_default-config/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_docs/","text":"k0s docs # Generate Markdown docs for the k0s binary k0s docs [flags] Options # -h, --help help for docs Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s docs"},{"location":"cli/k0s_docs/#k0s-docs","text":"Generate Markdown docs for the k0s binary k0s docs [flags]","title":"k0s docs"},{"location":"cli/k0s_docs/#options","text":"-h, --help help for docs","title":"Options"},{"location":"cli/k0s_docs/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_docs/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_etcd/","text":"k0s etcd # Manage etcd cluster Options # -h, --help help for etcd Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes k0s etcd leave - Sign off a given etc node from etcd cluster k0s etcd member-list - Returns etcd cluster members list","title":"K0s etcd"},{"location":"cli/k0s_etcd/#k0s-etcd","text":"Manage etcd cluster","title":"k0s etcd"},{"location":"cli/k0s_etcd/#options","text":"-h, --help help for etcd","title":"Options"},{"location":"cli/k0s_etcd/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_etcd/#see-also","text":"k0s - k0s - Zero Friction Kubernetes k0s etcd leave - Sign off a given etc node from etcd cluster k0s etcd member-list - Returns etcd cluster members list","title":"SEE ALSO"},{"location":"cli/k0s_etcd_leave/","text":"k0s etcd leave # Sign off a given etc node from etcd cluster k0s etcd leave [flags] Options # -h, --help help for leave --peer-address string etcd peer address Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1]) SEE ALSO # k0s etcd - Manage etcd cluster","title":"K0s etcd leave"},{"location":"cli/k0s_etcd_leave/#k0s-etcd-leave","text":"Sign off a given etc node from etcd cluster k0s etcd leave [flags]","title":"k0s etcd leave"},{"location":"cli/k0s_etcd_leave/#options","text":"-h, --help help for leave --peer-address string etcd peer address","title":"Options"},{"location":"cli/k0s_etcd_leave/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_etcd_leave/#see-also","text":"k0s etcd - Manage etcd cluster","title":"SEE ALSO"},{"location":"cli/k0s_etcd_member-list/","text":"k0s etcd member-list # Returns etcd cluster members list k0s etcd member-list [flags] Options # -h, --help help for member-list Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1]) SEE ALSO # k0s etcd - Manage etcd cluster","title":"K0s etcd member list"},{"location":"cli/k0s_etcd_member-list/#k0s-etcd-member-list","text":"Returns etcd cluster members list k0s etcd member-list [flags]","title":"k0s etcd member-list"},{"location":"cli/k0s_etcd_member-list/#options","text":"-h, --help help for member-list","title":"Options"},{"location":"cli/k0s_etcd_member-list/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_etcd_member-list/#see-also","text":"k0s etcd - Manage etcd cluster","title":"SEE ALSO"},{"location":"cli/k0s_install/","text":"k0s install # Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo) k0s install [flags] Options # -h, --help help for install --role string node role (possible values: server or worker. In a single-node setup, a worker role should be used) (default \"server\") Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s install"},{"location":"cli/k0s_install/#k0s-install","text":"Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo) k0s install [flags]","title":"k0s install"},{"location":"cli/k0s_install/#options","text":"-h, --help help for install --role string node role (possible values: server or worker. In a single-node setup, a worker role should be used) (default \"server\")","title":"Options"},{"location":"cli/k0s_install/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_install/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_kubeconfig/","text":"k0s kubeconfig # Create a kubeconfig file for a specified user k0s kubeconfig [command] [flags] Options # -h, --help help for kubeconfig Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes k0s kubeconfig admin - Display Admin's Kubeconfig file k0s kubeconfig create - Create a kubeconfig for a user","title":"K0s kubeconfig"},{"location":"cli/k0s_kubeconfig/#k0s-kubeconfig","text":"Create a kubeconfig file for a specified user k0s kubeconfig [command] [flags]","title":"k0s kubeconfig"},{"location":"cli/k0s_kubeconfig/#options","text":"-h, --help help for kubeconfig","title":"Options"},{"location":"cli/k0s_kubeconfig/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_kubeconfig/#see-also","text":"k0s - k0s - Zero Friction Kubernetes k0s kubeconfig admin - Display Admin's Kubeconfig file k0s kubeconfig create - Create a kubeconfig for a user","title":"SEE ALSO"},{"location":"cli/k0s_kubeconfig_admin/","text":"k0s kubeconfig admin # Display Admin's Kubeconfig file Synopsis # Print kubeconfig for the Admin user to stdout k0s kubeconfig admin [command] [flags] Examples # $ k0s kubeconfig admin > ~/.kube/config $ export KUBECONFIG=~/.kube/config $ kubectl get nodes Options # -h, --help help for admin Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1]) SEE ALSO # k0s kubeconfig - Create a kubeconfig file for a specified user","title":"K0s kubeconfig admin"},{"location":"cli/k0s_kubeconfig_admin/#k0s-kubeconfig-admin","text":"Display Admin's Kubeconfig file","title":"k0s kubeconfig admin"},{"location":"cli/k0s_kubeconfig_admin/#synopsis","text":"Print kubeconfig for the Admin user to stdout k0s kubeconfig admin [command] [flags]","title":"Synopsis"},{"location":"cli/k0s_kubeconfig_admin/#examples","text":"$ k0s kubeconfig admin > ~/.kube/config $ export KUBECONFIG=~/.kube/config $ kubectl get nodes","title":"Examples"},{"location":"cli/k0s_kubeconfig_admin/#options","text":"-h, --help help for admin","title":"Options"},{"location":"cli/k0s_kubeconfig_admin/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_kubeconfig_admin/#see-also","text":"k0s kubeconfig - Create a kubeconfig file for a specified user","title":"SEE ALSO"},{"location":"cli/k0s_kubeconfig_create/","text":"k0s kubeconfig create # Create a kubeconfig for a user Synopsis # Create a kubeconfig with a signed certificate and public key for a given user (and optionally user groups) Note: A certificate once signed cannot be revoked for a particular user k0s kubeconfig create [username] [flags] Examples # Command to create a kubeconfig for a user: CLI argument: $ k0s kubeconfig create [username] optionally add groups: $ k0s kubeconfig create [username] --groups [groups] Options # --groups string Specify groups -h, --help help for create Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1]) SEE ALSO # k0s kubeconfig - Create a kubeconfig file for a specified user","title":"K0s kubeconfig create"},{"location":"cli/k0s_kubeconfig_create/#k0s-kubeconfig-create","text":"Create a kubeconfig for a user","title":"k0s kubeconfig create"},{"location":"cli/k0s_kubeconfig_create/#synopsis","text":"Create a kubeconfig with a signed certificate and public key for a given user (and optionally user groups) Note: A certificate once signed cannot be revoked for a particular user k0s kubeconfig create [username] [flags]","title":"Synopsis"},{"location":"cli/k0s_kubeconfig_create/#examples","text":"Command to create a kubeconfig for a user: CLI argument: $ k0s kubeconfig create [username] optionally add groups: $ k0s kubeconfig create [username] --groups [groups]","title":"Examples"},{"location":"cli/k0s_kubeconfig_create/#options","text":"--groups string Specify groups -h, --help help for create","title":"Options"},{"location":"cli/k0s_kubeconfig_create/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_kubeconfig_create/#see-also","text":"k0s kubeconfig - Create a kubeconfig file for a specified user","title":"SEE ALSO"},{"location":"cli/k0s_server/","text":"k0s server # Run server k0s server [join-token] [flags] Examples # Command to associate master nodes: CLI argument: $ k0s server [join-token] or CLI flag: $ k0s server --token-file [path_to_file] Note: Token can be passed either as a CLI argument or as a flag Options # --enable-worker enable worker (default false) -h, --help help for server --profile string worker profile to use on the node (default \"default\") --token-file string Path to the file containing join-token. Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s server"},{"location":"cli/k0s_server/#k0s-server","text":"Run server k0s server [join-token] [flags]","title":"k0s server"},{"location":"cli/k0s_server/#examples","text":"Command to associate master nodes: CLI argument: $ k0s server [join-token] or CLI flag: $ k0s server --token-file [path_to_file] Note: Token can be passed either as a CLI argument or as a flag","title":"Examples"},{"location":"cli/k0s_server/#options","text":"--enable-worker enable worker (default false) -h, --help help for server --profile string worker profile to use on the node (default \"default\") --token-file string Path to the file containing join-token.","title":"Options"},{"location":"cli/k0s_server/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_server/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_token/","text":"k0s token # Manage join tokens k0s token [flags] Options # -h, --help help for token --kubeconfig string path to kubeconfig file [$KUBECONFIG] Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes k0s token create - Create join token","title":"K0s token"},{"location":"cli/k0s_token/#k0s-token","text":"Manage join tokens k0s token [flags]","title":"k0s token"},{"location":"cli/k0s_token/#options","text":"-h, --help help for token --kubeconfig string path to kubeconfig file [$KUBECONFIG]","title":"Options"},{"location":"cli/k0s_token/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_token/#see-also","text":"k0s - k0s - Zero Friction Kubernetes k0s token create - Create join token","title":"SEE ALSO"},{"location":"cli/k0s_token_create/","text":"k0s token create # Create join token k0s token create [flags] Options # --expiry string set duration time for token (default \"0\") -h, --help help for create --role string Either worker or controller (default \"worker\") --wait wait forever (default false) Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1]) SEE ALSO # k0s token - Manage join tokens","title":"K0s token create"},{"location":"cli/k0s_token_create/#k0s-token-create","text":"Create join token k0s token create [flags]","title":"k0s token create"},{"location":"cli/k0s_token_create/#options","text":"--expiry string set duration time for token (default \"0\") -h, --help help for create --role string Either worker or controller (default \"worker\") --wait wait forever (default false)","title":"Options"},{"location":"cli/k0s_token_create/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_token_create/#see-also","text":"k0s token - Manage join tokens","title":"SEE ALSO"},{"location":"cli/k0s_version/","text":"k0s version # Print the k0s version k0s version [flags] Options # -h, --help help for version Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s version"},{"location":"cli/k0s_version/#k0s-version","text":"Print the k0s version k0s version [flags]","title":"k0s version"},{"location":"cli/k0s_version/#options","text":"-h, --help help for version","title":"Options"},{"location":"cli/k0s_version/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_version/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_worker/","text":"k0s worker # Run worker k0s worker [join-token] [flags] Examples # Command to add worker node to the master node: CLI argument: $ k0s worker [token] or CLI flag: $ k0s worker --token-file [path_to_file] Note: Token can be passed either as a CLI argument or as a flag Options # --cri-socket string contrainer runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket] --enable-cloud-provider Whether or not to enable cloud provider support in kubelet -h, --help help for worker --profile string worker profile to use on the node (default \"default\") --token-file string Path to the file containing token. Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s worker"},{"location":"cli/k0s_worker/#k0s-worker","text":"Run worker k0s worker [join-token] [flags]","title":"k0s worker"},{"location":"cli/k0s_worker/#examples","text":"Command to add worker node to the master node: CLI argument: $ k0s worker [token] or CLI flag: $ k0s worker --token-file [path_to_file] Note: Token can be passed either as a CLI argument or as a flag","title":"Examples"},{"location":"cli/k0s_worker/#options","text":"--cri-socket string contrainer runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket] --enable-cloud-provider Whether or not to enable cloud provider support in kubelet -h, --help help for worker --profile string worker profile to use on the node (default \"default\") --token-file string Path to the file containing token.","title":"Options"},{"location":"cli/k0s_worker/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-controller-manager=1,kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_worker/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"contributors/CODE_OF_CONDUCT/","text":"k0s Community Code of Conduct # k0s follows the CNCF Code of Conduct .","title":"k0s Community Code of Conduct"},{"location":"contributors/CODE_OF_CONDUCT/#k0s-community-code-of-conduct","text":"k0s follows the CNCF Code of Conduct .","title":"k0s Community Code of Conduct"},{"location":"contributors/github_workflow/","text":"Github Workflow # Fork The Project Adding the Forked Remote Create & Rebase Your Feature Branch Commit & Push Open a Pull Request Get a code review Squash commits Push Your Final Changes This guide assumes you have already cloned the upstream repo to your system via git clone, or via go get github.com/k0sproject/k0s . Fork The Project # Go to http://github.com/k0sproject/k0s On the top, right-hand side, click on \"fork\" and select your username for the fork destination. Adding the Forked Remote # export GITHUB_USER={ your github's username } cd $WORKDIR/k0s git remote add $GITHUB_USER git@github.com:${GITHUB_USER}/k0s.git # Prevent push to Upstream git remote set-url --push origin no_push # Set your fork remote as a default push target git push --set-upstream $GITHUB_USER main Your remotes should look something like this: \u279c git remote -v origin https://github.com/k0sproject/k0s (fetch) origin no_push (push) my_fork git@github.com:{ github_username }/k0s.git (fetch) my_fork git@github.com:{ github_username }/k0s.git (push) Create & Rebase Your Feature Branch # Create a feature branch: git branch -b my_feature_branch Rebase your branch: git fetch origin git rebase origin/main Current branch my_feature_branch is up to date. Please don't use git pull instead of the above fetch / rebase . git pull does a merge, which leaves merge commits. These make the commit history messy and violate the principle that commits ought to be individually understandable and useful. Commit & Push # Commit and sign your changes: git commit -m \"my commit title\" --signoff You can go back and edit/build/test some more, then commit --amend in a few cycles. When ready, push your changes to your fork's repository: git push --set-upstream my_fork my_feature_branch Open a Pull Request # Github Docs Get a code review # Once your pull request has been opened it will be assigned to one or more reviewers, and will go through a series of smoke tests. Commit changes made in response to review comments should be added to the same branch on your fork. Very small PRs are easy to review. Very large PRs are very difficult to review. Squashing Commits # Commits on your branch should represent meaningful milestones or units of work. Small commits that contain typo fixes, rebases, review feedbacks, etc should be squashed. To do that, it's best to perform an interactive rebase : Example If you PR has 3 commits, count backwards from your last commit using HEAD~3 : git rebase -i HEAD~3 Output would be similar to this: pick f7f3f6d Changed some code pick 310154e fixed some typos pick a5f4a0d made some review changes # Rebase 710f0f8..a5f4a0d onto 710f0f8 # # Commands: # p, pick <commit> = use commit # r, reword <commit> = use commit, but edit the commit message # e, edit <commit> = use commit, but stop for amending # s, squash <commit> = use commit, but meld into previous commit # f, fixup <commit> = like \"squash\", but discard this commit's log message # x, exec <command> = run command (the rest of the line) using shell # b, break = stop here (continue rebase later with 'git rebase --continue') # d, drop <commit> = remove commit # l, label <label> = label current HEAD with a name # t, reset <label> = reset HEAD to a label # m, merge [-C <commit> | -c <commit>] <label> [# <oneline>] # . create a merge commit using the original merge commit's # . message (or the oneline, if no original merge commit was # . specified). Use -c <commit> to reword the commit message. # # These lines can be re-ordered; they are executed from top to bottom. # # However, if you remove everything, the rebase will be aborted. # # Note that empty commits are commented out Use a command line text editor to change the word pick to fixup for the commits you want to squash, then save your changes and continue the rebase: Per the output above, you can see that: fixup <commit> = like \"squash\", but discard this commit's log message Which means that when rebased, the commit message \"fixed some typos\" will be removed, and squashed with the parent commit. Push Your Final Changes # Once done, you can push the final commits to your branch: git push --force You can run multiple iteration of rebase / push -f , if needed.","title":"Workflow"},{"location":"contributors/github_workflow/#github-workflow","text":"Fork The Project Adding the Forked Remote Create & Rebase Your Feature Branch Commit & Push Open a Pull Request Get a code review Squash commits Push Your Final Changes This guide assumes you have already cloned the upstream repo to your system via git clone, or via go get github.com/k0sproject/k0s .","title":"Github Workflow"},{"location":"contributors/github_workflow/#fork-the-project","text":"Go to http://github.com/k0sproject/k0s On the top, right-hand side, click on \"fork\" and select your username for the fork destination.","title":"Fork The Project"},{"location":"contributors/github_workflow/#adding-the-forked-remote","text":"export GITHUB_USER={ your github's username } cd $WORKDIR/k0s git remote add $GITHUB_USER git@github.com:${GITHUB_USER}/k0s.git # Prevent push to Upstream git remote set-url --push origin no_push # Set your fork remote as a default push target git push --set-upstream $GITHUB_USER main Your remotes should look something like this: \u279c git remote -v origin https://github.com/k0sproject/k0s (fetch) origin no_push (push) my_fork git@github.com:{ github_username }/k0s.git (fetch) my_fork git@github.com:{ github_username }/k0s.git (push)","title":"Adding the Forked Remote"},{"location":"contributors/github_workflow/#create-rebase-your-feature-branch","text":"Create a feature branch: git branch -b my_feature_branch Rebase your branch: git fetch origin git rebase origin/main Current branch my_feature_branch is up to date. Please don't use git pull instead of the above fetch / rebase . git pull does a merge, which leaves merge commits. These make the commit history messy and violate the principle that commits ought to be individually understandable and useful.","title":"Create &amp; Rebase Your Feature Branch"},{"location":"contributors/github_workflow/#commit-push","text":"Commit and sign your changes: git commit -m \"my commit title\" --signoff You can go back and edit/build/test some more, then commit --amend in a few cycles. When ready, push your changes to your fork's repository: git push --set-upstream my_fork my_feature_branch","title":"Commit &amp; Push"},{"location":"contributors/github_workflow/#open-a-pull-request","text":"Github Docs","title":"Open a Pull Request"},{"location":"contributors/github_workflow/#get-a-code-review","text":"Once your pull request has been opened it will be assigned to one or more reviewers, and will go through a series of smoke tests. Commit changes made in response to review comments should be added to the same branch on your fork. Very small PRs are easy to review. Very large PRs are very difficult to review.","title":"Get a code review"},{"location":"contributors/github_workflow/#squashing-commits","text":"Commits on your branch should represent meaningful milestones or units of work. Small commits that contain typo fixes, rebases, review feedbacks, etc should be squashed. To do that, it's best to perform an interactive rebase :","title":"Squashing Commits"},{"location":"contributors/github_workflow/#push-your-final-changes","text":"Once done, you can push the final commits to your branch: git push --force You can run multiple iteration of rebase / push -f , if needed.","title":"Push Your Final Changes"},{"location":"contributors/overview/","text":"Contributing to k0s # Thank you for taking the time to make a contribution to k0s. The following document is a set of guidelines and instructions for contributing to k0s. When contributing to this repository, please consider first discussing the change you wish to make by opening an issue. Code of Conduct # Our code of conduct can be found in the link below. Please follow it in all your interactions with the project. Code Of Conduct Github Workflow # We Use Github Flow , so all code changes are tracked via Pull Requests. A detailed guide on the recommended workflow can be found below: Github Workflow Code Testing # All submitted PRs go through a set of tests and reviews. You can run most of these tests before a PR is submitted. In fact, we recommend it, because it will save on many possible review iterations and automated tests. The testing guidelines can be found here: Contributor's Guide to Testing License # By contributing, you agree that your contributions will be licensed as followed: All content residing under the \"docs/\" directory of this repository is licensed under \"Creative Commons Attribution Share Alike 4.0 International\" (CC-BY-SA-4.0). See docs/LICENCE for details. Content outside of the above mentioned directories or restrictions above is available under the \"Apache License 2.0\". Community # Some of you might have noticed we have official community blog hosted on Medium . If you are not yet following us, we'd like to invite you to do so now! Make sure to follow us on Twitter as well \ud83d\ude0a We have also agreed to share Slack with Lens IDE team. They are close friends of the k0s crew, and many of the Kubernetes users are using Lens anyway, so it felt very natural for us to share their Slack. You\u2019ll find us on dedicated k0s channel. Please join the k0s Slack channel to hear the latest news, discussions and provide your feedback!","title":"Overview"},{"location":"contributors/overview/#contributing-to-k0s","text":"Thank you for taking the time to make a contribution to k0s. The following document is a set of guidelines and instructions for contributing to k0s. When contributing to this repository, please consider first discussing the change you wish to make by opening an issue.","title":"Contributing to k0s"},{"location":"contributors/overview/#code-of-conduct","text":"Our code of conduct can be found in the link below. Please follow it in all your interactions with the project. Code Of Conduct","title":"Code of Conduct"},{"location":"contributors/overview/#github-workflow","text":"We Use Github Flow , so all code changes are tracked via Pull Requests. A detailed guide on the recommended workflow can be found below: Github Workflow","title":"Github Workflow"},{"location":"contributors/overview/#code-testing","text":"All submitted PRs go through a set of tests and reviews. You can run most of these tests before a PR is submitted. In fact, we recommend it, because it will save on many possible review iterations and automated tests. The testing guidelines can be found here: Contributor's Guide to Testing","title":"Code Testing"},{"location":"contributors/overview/#license","text":"By contributing, you agree that your contributions will be licensed as followed: All content residing under the \"docs/\" directory of this repository is licensed under \"Creative Commons Attribution Share Alike 4.0 International\" (CC-BY-SA-4.0). See docs/LICENCE for details. Content outside of the above mentioned directories or restrictions above is available under the \"Apache License 2.0\".","title":"License"},{"location":"contributors/overview/#community","text":"Some of you might have noticed we have official community blog hosted on Medium . If you are not yet following us, we'd like to invite you to do so now! Make sure to follow us on Twitter as well \ud83d\ude0a We have also agreed to share Slack with Lens IDE team. They are close friends of the k0s crew, and many of the Kubernetes users are using Lens anyway, so it felt very natural for us to share their Slack. You\u2019ll find us on dedicated k0s channel. Please join the k0s Slack channel to hear the latest news, discussions and provide your feedback!","title":"Community"},{"location":"contributors/testing/","text":"Testing Your Code # k0s uses github actions to run automated tests on any PR, before merging. However, a PR will not be reviewed before all tests are green, so to save time and prevent your PR from going stale, it is best to test it before submitting the PR. Run Local Verifications # Please run the following style and formatting commands and fix/check-in any changes: 1. Linting We use golangci-lint for style verification. In the repository's root directory, simply run: make lint 2. Go fmt go fmt ./... 3. Pre-submit Flight Checks In the repository root directory, make sure that: make build runs successfully. make check-basic runs successfully. make check-unit has no errors. make check-hacontrolplane runs successfully. Please note that this last test is prone to \"flakiness\", so it might fail on occasion. If it fails constantly, take a deeper look at your code to find the source of the problem. If you find that all tests passed, you may open a pull request upstream. Opening A Pull Request # Draft Mode You may open a pull request in draft mode . All automated tests will still run against the PR, but the PR will not be assigned for review. Once a PR is ready for review, transition it from Draft mode, and code owners will be notified. Conformance Testing Once a PR has been reviewed and all other tests have passed, a code owner will run a full end-to-end conformance test against the PR. This is usually the last step before merging. Pre-Requisites for PR Merge In order for a PR to be merged, the following conditions should exist: 1. The PR has passed all the automated tests (style, build & conformance tests). 2. PR commits have been signed with the --signoff option. 3. PR was reviewed and approved by a code owner. 4. PR is rebased against upstream's main branch.","title":"Testing"},{"location":"contributors/testing/#testing-your-code","text":"k0s uses github actions to run automated tests on any PR, before merging. However, a PR will not be reviewed before all tests are green, so to save time and prevent your PR from going stale, it is best to test it before submitting the PR.","title":"Testing Your Code"},{"location":"contributors/testing/#run-local-verifications","text":"Please run the following style and formatting commands and fix/check-in any changes:","title":"Run Local Verifications"},{"location":"contributors/testing/#opening-a-pull-request","text":"","title":"Opening A Pull Request"},{"location":"examples/ansible-playbook/","text":"Install a k0s cluster with a Ansible Playbook # Using Ansible and the k0s-ansible playbook, you can install a multi-node Kubernetes Cluster in a couple of minutes. Ansible is a popular infrastructure as code tool which helps you automate tasks to achieve the desired state in a system. This guide shows how you can install k0s on local virtual machines. In this guide, the following tools are used: multipass , a lightweight VM manager that uses KVM on Linux, Hyper-V on Windows, and hypervisor.framework on macOS ( installation guide ). ansible , a popular infrastructure as code tool ( installation guide ). and of course kubectl on your local machine ( Installation guide ). Before following this tutorial, you should have a general understanding of Ansible. A great way to start is the official Ansible User Guide . Please note: k0s users created k0s-ansible. Please send your feedback, bug reports, and pull requests to github.com/movd/k0s-ansible . Without further ado, let's jump right in. Download k0s-ansible # On your local machine clone the k0s-ansible repository: $ git clone https://github.com/movd/k0s-ansible.git $ cd k0s-ansible Create virtual machines # For this tutorial, multipass was used. However, there is no interdependence. This playbook should also work with VMs created in alternative ways or Raspberry Pis. Next, create a couple of virtual machines. For the automation to work, each instance must have passwordless SSH access. To achieve this, we provision each instance with a cloud-init manifest that imports your current users' public SSH key and into a user k0s . For your convenience, a bash script is included that does just that: ./tools/multipass_create_instances.sh 7 \u25c0\ufe0f this creates 7 virtual machines $ ./tools/multipass_create_instances.sh 7 Create cloud-init to import ssh key... [1/7] Creating instance k0s-1 with multipass... Launched: k0s-1 [2/7] Creating instance k0s-2 with multipass... Launched: k0s-2 [3/7] Creating instance k0s-3 with multipass... Launched: k0s-3 [4/7] Creating instance k0s-4 with multipass... Launched: k0s-4 [5/7] Creating instance k0s-5 with multipass... Launched: k0s-5 [6/7] Creating instance k0s-6 with multipass... Launched: k0s-6 [7/7] Creating instance k0s-7 with multipass... Launched: k0s-7 Name State IPv4 Image k0s-1 Running 192.168.64.32 Ubuntu 20.04 LTS k0s-2 Running 192.168.64.33 Ubuntu 20.04 LTS k0s-3 Running 192.168.64.56 Ubuntu 20.04 LTS k0s-4 Running 192.168.64.57 Ubuntu 20.04 LTS k0s-5 Running 192.168.64.58 Ubuntu 20.04 LTS k0s-6 Running 192.168.64.60 Ubuntu 20.04 LTS k0s-7 Running 192.168.64.61 Ubuntu 20.04 LTS Create Ansible inventory # After that, we create our inventory directory by copying the sample: $ cp -rfp inventory/sample inventory/multipass Now we need to create our inventory. The before built virtual machines need to be assigned to the different host groups required by the playbook's logic. initial_controller = must contain a single node that creates the worker and server tokens needed by the other nodes. controller = can contain nodes that, together with the host from initial_controller form a highly available isolated control plane. worker = must contain at least one node so that we can deploy Kubernetes objects. We could fill inventory/multipass/inventory.yml by hand with the metadata provided by multipass list, but since we are lazy and want to automate as much as possible, we can use the included Python script multipass_generate_inventory.py : To automatically fill our inventory run: $ ./tools/multipass_generate_inventory.py Designate first three instances as control plane Created Ansible Inventory at: /Users/dev/k0s-ansible/tools/inventory.yml $ cp tools/inventory.yml inventory/multipass/inventory.yml Now inventory/multipass/inventory.yml should look like this (Of course, your IP addresses might differ): --- all : children : initial_controller : hosts : k0s-1 : controller : hosts : k0s-2 : k0s-3 : worker : hosts : k0s-4 : k0s-5 : k0s-6 : k0s-7 : hosts : k0s-1 : ansible_host : 192.168.64.32 k0s-2 : ansible_host : 192.168.64.33 k0s-3 : ansible_host : 192.168.64.56 k0s-4 : ansible_host : 192.168.64.57 k0s-5 : ansible_host : 192.168.64.58 k0s-6 : ansible_host : 192.168.64.60 k0s-7 : ansible_host : 192.168.64.61 vars : ansible_user : k0s Test the connection to the virtual machines # To test the connection to your hosts just run: $ ansible -i inventory/multipass/inventory.yml -m ping k0s-4 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" } ... If all is green and successful, you can proceed. Provision the cluster with Ansible # Finally, we can start provisioning the cluster. Applying the playbook, k0s will get downloaded and set up on all nodes, tokens will get exchanged, and a kubeconfig will get dumped to your local deployment environment. $ ansible-playbook site.yml -i inventory/multipass/inventory.yml ... TASK [k0s/initial_controller : print kubeconfig command] ******************************************************* Tuesday 22 December 2020 17:43:20 +0100 (0:00:00.257) 0:00:41.287 ****** ok: [k0s-1] => { \"msg\": \"To use Cluster: export KUBECONFIG=/Users/dev/k0s-ansible/inventory/multipass/artifacts/k0s-kubeconfig.yml\" } ... PLAY RECAP ***************************************************************************************************** k0s-1 : ok=21 changed=11 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 k0s-2 : ok=10 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 k0s-3 : ok=10 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 k0s-4 : ok=9 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 k0s-5 : ok=9 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 k0s-6 : ok=9 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 k0s-7 : ok=9 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 Tuesday 22 December 2020 17:43:36 +0100 (0:00:01.204) 0:00:57.478 ****** =============================================================================== prereq : Install apt packages -------------------------------------------------------------------------- 22.70s k0s/controller : Wait for k8s apiserver ----------------------------------------------------------------- 4.30s k0s/initial_controller : Create worker join token ------------------------------------------------------- 3.38s k0s/initial_controller : Wait for k8s apiserver --------------------------------------------------------- 3.36s download : Download k0s binary k0s-v0.9.0-rc1-amd64 ----------------------------------------------------- 3.11s Gathering Facts ----------------------------------------------------------------------------------------- 2.85s Gathering Facts ----------------------------------------------------------------------------------------- 1.95s prereq : Create k0s Directories ------------------------------------------------------------------------- 1.53s k0s/worker : Enable and check k0s service --------------------------------------------------------------- 1.20s prereq : Write the k0s config file ---------------------------------------------------------------------- 1.09s k0s/initial_controller : Enable and check k0s service --------------------------------------------------- 0.94s k0s/controller : Enable and check k0s service ----------------------------------------------------------- 0.73s Gathering Facts ----------------------------------------------------------------------------------------- 0.71s Gathering Facts ----------------------------------------------------------------------------------------- 0.66s Gathering Facts ----------------------------------------------------------------------------------------- 0.64s k0s/worker : Write the k0s token file on worker --------------------------------------------------------- 0.64s k0s/worker : Copy k0s service file ---------------------------------------------------------------------- 0.53s k0s/controller : Write the k0s token file on controller ------------------------------------------------- 0.41s k0s/controller : Copy k0s service file ------------------------------------------------------------------ 0.40s k0s/initial_controller : Copy k0s service file ---------------------------------------------------------- 0.36s Use the cluster with kubectl # While the playbook ran, a kubeconfig got copied to your local machine. You can use it to get simple access to your new Kubernetes cluster: $ export KUBECONFIG=/Users/dev/k0s-ansible/inventory/multipass/artifacts/k0s-kubeconfig.yml $ kubectl cluster-info Kubernetes control plane is running at https://192.168.64.32:6443 CoreDNS is running at https://192.168.64.32:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://192.168.64.32:6443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k0s-4 Ready <none> 21s v1.20.1-k0s1 192.168.64.57 <none> Ubuntu 20.04.1 LTS 5.4.0-54-generic containerd://1.4.3 k0s-5 Ready <none> 21s v1.20.1-k0s1 192.168.64.58 <none> Ubuntu 20.04.1 LTS 5.4.0-54-generic containerd://1.4.3 k0s-6 NotReady <none> 21s v1.20.1-k0s1 192.168.64.60 <none> Ubuntu 20.04.1 LTS 5.4.0-54-generic containerd://1.4.3 k0s-7 NotReady <none> 21s v1.20.1-k0s1 192.168.64.61 <none> Ubuntu 20.04.1 LTS 5.4.0-54-generic containerd://1.4.3 \u2b06\ufe0f Of course, the first three control plane nodes won't show up here because the control plane is fully isolated. You can check on the distributed etcd cluster by running this ad-hoc command (or ssh'ing directly into a controller node): $ ansible k0s-1 -a \"k0s etcd member-list -c /etc/k0s/k0s.yaml\" -i inventory/multipass/inventory.yml | tail -1 | jq { \"level\": \"info\", \"members\": { \"k0s-1\": \"https://192.168.64.32:2380\", \"k0s-2\": \"https://192.168.64.33:2380\", \"k0s-3\": \"https://192.168.64.56:2380\" }, \"msg\": \"done\", \"time\": \"2020-12-23T00:21:22+01:00\" } After a while, all worker nodes become Ready . Your cluster is now waiting to get used. We can test by creating a simple nginx deployment. $ kubectl create deployment nginx --image=gcr.io/google-containers/nginx --replicas=5 deployment.apps/nginx created $ kubectl expose deployment nginx --target-port=80 --port=8100 service/nginx exposed $ kubectl run hello-k0s --image=quay.io/prometheus/busybox --rm -it --restart=Never --command -- wget -qO- nginx:8100 <!DOCTYPE html> <html> <head> <title>Welcome to nginx on Debian!</title> ... pod \"hello-k0s\" deleted","title":"Create a cluster with Ansible"},{"location":"examples/ansible-playbook/#install-a-k0s-cluster-with-a-ansible-playbook","text":"Using Ansible and the k0s-ansible playbook, you can install a multi-node Kubernetes Cluster in a couple of minutes. Ansible is a popular infrastructure as code tool which helps you automate tasks to achieve the desired state in a system. This guide shows how you can install k0s on local virtual machines. In this guide, the following tools are used: multipass , a lightweight VM manager that uses KVM on Linux, Hyper-V on Windows, and hypervisor.framework on macOS ( installation guide ). ansible , a popular infrastructure as code tool ( installation guide ). and of course kubectl on your local machine ( Installation guide ). Before following this tutorial, you should have a general understanding of Ansible. A great way to start is the official Ansible User Guide . Please note: k0s users created k0s-ansible. Please send your feedback, bug reports, and pull requests to github.com/movd/k0s-ansible . Without further ado, let's jump right in.","title":"Install a k0s cluster with a Ansible Playbook"},{"location":"examples/ansible-playbook/#download-k0s-ansible","text":"On your local machine clone the k0s-ansible repository: $ git clone https://github.com/movd/k0s-ansible.git $ cd k0s-ansible","title":"Download k0s-ansible"},{"location":"examples/ansible-playbook/#create-virtual-machines","text":"For this tutorial, multipass was used. However, there is no interdependence. This playbook should also work with VMs created in alternative ways or Raspberry Pis. Next, create a couple of virtual machines. For the automation to work, each instance must have passwordless SSH access. To achieve this, we provision each instance with a cloud-init manifest that imports your current users' public SSH key and into a user k0s . For your convenience, a bash script is included that does just that: ./tools/multipass_create_instances.sh 7 \u25c0\ufe0f this creates 7 virtual machines $ ./tools/multipass_create_instances.sh 7 Create cloud-init to import ssh key... [1/7] Creating instance k0s-1 with multipass... Launched: k0s-1 [2/7] Creating instance k0s-2 with multipass... Launched: k0s-2 [3/7] Creating instance k0s-3 with multipass... Launched: k0s-3 [4/7] Creating instance k0s-4 with multipass... Launched: k0s-4 [5/7] Creating instance k0s-5 with multipass... Launched: k0s-5 [6/7] Creating instance k0s-6 with multipass... Launched: k0s-6 [7/7] Creating instance k0s-7 with multipass... Launched: k0s-7 Name State IPv4 Image k0s-1 Running 192.168.64.32 Ubuntu 20.04 LTS k0s-2 Running 192.168.64.33 Ubuntu 20.04 LTS k0s-3 Running 192.168.64.56 Ubuntu 20.04 LTS k0s-4 Running 192.168.64.57 Ubuntu 20.04 LTS k0s-5 Running 192.168.64.58 Ubuntu 20.04 LTS k0s-6 Running 192.168.64.60 Ubuntu 20.04 LTS k0s-7 Running 192.168.64.61 Ubuntu 20.04 LTS","title":"Create virtual machines"},{"location":"examples/ansible-playbook/#create-ansible-inventory","text":"After that, we create our inventory directory by copying the sample: $ cp -rfp inventory/sample inventory/multipass Now we need to create our inventory. The before built virtual machines need to be assigned to the different host groups required by the playbook's logic. initial_controller = must contain a single node that creates the worker and server tokens needed by the other nodes. controller = can contain nodes that, together with the host from initial_controller form a highly available isolated control plane. worker = must contain at least one node so that we can deploy Kubernetes objects. We could fill inventory/multipass/inventory.yml by hand with the metadata provided by multipass list, but since we are lazy and want to automate as much as possible, we can use the included Python script multipass_generate_inventory.py : To automatically fill our inventory run: $ ./tools/multipass_generate_inventory.py Designate first three instances as control plane Created Ansible Inventory at: /Users/dev/k0s-ansible/tools/inventory.yml $ cp tools/inventory.yml inventory/multipass/inventory.yml Now inventory/multipass/inventory.yml should look like this (Of course, your IP addresses might differ): --- all : children : initial_controller : hosts : k0s-1 : controller : hosts : k0s-2 : k0s-3 : worker : hosts : k0s-4 : k0s-5 : k0s-6 : k0s-7 : hosts : k0s-1 : ansible_host : 192.168.64.32 k0s-2 : ansible_host : 192.168.64.33 k0s-3 : ansible_host : 192.168.64.56 k0s-4 : ansible_host : 192.168.64.57 k0s-5 : ansible_host : 192.168.64.58 k0s-6 : ansible_host : 192.168.64.60 k0s-7 : ansible_host : 192.168.64.61 vars : ansible_user : k0s","title":"Create Ansible inventory"},{"location":"examples/ansible-playbook/#test-the-connection-to-the-virtual-machines","text":"To test the connection to your hosts just run: $ ansible -i inventory/multipass/inventory.yml -m ping k0s-4 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" } ... If all is green and successful, you can proceed.","title":"Test the connection to the virtual machines"},{"location":"examples/ansible-playbook/#provision-the-cluster-with-ansible","text":"Finally, we can start provisioning the cluster. Applying the playbook, k0s will get downloaded and set up on all nodes, tokens will get exchanged, and a kubeconfig will get dumped to your local deployment environment. $ ansible-playbook site.yml -i inventory/multipass/inventory.yml ... TASK [k0s/initial_controller : print kubeconfig command] ******************************************************* Tuesday 22 December 2020 17:43:20 +0100 (0:00:00.257) 0:00:41.287 ****** ok: [k0s-1] => { \"msg\": \"To use Cluster: export KUBECONFIG=/Users/dev/k0s-ansible/inventory/multipass/artifacts/k0s-kubeconfig.yml\" } ... PLAY RECAP ***************************************************************************************************** k0s-1 : ok=21 changed=11 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 k0s-2 : ok=10 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 k0s-3 : ok=10 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 k0s-4 : ok=9 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 k0s-5 : ok=9 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 k0s-6 : ok=9 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 k0s-7 : ok=9 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 Tuesday 22 December 2020 17:43:36 +0100 (0:00:01.204) 0:00:57.478 ****** =============================================================================== prereq : Install apt packages -------------------------------------------------------------------------- 22.70s k0s/controller : Wait for k8s apiserver ----------------------------------------------------------------- 4.30s k0s/initial_controller : Create worker join token ------------------------------------------------------- 3.38s k0s/initial_controller : Wait for k8s apiserver --------------------------------------------------------- 3.36s download : Download k0s binary k0s-v0.9.0-rc1-amd64 ----------------------------------------------------- 3.11s Gathering Facts ----------------------------------------------------------------------------------------- 2.85s Gathering Facts ----------------------------------------------------------------------------------------- 1.95s prereq : Create k0s Directories ------------------------------------------------------------------------- 1.53s k0s/worker : Enable and check k0s service --------------------------------------------------------------- 1.20s prereq : Write the k0s config file ---------------------------------------------------------------------- 1.09s k0s/initial_controller : Enable and check k0s service --------------------------------------------------- 0.94s k0s/controller : Enable and check k0s service ----------------------------------------------------------- 0.73s Gathering Facts ----------------------------------------------------------------------------------------- 0.71s Gathering Facts ----------------------------------------------------------------------------------------- 0.66s Gathering Facts ----------------------------------------------------------------------------------------- 0.64s k0s/worker : Write the k0s token file on worker --------------------------------------------------------- 0.64s k0s/worker : Copy k0s service file ---------------------------------------------------------------------- 0.53s k0s/controller : Write the k0s token file on controller ------------------------------------------------- 0.41s k0s/controller : Copy k0s service file ------------------------------------------------------------------ 0.40s k0s/initial_controller : Copy k0s service file ---------------------------------------------------------- 0.36s","title":"Provision the cluster with Ansible"},{"location":"examples/ansible-playbook/#use-the-cluster-with-kubectl","text":"While the playbook ran, a kubeconfig got copied to your local machine. You can use it to get simple access to your new Kubernetes cluster: $ export KUBECONFIG=/Users/dev/k0s-ansible/inventory/multipass/artifacts/k0s-kubeconfig.yml $ kubectl cluster-info Kubernetes control plane is running at https://192.168.64.32:6443 CoreDNS is running at https://192.168.64.32:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://192.168.64.32:6443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k0s-4 Ready <none> 21s v1.20.1-k0s1 192.168.64.57 <none> Ubuntu 20.04.1 LTS 5.4.0-54-generic containerd://1.4.3 k0s-5 Ready <none> 21s v1.20.1-k0s1 192.168.64.58 <none> Ubuntu 20.04.1 LTS 5.4.0-54-generic containerd://1.4.3 k0s-6 NotReady <none> 21s v1.20.1-k0s1 192.168.64.60 <none> Ubuntu 20.04.1 LTS 5.4.0-54-generic containerd://1.4.3 k0s-7 NotReady <none> 21s v1.20.1-k0s1 192.168.64.61 <none> Ubuntu 20.04.1 LTS 5.4.0-54-generic containerd://1.4.3 \u2b06\ufe0f Of course, the first three control plane nodes won't show up here because the control plane is fully isolated. You can check on the distributed etcd cluster by running this ad-hoc command (or ssh'ing directly into a controller node): $ ansible k0s-1 -a \"k0s etcd member-list -c /etc/k0s/k0s.yaml\" -i inventory/multipass/inventory.yml | tail -1 | jq { \"level\": \"info\", \"members\": { \"k0s-1\": \"https://192.168.64.32:2380\", \"k0s-2\": \"https://192.168.64.33:2380\", \"k0s-3\": \"https://192.168.64.56:2380\" }, \"msg\": \"done\", \"time\": \"2020-12-23T00:21:22+01:00\" } After a while, all worker nodes become Ready . Your cluster is now waiting to get used. We can test by creating a simple nginx deployment. $ kubectl create deployment nginx --image=gcr.io/google-containers/nginx --replicas=5 deployment.apps/nginx created $ kubectl expose deployment nginx --target-port=80 --port=8100 service/nginx exposed $ kubectl run hello-k0s --image=quay.io/prometheus/busybox --rm -it --restart=Never --command -- wget -qO- nginx:8100 <!DOCTYPE html> <html> <head> <title>Welcome to nginx on Debian!</title> ... pod \"hello-k0s\" deleted","title":"Use the cluster with kubectl"},{"location":"examples/traefik-ingress/","text":"Installing the Traefik Ingress Controller on k0s # In this tutorial, you'll learn how to configure k0s with the Traefik ingress controller , a MetalLB service loadbalancer , and deploy the Traefik Dashboard along with a service example. Utilizing the extensible bootstrapping functionality with Helm, it's as simple as adding the right extensions to the k0s.yaml file when configuring your cluster. Configuring k0s.yaml # Modify your k0s.yaml file to include the Traefik and MetalLB helm charts as extensions, and these will install during the cluster's bootstrap. Note: You may want to have a small range of IP addresses that are addressable on your network, preferably outside the assignment pool allocated by your DHCP server. Providing an addressable range should allow you to access your LoadBalancer and Ingress services from anywhere on your local network. However, any valid IP range should work locally on your machine. extensions : helm : repositories : - name : traefik url : https://helm.traefik.io/traefik - name : bitnami url : https://charts.bitnami.com/bitnami charts : - name : traefik chartname : traefik/traefik version : \"9.11.0\" namespace : default - name : metallb chartname : bitnami/metallb version : \"1.0.1\" namespace : default values : |2 configInline: address-pools: - name: generic-cluster-pool protocol: layer2 addresses: - 192.168.0.5-192.168.0.10 Providing a range of IPs for MetalLB that are addressable on your LAN is suggested if you want to access LoadBalancer and Ingress services from anywhere on your local network. Retrieving the Load Balancer IP # Once you've started your cluster, you should confirm the deployment of Traefik and MetalLB. Executing a kubectl get all should include a response with the metallb and traefik resources, along with a service loadbalancer that has an EXTERNAL-IP assigned to it. See the example below: root@k0s-host \u279c kubectl get all NAME READY STATUS RESTARTS AGE pod/metallb-1607085578-controller-864c9757f6-bpx6r 1 /1 Running 0 81s pod/metallb-1607085578-speaker-245c2 1 /1 Running 0 60s pod/traefik-1607085579-77bbc57699-b2f2t 1 /1 Running 0 81s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 96s service/traefik-1607085579 LoadBalancer 10 .105.119.102 192 .168.0.5 80 :32153/TCP,443:30791/TCP 84s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/metallb-1607085578-speaker 1 1 1 1 1 kubernetes.io/os = linux 87s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/metallb-1607085578-controller 1 /1 1 1 87s deployment.apps/traefik-1607085579 1 /1 1 1 84s NAME DESIRED CURRENT READY AGE replicaset.apps/metallb-1607085578-controller-864c9757f6 1 1 1 81s replicaset.apps/traefik-1607085579-77bbc57699 1 1 1 81s Take note of the EXTERNAL-IP given to the service/traefik-n LoadBalancer. In this example, 192.168.0.5 has been assigned and can be used to access services via the Ingress proxy: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/traefik-1607085579 LoadBalancer 10 .105.119.102 192 .168.0.5 80 :32153/TCP,443:30791/TCP 84s # Receiving a 404 response here is normal, as you've not configured any Ingress resources to respond yet root@k0s-host \u279c curl http://192.168.0.5 404 page not found Deploy and access the Traefik Dashboard # Now that you have an available and addressable load balancer on your cluster, you can quickly deploy the Traefik dashboard and access it from anywhere on your local network (provided that you configured MetalLB with an addressable range). Create the Traefik Dashboard IngressRoute in a YAML file: apiVersion : traefik.containo.us/v1alpha1 kind : IngressRoute metadata : name : dashboard spec : entryPoints : - web routes : - match : PathPrefix(`/dashboard`) || PathPrefix(`/api`) kind : Rule services : - name : api@internal kind : TraefikService Next, deploy the resource: root@k0s-host \u279c kubectl apply -f traefik-dashboard.yaml ingressroute.traefik.containo.us/dashboard created Once deployed, you should be able to access the dashboard using the EXTERNAL-IP that you noted above by visiting http://192.168.0.5 in your browser: Now, create a simple whoami Deployment, Service, and Ingress manifest: apiVersion : apps/v1 kind : Deployment metadata : name : whoami-deployment spec : replicas : 1 selector : matchLabels : app : whoami template : metadata : labels : app : whoami spec : containers : - name : whoami-container image : containous/whoami --- apiVersion : v1 kind : Service metadata : name : whoami-service spec : ports : - name : http targetPort : 80 port : 80 selector : app : whoami --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : whoami-ingress spec : rules : - http : paths : - path : /whoami pathType : Exact backend : service : name : whoami-service port : number : 80 Once you've created this, apply and test it: # apply the manifests root@k0s-host \u279c kubectl apply -f whoami.yaml deployment.apps/whoami-deployment created service/whoami-service created ingress.networking.k8s.io/whoami-ingress created # test the ingress and service root@k0s-host \u279c curl http://192.168.0.5/whoami Hostname: whoami-deployment-85bfbd48f-7l77c IP: 127 .0.0.1 IP: ::1 IP: 10 .244.214.198 IP: fe80::b049:f8ff:fe77:3e64 RemoteAddr: 10 .244.214.196:34858 GET /whoami HTTP/1.1 Host: 192 .168.0.5 User-Agent: curl/7.68.0 Accept: */* Accept-Encoding: gzip X-Forwarded-For: 192 .168.0.82 X-Forwarded-Host: 192 .168.0.5 X-Forwarded-Port: 80 X-Forwarded-Proto: http X-Forwarded-Server: traefik-1607085579-77bbc57699-b2f2t X-Real-Ip: 192 .168.0.82 Summary # From here, it's possible to use 3rd party tools, such as ngrok , to go further and expose your LoadBalancer to the world. Doing so then enables dynamic certificate provisioning through Let's Encrypt utilizing either cert-manager or Traefik's own built-in ACME provider . This guide should have given you a general idea of getting started with Ingress on k0s and exposing your applications and services quickly.","title":"Running k0s with Traefik"},{"location":"examples/traefik-ingress/#installing-the-traefik-ingress-controller-on-k0s","text":"In this tutorial, you'll learn how to configure k0s with the Traefik ingress controller , a MetalLB service loadbalancer , and deploy the Traefik Dashboard along with a service example. Utilizing the extensible bootstrapping functionality with Helm, it's as simple as adding the right extensions to the k0s.yaml file when configuring your cluster.","title":"Installing the Traefik Ingress Controller on k0s"},{"location":"examples/traefik-ingress/#configuring-k0syaml","text":"Modify your k0s.yaml file to include the Traefik and MetalLB helm charts as extensions, and these will install during the cluster's bootstrap. Note: You may want to have a small range of IP addresses that are addressable on your network, preferably outside the assignment pool allocated by your DHCP server. Providing an addressable range should allow you to access your LoadBalancer and Ingress services from anywhere on your local network. However, any valid IP range should work locally on your machine. extensions : helm : repositories : - name : traefik url : https://helm.traefik.io/traefik - name : bitnami url : https://charts.bitnami.com/bitnami charts : - name : traefik chartname : traefik/traefik version : \"9.11.0\" namespace : default - name : metallb chartname : bitnami/metallb version : \"1.0.1\" namespace : default values : |2 configInline: address-pools: - name: generic-cluster-pool protocol: layer2 addresses: - 192.168.0.5-192.168.0.10 Providing a range of IPs for MetalLB that are addressable on your LAN is suggested if you want to access LoadBalancer and Ingress services from anywhere on your local network.","title":"Configuring k0s.yaml"},{"location":"examples/traefik-ingress/#retrieving-the-load-balancer-ip","text":"Once you've started your cluster, you should confirm the deployment of Traefik and MetalLB. Executing a kubectl get all should include a response with the metallb and traefik resources, along with a service loadbalancer that has an EXTERNAL-IP assigned to it. See the example below: root@k0s-host \u279c kubectl get all NAME READY STATUS RESTARTS AGE pod/metallb-1607085578-controller-864c9757f6-bpx6r 1 /1 Running 0 81s pod/metallb-1607085578-speaker-245c2 1 /1 Running 0 60s pod/traefik-1607085579-77bbc57699-b2f2t 1 /1 Running 0 81s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 96s service/traefik-1607085579 LoadBalancer 10 .105.119.102 192 .168.0.5 80 :32153/TCP,443:30791/TCP 84s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/metallb-1607085578-speaker 1 1 1 1 1 kubernetes.io/os = linux 87s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/metallb-1607085578-controller 1 /1 1 1 87s deployment.apps/traefik-1607085579 1 /1 1 1 84s NAME DESIRED CURRENT READY AGE replicaset.apps/metallb-1607085578-controller-864c9757f6 1 1 1 81s replicaset.apps/traefik-1607085579-77bbc57699 1 1 1 81s Take note of the EXTERNAL-IP given to the service/traefik-n LoadBalancer. In this example, 192.168.0.5 has been assigned and can be used to access services via the Ingress proxy: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/traefik-1607085579 LoadBalancer 10 .105.119.102 192 .168.0.5 80 :32153/TCP,443:30791/TCP 84s # Receiving a 404 response here is normal, as you've not configured any Ingress resources to respond yet root@k0s-host \u279c curl http://192.168.0.5 404 page not found","title":"Retrieving the Load Balancer IP"},{"location":"examples/traefik-ingress/#deploy-and-access-the-traefik-dashboard","text":"Now that you have an available and addressable load balancer on your cluster, you can quickly deploy the Traefik dashboard and access it from anywhere on your local network (provided that you configured MetalLB with an addressable range). Create the Traefik Dashboard IngressRoute in a YAML file: apiVersion : traefik.containo.us/v1alpha1 kind : IngressRoute metadata : name : dashboard spec : entryPoints : - web routes : - match : PathPrefix(`/dashboard`) || PathPrefix(`/api`) kind : Rule services : - name : api@internal kind : TraefikService Next, deploy the resource: root@k0s-host \u279c kubectl apply -f traefik-dashboard.yaml ingressroute.traefik.containo.us/dashboard created Once deployed, you should be able to access the dashboard using the EXTERNAL-IP that you noted above by visiting http://192.168.0.5 in your browser: Now, create a simple whoami Deployment, Service, and Ingress manifest: apiVersion : apps/v1 kind : Deployment metadata : name : whoami-deployment spec : replicas : 1 selector : matchLabels : app : whoami template : metadata : labels : app : whoami spec : containers : - name : whoami-container image : containous/whoami --- apiVersion : v1 kind : Service metadata : name : whoami-service spec : ports : - name : http targetPort : 80 port : 80 selector : app : whoami --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : whoami-ingress spec : rules : - http : paths : - path : /whoami pathType : Exact backend : service : name : whoami-service port : number : 80 Once you've created this, apply and test it: # apply the manifests root@k0s-host \u279c kubectl apply -f whoami.yaml deployment.apps/whoami-deployment created service/whoami-service created ingress.networking.k8s.io/whoami-ingress created # test the ingress and service root@k0s-host \u279c curl http://192.168.0.5/whoami Hostname: whoami-deployment-85bfbd48f-7l77c IP: 127 .0.0.1 IP: ::1 IP: 10 .244.214.198 IP: fe80::b049:f8ff:fe77:3e64 RemoteAddr: 10 .244.214.196:34858 GET /whoami HTTP/1.1 Host: 192 .168.0.5 User-Agent: curl/7.68.0 Accept: */* Accept-Encoding: gzip X-Forwarded-For: 192 .168.0.82 X-Forwarded-Host: 192 .168.0.5 X-Forwarded-Port: 80 X-Forwarded-Proto: http X-Forwarded-Server: traefik-1607085579-77bbc57699-b2f2t X-Real-Ip: 192 .168.0.82","title":"Deploy and access the Traefik Dashboard"},{"location":"examples/traefik-ingress/#summary","text":"From here, it's possible to use 3rd party tools, such as ngrok , to go further and expose your LoadBalancer to the world. Doing so then enables dynamic certificate provisioning through Let's Encrypt utilizing either cert-manager or Traefik's own built-in ACME provider . This guide should have given you a general idea of getting started with Ingress on k0s and exposing your applications and services quickly.","title":"Summary"},{"location":"internal/host-dependencies/","text":"Host Dependencies # The goal of k0s is to only depend on kernel functionality. However, the kubelet currently has several host dependencies. Some of these may need to be bundled, but we should prefer upstream contributions to remove these dependencies. List of hard dependencies # find -- PR by @ncopa to resolve this: https://github.com/kubernetes/kubernetes/pull/95189 du -- PR by @ncopa to resolve this: https://github.com/kubernetes/kubernetes/pull/95178 -- note that du dependency remains, but using POSIX-compliant argument nice iptables -- as documented in https://github.com/k0sproject/k0s/issues/176 it is unclear whether iptables is needed. It appears to come from the portmap plugin, but the most robust solution may be to simply bundle iptables with k0s.","title":"Host Dependencies"},{"location":"internal/host-dependencies/#host-dependencies","text":"The goal of k0s is to only depend on kernel functionality. However, the kubelet currently has several host dependencies. Some of these may need to be bundled, but we should prefer upstream contributions to remove these dependencies.","title":"Host Dependencies"},{"location":"internal/host-dependencies/#list-of-hard-dependencies","text":"find -- PR by @ncopa to resolve this: https://github.com/kubernetes/kubernetes/pull/95189 du -- PR by @ncopa to resolve this: https://github.com/kubernetes/kubernetes/pull/95178 -- note that du dependency remains, but using POSIX-compliant argument nice iptables -- as documented in https://github.com/k0sproject/k0s/issues/176 it is unclear whether iptables is needed. It appears to come from the portmap plugin, but the most robust solution may be to simply bundle iptables with k0s.","title":"List of hard dependencies"},{"location":"internal/publishing_docs_using_mkdocs/","text":"Publishing Docs # We use mkdocs and mike for publishing docs to docs.k0sproject.io . This guide will provide a simple how-to on how to configure and deploy newly added docs to our website. Requirements # Install mike: https://github.com/jimporter/mike#installation Adding A New link to the Navigation # All docs must live under the docs directory (I.E., changes to the main README.md are not reflected in the website). Add a new link under nav in the main mkdocs.yml file: ``` nav: Overview: README.md Creating A Cluster: Quick Start Guide: create-cluster.md Run in Docker: k0s-in-docker.md Single node set-up: k0s-single-node.md Configuration Reference: Architecture: architecture.md Networking: networking.md Configuration Options: configuration.md Configuring Containerd: containerd_config.md Using A Custom CRI: custom-cri-runtime.md Using Cloud Providers: cloud-providers.md Running k0s with Traefik: examples/traefik-ingress.md Running k0s as a service: install.md k0s CLI Help Pages: cli/k0s.md Deploying Manifests: manifests.md FAQ: FAQ.md Troubleshooting: troubleshooting.md Contributing: Overview: contributors/overview.md Workflow: contributors/github_workflow.md Testing: contributors/testing.md ``` Test your deployment locally, using mike : version=\"v0.9.0\" # example mike deploy ${version} mike set-default ${version} mike serve Your local version should be served under: http://localhost:8000. Once your changes are pushed to main , the \"Publish Docs\" jos will start running: https://github.com/k0sproject/k0s/actions?query=workflow%3A%22Publish+docs+via+GitHub+Pages%22 You should see the deployment outcome in the gh-pages deployment page: https://github.com/k0sproject/k0s/deployments/activity_log?environment=github-pages","title":"Publishing Docs"},{"location":"internal/publishing_docs_using_mkdocs/#publishing-docs","text":"We use mkdocs and mike for publishing docs to docs.k0sproject.io . This guide will provide a simple how-to on how to configure and deploy newly added docs to our website.","title":"Publishing Docs"},{"location":"internal/publishing_docs_using_mkdocs/#requirements","text":"Install mike: https://github.com/jimporter/mike#installation","title":"Requirements"},{"location":"internal/publishing_docs_using_mkdocs/#adding-a-new-link-to-the-navigation","text":"All docs must live under the docs directory (I.E., changes to the main README.md are not reflected in the website). Add a new link under nav in the main mkdocs.yml file: ``` nav: Overview: README.md Creating A Cluster: Quick Start Guide: create-cluster.md Run in Docker: k0s-in-docker.md Single node set-up: k0s-single-node.md Configuration Reference: Architecture: architecture.md Networking: networking.md Configuration Options: configuration.md Configuring Containerd: containerd_config.md Using A Custom CRI: custom-cri-runtime.md Using Cloud Providers: cloud-providers.md Running k0s with Traefik: examples/traefik-ingress.md Running k0s as a service: install.md k0s CLI Help Pages: cli/k0s.md Deploying Manifests: manifests.md FAQ: FAQ.md Troubleshooting: troubleshooting.md Contributing: Overview: contributors/overview.md Workflow: contributors/github_workflow.md Testing: contributors/testing.md ``` Test your deployment locally, using mike : version=\"v0.9.0\" # example mike deploy ${version} mike set-default ${version} mike serve Your local version should be served under: http://localhost:8000. Once your changes are pushed to main , the \"Publish Docs\" jos will start running: https://github.com/k0sproject/k0s/actions?query=workflow%3A%22Publish+docs+via+GitHub+Pages%22 You should see the deployment outcome in the gh-pages deployment page: https://github.com/k0sproject/k0s/deployments/activity_log?environment=github-pages","title":"Adding A New link to the Navigation"},{"location":"internal/upgrading-calico/","text":"Upgrading Calico # k0s bundles Kubernetes manifests for Calico. The manifests are retrieved from the official Calico docs . As fetching and modifying the entire multi-thousand line file is error-prone, you may follow these steps to upgrade Calico to the latest version: run ./get-calico.sh check the git diff to see if it looks sensible re-apply our manual adjustments (documented below) run make bindata-manifests compile, pray, and test commit and create a PR Manual Adjustments # Note: All manual adjustments should be fairly obvious from the git diff. This section attempts to provide a sanity checklist to go through and make sure we still have those changes applied. The code blocks in this section are our modifications , not the calico originals. static/manifests/calico/DaemonSet/calico-node.yaml : variable-based support for both vxlan and ipip (search for ipip to find): {{- if eq .Mode \"ipip\" }} # Enable IPIP - name: CALICO_IPV4POOL_IPIP value: \"Always\" # Enable or Disable VXLAN on the default IP pool. - name: CALICO_IPV4POOL_VXLAN value: \"Never\" {{- else if eq .Mode \"vxlan\" }} # Disable IPIP - name: CALICO_IPV4POOL_IPIP value: \"Never\" # Enable VXLAN on the default IP pool. - name: CALICO_IPV4POOL_VXLAN value: \"Always\" - name: FELIX_VXLANPORT value: \"{{ .VxlanPort }}\" - name: FELIX_VXLANVNI value: \"{{ .VxlanVNI }}\" {{- end }} iptables auto detect: # Auto detect the iptables backend - name: FELIX_IPTABLESBACKEND value: \"auto\" variable-based WireGuard support: {{- if .EnableWireguard }} - name: FELIX_WIREGUARDENABLED value: \"true\" {{- end }} variable-based cluster CIDR: - name: CALICO_IPV4POOL_CIDR value: \"{{ .ClusterCIDR }}\" custom backend and MTU # calico-config.yaml calico_backend: \"{{ .Mode }}\" veth_mtu: \"{{ .MTU }}\" remove bgp from CLUSTER_TYPE - name: CLUSTER_TYPE value: \"k8s\" disable BIRD checks on liveness and readiness as we don't support BGP by removing -bird-ready and -bird-live from the readiness and liveness probes respectively Container image names # Instead of hardcoded image names and versions use placeholders to support configuration level settings. Following placeholders are used: CalicoCNIImage for calico/cni CalicoFlexVolumeImage for calico/pod2daemon-flexvol CalicoNodeImage for calico/node CalicoKubeControllersImage for calico/kube-controllers Example: # calico-node.yaml image: {{ .CalicoCNIImage }}","title":"Upgrading Calico"},{"location":"internal/upgrading-calico/#upgrading-calico","text":"k0s bundles Kubernetes manifests for Calico. The manifests are retrieved from the official Calico docs . As fetching and modifying the entire multi-thousand line file is error-prone, you may follow these steps to upgrade Calico to the latest version: run ./get-calico.sh check the git diff to see if it looks sensible re-apply our manual adjustments (documented below) run make bindata-manifests compile, pray, and test commit and create a PR","title":"Upgrading Calico"},{"location":"internal/upgrading-calico/#manual-adjustments","text":"Note: All manual adjustments should be fairly obvious from the git diff. This section attempts to provide a sanity checklist to go through and make sure we still have those changes applied. The code blocks in this section are our modifications , not the calico originals. static/manifests/calico/DaemonSet/calico-node.yaml : variable-based support for both vxlan and ipip (search for ipip to find): {{- if eq .Mode \"ipip\" }} # Enable IPIP - name: CALICO_IPV4POOL_IPIP value: \"Always\" # Enable or Disable VXLAN on the default IP pool. - name: CALICO_IPV4POOL_VXLAN value: \"Never\" {{- else if eq .Mode \"vxlan\" }} # Disable IPIP - name: CALICO_IPV4POOL_IPIP value: \"Never\" # Enable VXLAN on the default IP pool. - name: CALICO_IPV4POOL_VXLAN value: \"Always\" - name: FELIX_VXLANPORT value: \"{{ .VxlanPort }}\" - name: FELIX_VXLANVNI value: \"{{ .VxlanVNI }}\" {{- end }} iptables auto detect: # Auto detect the iptables backend - name: FELIX_IPTABLESBACKEND value: \"auto\" variable-based WireGuard support: {{- if .EnableWireguard }} - name: FELIX_WIREGUARDENABLED value: \"true\" {{- end }} variable-based cluster CIDR: - name: CALICO_IPV4POOL_CIDR value: \"{{ .ClusterCIDR }}\" custom backend and MTU # calico-config.yaml calico_backend: \"{{ .Mode }}\" veth_mtu: \"{{ .MTU }}\" remove bgp from CLUSTER_TYPE - name: CLUSTER_TYPE value: \"k8s\" disable BIRD checks on liveness and readiness as we don't support BGP by removing -bird-ready and -bird-live from the readiness and liveness probes respectively","title":"Manual Adjustments"},{"location":"internal/upgrading-calico/#container-image-names","text":"Instead of hardcoded image names and versions use placeholders to support configuration level settings. Following placeholders are used: CalicoCNIImage for calico/cni CalicoFlexVolumeImage for calico/pod2daemon-flexvol CalicoNodeImage for calico/node CalicoKubeControllersImage for calico/kube-controllers Example: # calico-node.yaml image: {{ .CalicoCNIImage }}","title":"Container image names"}]}