{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview # k0s is an all-inclusive Kubernetes distribution with all the required bells and whistles preconfigured to make building a Kubernetes clusters a matter of just copying an executable to every host and running it. Key Features # Packaged as a single static binary Self-hosted, isolated control plane Variety of storage backends: etcd, SQLite, MySQL (or any compatible), PostgreSQL Elastic control-plane Vanilla upstream Kubernetes Supports custom container runtimes (containerd is the default) Supports custom Container Network Interface (CNI) plugins (calico is the default) Supports x86_64 and arm64 Join the Community # If you'd like to help build k0s, please check out our guide to Contributing and our Code of Conduct . Demo # Downloading k0s # Download k0s for linux amd64 and arm64 architectures. Quick start # Creating A k0s Cluster","title":"Overview"},{"location":"#overview","text":"k0s is an all-inclusive Kubernetes distribution with all the required bells and whistles preconfigured to make building a Kubernetes clusters a matter of just copying an executable to every host and running it.","title":"Overview"},{"location":"#key-features","text":"Packaged as a single static binary Self-hosted, isolated control plane Variety of storage backends: etcd, SQLite, MySQL (or any compatible), PostgreSQL Elastic control-plane Vanilla upstream Kubernetes Supports custom container runtimes (containerd is the default) Supports custom Container Network Interface (CNI) plugins (calico is the default) Supports x86_64 and arm64","title":"Key Features"},{"location":"#join-the-community","text":"If you'd like to help build k0s, please check out our guide to Contributing and our Code of Conduct .","title":"Join the Community"},{"location":"#demo","text":"","title":"Demo"},{"location":"#downloading-k0s","text":"Download k0s for linux amd64 and arm64 architectures.","title":"Downloading k0s"},{"location":"#quick-start","text":"Creating A k0s Cluster","title":"Quick start"},{"location":"CODE_OF_CONDUCT/","text":"K0s Community Code Of Conduct # Please refer to our contributor code of conduct .","title":"K0s Community Code Of Conduct"},{"location":"CODE_OF_CONDUCT/#k0s-community-code-of-conduct","text":"Please refer to our contributor code of conduct .","title":"K0s Community Code Of Conduct"},{"location":"FAQ/","text":"Frequently asked questions # How is k0s pronounced? # kay-zero-ess How do I run a single node cluster? # k0s server --enable-worker How do I connect to the cluster? # You find the config in ${DATADIR}/pki/admin.conf (default: /var/lib/k0s/pki/admin.conf ). Copy this file, and change the localhost entry to the public ip of the controller. Use the modified config to connect with kubectl: export KUBECONFIG=/path/to/admin.conf kubectl ... Why doesn't kubectl get nodes list the k0s server? # As a default, the control plane does not run kubelet at all, and will not accept any workloads, so the server will not show up on the node list in kubectl. If you want your server to accept workloads and run pods, you do so with: k0s server --enable-worker (recommended only as test/dev/POC environments).","title":"FAQ"},{"location":"FAQ/#frequently-asked-questions","text":"","title":"Frequently asked questions"},{"location":"FAQ/#how-is-k0s-pronounced","text":"kay-zero-ess","title":"How is k0s pronounced?"},{"location":"FAQ/#how-do-i-run-a-single-node-cluster","text":"k0s server --enable-worker","title":"How do I run a single node cluster?"},{"location":"FAQ/#how-do-i-connect-to-the-cluster","text":"You find the config in ${DATADIR}/pki/admin.conf (default: /var/lib/k0s/pki/admin.conf ). Copy this file, and change the localhost entry to the public ip of the controller. Use the modified config to connect with kubectl: export KUBECONFIG=/path/to/admin.conf kubectl ...","title":"How do I connect to the cluster?"},{"location":"FAQ/#why-doesnt-kubectl-get-nodes-list-the-k0s-server","text":"As a default, the control plane does not run kubelet at all, and will not accept any workloads, so the server will not show up on the node list in kubectl. If you want your server to accept workloads and run pods, you do so with: k0s server --enable-worker (recommended only as test/dev/POC environments).","title":"Why doesn't kubectl get nodes list the k0s server?"},{"location":"architecture/","text":"Architecture # Note: As with any young project, things change rapidly. Thus all the details in this architecture documentation may not be always up-to-date, but the high level concepts and patterns should still apply. Packaging # k0s is packaged as single, self-extracting binary which embeds Kubernetes binaries. This has many benefits: - Everything can be, and is, statically compiled - No OS level deps - No RPMs, dep's, snaps or any other OS specific packaging needed. Single \"package\" for all OSes - We can fully control the versions of each and every dependency Control plane # k0s as a single binary acts as the process supervisor for all other control plane components. This means there's no container engine or kubelet running on controllers (by default). Which means there is no way for a cluster user to schedule workloads onto controller nodes. k0s creates, manages and configures each of the components. k0s runs all control plane components as \"naked\" processes. So on the controller node there's no container engine running. Storage # Typically Kubernetes control plane supports only etcd as the datastore. In addition to etcd, k0s supports many other datastore options. This is achieved by including kine . Kine allows wide variety of backend data stores to be used such as MySQL, PostgreSQL, SQLite and dqlite. See more in storage documentation In case of k0s managed etcd, k0s manages the full lifecycle of the etcd cluster. This means for example that by joining a new controller node with k0s server \"long-join-token\" k0s will automatically adjust the etcd cluster membership info to allow the new member to join the cluster. Note: Currently k0s cannot shrink the etcd cluster. For now user needs to manually remove the etcd member and only after that shutdown the k0s controller on the removed node. Worker node # Like for the control plane, k0s creates and manages the core worker components as naked processes on the worker node. By default, k0s workers use containerd as a high-level runtime and runc as a low-level runtime. Custom runtimes are also supported as described here .","title":"Architecture"},{"location":"architecture/#architecture","text":"Note: As with any young project, things change rapidly. Thus all the details in this architecture documentation may not be always up-to-date, but the high level concepts and patterns should still apply.","title":"Architecture"},{"location":"architecture/#packaging","text":"k0s is packaged as single, self-extracting binary which embeds Kubernetes binaries. This has many benefits: - Everything can be, and is, statically compiled - No OS level deps - No RPMs, dep's, snaps or any other OS specific packaging needed. Single \"package\" for all OSes - We can fully control the versions of each and every dependency","title":"Packaging"},{"location":"architecture/#control-plane","text":"k0s as a single binary acts as the process supervisor for all other control plane components. This means there's no container engine or kubelet running on controllers (by default). Which means there is no way for a cluster user to schedule workloads onto controller nodes. k0s creates, manages and configures each of the components. k0s runs all control plane components as \"naked\" processes. So on the controller node there's no container engine running.","title":"Control plane"},{"location":"architecture/#storage","text":"Typically Kubernetes control plane supports only etcd as the datastore. In addition to etcd, k0s supports many other datastore options. This is achieved by including kine . Kine allows wide variety of backend data stores to be used such as MySQL, PostgreSQL, SQLite and dqlite. See more in storage documentation In case of k0s managed etcd, k0s manages the full lifecycle of the etcd cluster. This means for example that by joining a new controller node with k0s server \"long-join-token\" k0s will automatically adjust the etcd cluster membership info to allow the new member to join the cluster. Note: Currently k0s cannot shrink the etcd cluster. For now user needs to manually remove the etcd member and only after that shutdown the k0s controller on the removed node.","title":"Storage"},{"location":"architecture/#worker-node","text":"Like for the control plane, k0s creates and manages the core worker components as naked processes on the worker node. By default, k0s workers use containerd as a high-level runtime and runc as a low-level runtime. Custom runtimes are also supported as described here .","title":"Worker node"},{"location":"cloud-providers/","text":"Using cloud providers # k0s builds Kubernetes components in \"providerless\" mode. This means that there is no cloud providers built into k0s managed Kubernetes components. This means the cloud providers have to be configured \"externally\". The following steps outline how to enable cloud providers support in your k0s cluster. For more information on running Kubernetes with cloud providers see the official documentation . Enabling cloud provider support in kubelet # Even when all components are built with \"providerless\" mode, we need to be able to enable cloud provider \"mode\" for kubelet. This is done by running the workers with --enable-cloud-provider=true . This enables --cloud-provider=external on kubelet process. Deploying the actual cloud provider # From Kubernetes point of view, it does not really matter how and where the cloud providers controller(s) are running. Of course the easiest way is to deploy them on the cluster itself. To deploy your cloud provider as k0s managed stack you can use the built-in manifest deployer . Simply drop all the needed manifests under e.g. /var/lib/k0s/manifests/aws/ directory and k0s will deploy everything. Some cloud providers do need some configuration files to be present on all the nodes or some other pre-requisites. Consult your cloud providers documentation for needed steps.","title":"Using Cloud Providers"},{"location":"cloud-providers/#using-cloud-providers","text":"k0s builds Kubernetes components in \"providerless\" mode. This means that there is no cloud providers built into k0s managed Kubernetes components. This means the cloud providers have to be configured \"externally\". The following steps outline how to enable cloud providers support in your k0s cluster. For more information on running Kubernetes with cloud providers see the official documentation .","title":"Using cloud providers"},{"location":"cloud-providers/#enabling-cloud-provider-support-in-kubelet","text":"Even when all components are built with \"providerless\" mode, we need to be able to enable cloud provider \"mode\" for kubelet. This is done by running the workers with --enable-cloud-provider=true . This enables --cloud-provider=external on kubelet process.","title":"Enabling cloud provider support in kubelet"},{"location":"cloud-providers/#deploying-the-actual-cloud-provider","text":"From Kubernetes point of view, it does not really matter how and where the cloud providers controller(s) are running. Of course the easiest way is to deploy them on the cluster itself. To deploy your cloud provider as k0s managed stack you can use the built-in manifest deployer . Simply drop all the needed manifests under e.g. /var/lib/k0s/manifests/aws/ directory and k0s will deploy everything. Some cloud providers do need some configuration files to be present on all the nodes or some other pre-requisites. Consult your cloud providers documentation for needed steps.","title":"Deploying the actual cloud provider"},{"location":"configuration/","text":"Configuration options # Control plane # k0s Control plane can be configured via a YAML config file. By default k0s server command reads a file called k0s.yaml but can be told to read any yaml file via --config option. An example config file with the most common options users should configure: apiVersion : k0s.k0sproject.io/v1beta1 kind : Cluster metadata : name : k0s spec : api : address : 192.168.68.106 sans : - my-k0s-control.my-domain.com network : podCIDR : 10.244.0.0/16 serviceCIDR : 10.96.0.0/12 extensions : helm : repositories : - name : prometheus-community url : https://prometheus-community.github.io/helm-charts charts : - name : prometheus-stack chartname : prometheus-community/prometheus version : \"11.16.8\" namespace : default spec.api # address : The local address to bind API on. Also used as one of the addresses pushed on the k0s create service certificate on the API. Defaults to first non-local address found on the node. sans : List of additional addresses to push to API servers serving certificate spec.network # podCIDR : Pod network CIDR to be used in the cluster serviceCIDR : Network CIDR to be used for cluster VIP services. extensions.helm # List of Helm repositories and charts to deploy during cluster bootstrap. This example configures Prometheus from \"stable\" Helms chart repository. Configuring an HA Control Plane # The following pre-requisites are required in order to configure an HA control plane: Requirements # Load Balancer # A load balancer with a single external address should be configured as the IP gateway for the controllers. The load balancer should allow traffic to each controller on the following ports: 6443 8132 8133 9443 Cluster configuration # On each controller node, a k0s.yaml configuration file should be configured. The following options need to match on each node, otherwise the control plane components will end up in very unknown states: network storage : Needless to say, one cannot create a clustered controlplane with each node only storing data locally on SQLite. externalAddress Full config reference # Note: Many of the options configure things deep down in the \"stack\" on various components. So please make sure you understand what is being configured and whether or not it works in your specific environment. A full config file with defaults generated by the k0s default-config command: apiVersion : k0s.k0sproject.io/v1beta1 kind : Cluster metadata : name : k0s spec : api : externalAddress : my-lb-address.example.com address : 192.168.68.106 sans : - 192.168.68.106 extraArgs : {} controllerManager : extraArgs : {} scheduler : extraArgs : {} storage : type : etcd etcd : peerAddress : 192.168.68.106 network : podCIDR : 10.244.0.0/16 serviceCIDR : 10.96.0.0/12 provider : calico calico : mode : vxlan vxlanPort : 4789 vxlanVNI : 4096 mtu : 1450 wireguard : false flexVolumeDriverPath : /usr/libexec/k0s/kubelet-plugins/volume/exec/nodeagent~uds ipAutodetectionMethod : \"\" podSecurityPolicy : defaultPolicy : 00-k0s-privileged workerProfiles : [] images : konnectivity : image : us.gcr.io/k8s-artifacts-prod/kas-network-proxy/proxy-agent version : v0.0.13 metricsserver : image : gcr.io/k8s-staging-metrics-server/metrics-server version : v0.3.7 kubeproxy : image : k8s.gcr.io/kube-proxy version : v1.20.2 coredns : image : docker.io/coredns/coredns version : 1.7.0 calico : cni : image : calico/cni version : v3.16.2 flexvolume : image : calico/pod2daemon-flexvol version : v3.16.2 node : image : calico/node version : v3.16.2 kubecontrollers : image : calico/kube-controllers version : v3.16.2 repository : \"\" telemetry : interval : 10m0s enabled : true extensions : helm : repositories : - name : stable url : https://charts.helm.sh/stable - name : prometheus-community url : https://prometheus-community.github.io/helm-charts charts : - name : prometheus-stack chartname : prometheus-community/prometheus version : \"11.16.8\" values : | server: podDisruptionBudget: enabled: false namespace : default spec.api # externalAddress : If k0s controllers are running behind a loadbalancer provide the loadbalancer address here. This will configure all cluster components to connect to this address and also configures this address to be used when joining new nodes into the cluster. address : The local address to bind API on. Also used as one of the addresses pushed on the k0s create service certificate on the API. Defaults to first non-local address found on the node. sans : List of additional addresses to push to API servers serving certificate extraArgs : Map of key-values (strings) for any extra arguments you wish to pass down to Kubernetes api-server process spec.controllerManager # extraArgs : Map of key-values (strings) for any extra arguments you wish to pass down to Kubernetes controller manager process spec.scheduler # extraArgs : Map of key-values (strings) for any extra arguments you wish to pass down to Kubernetes scheduler process spec.storage # type : Type of the data store, either etcd or kine . etcd.peerAddress : Nodes address to be used for etcd cluster peering. kine.dataSource : kine datasource URL. Using type etcd will make k0s to create and manage an elastic etcd cluster within the controller nodes. spec.network # provider : Network provider, either calico or custom . In case of custom user can push any network provider. podCIDR : Pod network CIDR to be used in the cluster serviceCIDR : Network CIDR to be used for cluster VIP services. Note: In case of custom network it's fully in users responsibility to configure ALL the CNI related setups. This includes the CNI provider itself plus all the host levels setups it might need such as CNI binaries. spec.network.calico # mode : vxlan (default) or ipip vxlanPort : The UDP port to use for VXLAN (default 4789 ) vxlanVNI : The virtual network ID to use for VXLAN. (default: 4096 ) mtu : MTU to use for overlay network (default 1450 ) wireguard : enable wireguard based encryption (default false ). Your host system must be wireguard ready. See https://docs.projectcalico.org/security/encrypt-cluster-pod-traffic for details. flexVolumeDriverPath : The host path to use for Calicos flex-volume-driver (default: /usr/libexec/k0s/kubelet-plugins/volume/exec/nodeagent~uds ). This should only need to be changed if the default path is unwriteable. See https://github.com/projectcalico/calico/issues/2712 for details. This option should ideally be paired with a custom volumePluginDir in the profile used on your worker nodes. ipAutodetectionMethod : To force non-default behaviour for Calico to pick up the interface for pod network inter-node routing. (default \"\" , i.e. not set so Calico will use it's own defaults) See more at: https://docs.projectcalico.org/reference/node/configuration#ip-autodetection-methods spec.podSecurityPolicy # Configures the default psp to be set. k0s creates two PSPs out of box: 00-k0s-privileged (default): no restrictions, always also used for Kubernetes/k0s level system pods 99-k0s-restricted : no host namespaces or root users allowed, no bind mounts from host As a user you can of course create any supplemental PSPs and bind them to users / access accounts as you need. spec.workerProfiles # Array of spec.workerProfiles.workerProfile Each element has following properties: - name : string, name, used as profile selector for the worker process - values : mapping object For each profile the control plane will create separate ConfigMap with kubelet-config yaml. Based on the --profile argument given to the k0s worker the corresponding ConfigMap would be used to extract kubelet-config.yaml from. values are recursively merged with default kubelet-config.yaml There are a few fields that cannot be overridden: - clusterDNS - clusterDomain - apiVersion - kind Example: workerProfiles: - name: custom-role values: key: value mapping: innerKey: innerValue Custom volumePluginDir: workerProfiles: - name: custom-role values: volumePluginDir: /var/libexec/k0s/kubelet-plugins/volume/exec images # Each node under the images key has the same structure images: konnectivity: image: calico/kube-controllers version: v3.16.2 Following keys are available: images.konnectivity images.metricsserver images.kubeproxy images.coredns images.calico.cni images.calico.flexvolume images.calico.node images.calico.kubecontrollers images.repository If images.repository is set and not empty, every image will be pulled from images.repository Example: images: repository: \"my.own.repo\" konnectivity: image: calico/kube-controllers version: v3.16.2 metricsserver: image: gcr.io/k8s-staging-metrics-server/metrics-server version: v0.3.7 In the runtime the image names will be calculated as my.own.repo/calico/kube-controllers:v3.16.2 and my.own.repo/k8s-staging-metrics-server/metrics-server . This only affects the location where images are getting pulled, omitting an image specification here will not disable the component from being deployed. Extensions # As stated in the project scope we intent to keep the scope of k0s quite small and not build gazillions of extensions into the product itself. To run k0s easily with your preferred extensions you have two options. Dump all needed extension manifest under /var/lib/k0s/manifests/my-extension . Read more on this approach here . Define your extensions as Helm charts : extensions: helm: repositories: - name: stable url: https://charts.helm.sh/stable - name: prometheus-community url: https://prometheus-community.github.io/helm-charts charts: - name: prometheus-stack chartname: prometheus-community/prometheus version: \"11.16.8\" values: | storageSpec: emptyDir: medium: Memory namespace: default This way you get a declarative way to configure the cluster and k0s controller manages the setup of the defined extension Helm charts as part of the cluster bootstrap process. Some examples what you could use as extension charts: - Ingress controllers: Nginx ingress , Traefix ingress ( tutorial ), - Volume storage providers: OpenEBS , Rook , Longhorn - Monitoring: Prometheus , Grafana Telemetry # To build better end user experience we collect and send telemetry data from clusters. It is enabled by default and can be disabled by settings corresponding option as false The default interval is 10 minutes, any valid value for time.Duration string representation can be used as a value. Example telemetry: interval: 2m0s enabled: true","title":"Configuration Options"},{"location":"configuration/#configuration-options","text":"","title":"Configuration options"},{"location":"configuration/#control-plane","text":"k0s Control plane can be configured via a YAML config file. By default k0s server command reads a file called k0s.yaml but can be told to read any yaml file via --config option. An example config file with the most common options users should configure: apiVersion : k0s.k0sproject.io/v1beta1 kind : Cluster metadata : name : k0s spec : api : address : 192.168.68.106 sans : - my-k0s-control.my-domain.com network : podCIDR : 10.244.0.0/16 serviceCIDR : 10.96.0.0/12 extensions : helm : repositories : - name : prometheus-community url : https://prometheus-community.github.io/helm-charts charts : - name : prometheus-stack chartname : prometheus-community/prometheus version : \"11.16.8\" namespace : default","title":"Control plane"},{"location":"configuration/#specapi","text":"address : The local address to bind API on. Also used as one of the addresses pushed on the k0s create service certificate on the API. Defaults to first non-local address found on the node. sans : List of additional addresses to push to API servers serving certificate","title":"spec.api"},{"location":"configuration/#specnetwork","text":"podCIDR : Pod network CIDR to be used in the cluster serviceCIDR : Network CIDR to be used for cluster VIP services.","title":"spec.network"},{"location":"configuration/#extensionshelm","text":"List of Helm repositories and charts to deploy during cluster bootstrap. This example configures Prometheus from \"stable\" Helms chart repository.","title":"extensions.helm"},{"location":"configuration/#configuring-an-ha-control-plane","text":"The following pre-requisites are required in order to configure an HA control plane:","title":"Configuring an HA Control Plane"},{"location":"configuration/#requirements","text":"","title":"Requirements"},{"location":"configuration/#full-config-reference","text":"Note: Many of the options configure things deep down in the \"stack\" on various components. So please make sure you understand what is being configured and whether or not it works in your specific environment. A full config file with defaults generated by the k0s default-config command: apiVersion : k0s.k0sproject.io/v1beta1 kind : Cluster metadata : name : k0s spec : api : externalAddress : my-lb-address.example.com address : 192.168.68.106 sans : - 192.168.68.106 extraArgs : {} controllerManager : extraArgs : {} scheduler : extraArgs : {} storage : type : etcd etcd : peerAddress : 192.168.68.106 network : podCIDR : 10.244.0.0/16 serviceCIDR : 10.96.0.0/12 provider : calico calico : mode : vxlan vxlanPort : 4789 vxlanVNI : 4096 mtu : 1450 wireguard : false flexVolumeDriverPath : /usr/libexec/k0s/kubelet-plugins/volume/exec/nodeagent~uds ipAutodetectionMethod : \"\" podSecurityPolicy : defaultPolicy : 00-k0s-privileged workerProfiles : [] images : konnectivity : image : us.gcr.io/k8s-artifacts-prod/kas-network-proxy/proxy-agent version : v0.0.13 metricsserver : image : gcr.io/k8s-staging-metrics-server/metrics-server version : v0.3.7 kubeproxy : image : k8s.gcr.io/kube-proxy version : v1.20.2 coredns : image : docker.io/coredns/coredns version : 1.7.0 calico : cni : image : calico/cni version : v3.16.2 flexvolume : image : calico/pod2daemon-flexvol version : v3.16.2 node : image : calico/node version : v3.16.2 kubecontrollers : image : calico/kube-controllers version : v3.16.2 repository : \"\" telemetry : interval : 10m0s enabled : true extensions : helm : repositories : - name : stable url : https://charts.helm.sh/stable - name : prometheus-community url : https://prometheus-community.github.io/helm-charts charts : - name : prometheus-stack chartname : prometheus-community/prometheus version : \"11.16.8\" values : | server: podDisruptionBudget: enabled: false namespace : default","title":"Full config reference"},{"location":"configuration/#specapi_1","text":"externalAddress : If k0s controllers are running behind a loadbalancer provide the loadbalancer address here. This will configure all cluster components to connect to this address and also configures this address to be used when joining new nodes into the cluster. address : The local address to bind API on. Also used as one of the addresses pushed on the k0s create service certificate on the API. Defaults to first non-local address found on the node. sans : List of additional addresses to push to API servers serving certificate extraArgs : Map of key-values (strings) for any extra arguments you wish to pass down to Kubernetes api-server process","title":"spec.api"},{"location":"configuration/#speccontrollermanager","text":"extraArgs : Map of key-values (strings) for any extra arguments you wish to pass down to Kubernetes controller manager process","title":"spec.controllerManager"},{"location":"configuration/#specscheduler","text":"extraArgs : Map of key-values (strings) for any extra arguments you wish to pass down to Kubernetes scheduler process","title":"spec.scheduler"},{"location":"configuration/#specstorage","text":"type : Type of the data store, either etcd or kine . etcd.peerAddress : Nodes address to be used for etcd cluster peering. kine.dataSource : kine datasource URL. Using type etcd will make k0s to create and manage an elastic etcd cluster within the controller nodes.","title":"spec.storage"},{"location":"configuration/#specnetwork_1","text":"provider : Network provider, either calico or custom . In case of custom user can push any network provider. podCIDR : Pod network CIDR to be used in the cluster serviceCIDR : Network CIDR to be used for cluster VIP services. Note: In case of custom network it's fully in users responsibility to configure ALL the CNI related setups. This includes the CNI provider itself plus all the host levels setups it might need such as CNI binaries.","title":"spec.network"},{"location":"configuration/#specpodsecuritypolicy","text":"Configures the default psp to be set. k0s creates two PSPs out of box: 00-k0s-privileged (default): no restrictions, always also used for Kubernetes/k0s level system pods 99-k0s-restricted : no host namespaces or root users allowed, no bind mounts from host As a user you can of course create any supplemental PSPs and bind them to users / access accounts as you need.","title":"spec.podSecurityPolicy"},{"location":"configuration/#specworkerprofiles","text":"Array of spec.workerProfiles.workerProfile Each element has following properties: - name : string, name, used as profile selector for the worker process - values : mapping object For each profile the control plane will create separate ConfigMap with kubelet-config yaml. Based on the --profile argument given to the k0s worker the corresponding ConfigMap would be used to extract kubelet-config.yaml from. values are recursively merged with default kubelet-config.yaml There are a few fields that cannot be overridden: - clusterDNS - clusterDomain - apiVersion - kind Example: workerProfiles: - name: custom-role values: key: value mapping: innerKey: innerValue Custom volumePluginDir: workerProfiles: - name: custom-role values: volumePluginDir: /var/libexec/k0s/kubelet-plugins/volume/exec","title":"spec.workerProfiles"},{"location":"configuration/#images","text":"Each node under the images key has the same structure images: konnectivity: image: calico/kube-controllers version: v3.16.2 Following keys are available: images.konnectivity images.metricsserver images.kubeproxy images.coredns images.calico.cni images.calico.flexvolume images.calico.node images.calico.kubecontrollers images.repository If images.repository is set and not empty, every image will be pulled from images.repository Example: images: repository: \"my.own.repo\" konnectivity: image: calico/kube-controllers version: v3.16.2 metricsserver: image: gcr.io/k8s-staging-metrics-server/metrics-server version: v0.3.7 In the runtime the image names will be calculated as my.own.repo/calico/kube-controllers:v3.16.2 and my.own.repo/k8s-staging-metrics-server/metrics-server . This only affects the location where images are getting pulled, omitting an image specification here will not disable the component from being deployed.","title":"images"},{"location":"configuration/#extensions","text":"As stated in the project scope we intent to keep the scope of k0s quite small and not build gazillions of extensions into the product itself. To run k0s easily with your preferred extensions you have two options. Dump all needed extension manifest under /var/lib/k0s/manifests/my-extension . Read more on this approach here . Define your extensions as Helm charts : extensions: helm: repositories: - name: stable url: https://charts.helm.sh/stable - name: prometheus-community url: https://prometheus-community.github.io/helm-charts charts: - name: prometheus-stack chartname: prometheus-community/prometheus version: \"11.16.8\" values: | storageSpec: emptyDir: medium: Memory namespace: default This way you get a declarative way to configure the cluster and k0s controller manages the setup of the defined extension Helm charts as part of the cluster bootstrap process. Some examples what you could use as extension charts: - Ingress controllers: Nginx ingress , Traefix ingress ( tutorial ), - Volume storage providers: OpenEBS , Rook , Longhorn - Monitoring: Prometheus , Grafana","title":"Extensions"},{"location":"configuration/#telemetry","text":"To build better end user experience we collect and send telemetry data from clusters. It is enabled by default and can be disabled by settings corresponding option as false The default interval is 10 minutes, any valid value for time.Duration string representation can be used as a value. Example telemetry: interval: 2m0s enabled: true","title":"Telemetry"},{"location":"conformance-testing/","text":"Kubernetes conformance testing for k0s # We run the conformance testing for the last RC build for a release. Follow the instructions as the conformance testing repository. In a nutshell, you need to: - Setup k0s on some VMs/bare metal boxes - Download, if you do not already have, sonobuoy tool - Run the conformance tests with something like sonobuoy run --mode=certified-conformance - Wait for couple hours - Collect results","title":"Kubernetes conformance testing for k0s"},{"location":"conformance-testing/#kubernetes-conformance-testing-for-k0s","text":"We run the conformance testing for the last RC build for a release. Follow the instructions as the conformance testing repository. In a nutshell, you need to: - Setup k0s on some VMs/bare metal boxes - Download, if you do not already have, sonobuoy tool - Run the conformance tests with something like sonobuoy run --mode=certified-conformance - Wait for couple hours - Collect results","title":"Kubernetes conformance testing for k0s"},{"location":"containerd_config/","text":"containerd configuration # containerd is an industry-standard container runtime. NOTE: In most use cases changes to the containerd configuration will not be required. In order to make changes to containerd configuration first you need to generate a default containerd configuration by running: containerd config default > /etc/k0s/containerd.toml This command will set the default values to /etc/k0s/containerd.toml . k0s runs containerd with the following default values: /var/lib/k0s/bin/containerd \\ --root=/var/lib/k0s/containerd \\ --state=/var/lib/k0s/run/containerd \\ --address=/var/lib/k0s/run/containerd.sock \\ --config=/etc/k0s/containerd.toml Before proceeding further, add the following default values to the configuration file: version = 2 root = \"/var/lib/k0s/containerd\" state = \"/var/lib/k0s/run/containerd\" ... [grpc] address = \"/var/lib/k0s/run/containerd.sock\" Next if you want to change CRI look into this section [plugins.\"io.containerd.runtime.v1.linux\"] shim = \"containerd-shim\" runtime = \"runc\" Using gVisor # gVisor is an application kernel, written in Go, that implements a substantial portion of the Linux system call interface. It provides an additional layer of isolation between running applications and the host operating system. First you must install the needed gVisor binaries into the host. ( set -e URL = https://storage.googleapis.com/gvisor/releases/release/latest wget ${ URL } /runsc ${ URL } /runsc.sha512 \\ ${ URL } /gvisor-containerd-shim ${ URL } /gvisor-containerd-shim.sha512 \\ ${ URL } /containerd-shim-runsc-v1 ${ URL } /containerd-shim-runsc-v1.sha512 sha512sum -c runsc.sha512 \\ -c gvisor-containerd-shim.sha512 \\ -c containerd-shim-runsc-v1.sha512 rm -f *.sha512 chmod a+rx runsc gvisor-containerd-shim containerd-shim-runsc-v1 sudo mv runsc gvisor-containerd-shim containerd-shim-runsc-v1 /usr/local/bin ) See gVisor install docs Next we need to prepare the config for k0s managed containerD to utilize gVisor as additional runtime: cat <<EOF | sudo tee /etc/k0s/containerd.toml disabled_plugins = [\"restart\"] [plugins.linux] shim_debug = true [plugins.cri.containerd.runtimes.runsc] runtime_type = \"io.containerd.runsc.v1\" EOF Then we can start and join the worker as normally into the cluster: k0s worker $token By default containerd uses nromal runc as the runtime. To make gVisor runtime usable for workloads we must register it to Kubernetes side: cat <<EOF | kubectl apply -f - apiVersion: node.k8s.io/v1beta1 kind: RuntimeClass metadata: name: gvisor handler: runsc EOF After this we can use it for our workloads: apiVersion : v1 kind : Pod metadata : name : nginx-gvisor spec : runtimeClassName : gvisor containers : - name : nginx image : nginx We can verify the created nginx pod is actually running under gVisor runtime: # kubectl exec nginx-gvisor -- dmesg | grep -i gvisor [ 0.000000] Starting gVisor... Using custom nvidia-container-runtime # By default CRI is set tu runC and if you want to configure Nvidia GPU support you will have to replace runc with nvidia-container-runtime as shown below: [plugins.\"io.containerd.runtime.v1.linux\"] shim = \"containerd-shim\" runtime = \"nvidia-container-runtime\" Note To run nvidia-container-runtime on your node please look here for detailed instructions. After changes to the configuration, restart k0s and in this case containerd will be using newly configured runtime.","title":"Containerd Configuration"},{"location":"containerd_config/#containerd-configuration","text":"containerd is an industry-standard container runtime. NOTE: In most use cases changes to the containerd configuration will not be required. In order to make changes to containerd configuration first you need to generate a default containerd configuration by running: containerd config default > /etc/k0s/containerd.toml This command will set the default values to /etc/k0s/containerd.toml . k0s runs containerd with the following default values: /var/lib/k0s/bin/containerd \\ --root=/var/lib/k0s/containerd \\ --state=/var/lib/k0s/run/containerd \\ --address=/var/lib/k0s/run/containerd.sock \\ --config=/etc/k0s/containerd.toml Before proceeding further, add the following default values to the configuration file: version = 2 root = \"/var/lib/k0s/containerd\" state = \"/var/lib/k0s/run/containerd\" ... [grpc] address = \"/var/lib/k0s/run/containerd.sock\" Next if you want to change CRI look into this section [plugins.\"io.containerd.runtime.v1.linux\"] shim = \"containerd-shim\" runtime = \"runc\"","title":"containerd configuration"},{"location":"containerd_config/#using-gvisor","text":"gVisor is an application kernel, written in Go, that implements a substantial portion of the Linux system call interface. It provides an additional layer of isolation between running applications and the host operating system. First you must install the needed gVisor binaries into the host. ( set -e URL = https://storage.googleapis.com/gvisor/releases/release/latest wget ${ URL } /runsc ${ URL } /runsc.sha512 \\ ${ URL } /gvisor-containerd-shim ${ URL } /gvisor-containerd-shim.sha512 \\ ${ URL } /containerd-shim-runsc-v1 ${ URL } /containerd-shim-runsc-v1.sha512 sha512sum -c runsc.sha512 \\ -c gvisor-containerd-shim.sha512 \\ -c containerd-shim-runsc-v1.sha512 rm -f *.sha512 chmod a+rx runsc gvisor-containerd-shim containerd-shim-runsc-v1 sudo mv runsc gvisor-containerd-shim containerd-shim-runsc-v1 /usr/local/bin ) See gVisor install docs Next we need to prepare the config for k0s managed containerD to utilize gVisor as additional runtime: cat <<EOF | sudo tee /etc/k0s/containerd.toml disabled_plugins = [\"restart\"] [plugins.linux] shim_debug = true [plugins.cri.containerd.runtimes.runsc] runtime_type = \"io.containerd.runsc.v1\" EOF Then we can start and join the worker as normally into the cluster: k0s worker $token By default containerd uses nromal runc as the runtime. To make gVisor runtime usable for workloads we must register it to Kubernetes side: cat <<EOF | kubectl apply -f - apiVersion: node.k8s.io/v1beta1 kind: RuntimeClass metadata: name: gvisor handler: runsc EOF After this we can use it for our workloads: apiVersion : v1 kind : Pod metadata : name : nginx-gvisor spec : runtimeClassName : gvisor containers : - name : nginx image : nginx We can verify the created nginx pod is actually running under gVisor runtime: # kubectl exec nginx-gvisor -- dmesg | grep -i gvisor [ 0.000000] Starting gVisor...","title":"Using gVisor"},{"location":"containerd_config/#using-custom-nvidia-container-runtime","text":"By default CRI is set tu runC and if you want to configure Nvidia GPU support you will have to replace runc with nvidia-container-runtime as shown below: [plugins.\"io.containerd.runtime.v1.linux\"] shim = \"containerd-shim\" runtime = \"nvidia-container-runtime\" Note To run nvidia-container-runtime on your node please look here for detailed instructions. After changes to the configuration, restart k0s and in this case containerd will be using newly configured runtime.","title":"Using custom nvidia-container-runtime"},{"location":"create-cluster/","text":"Creating a multi-node cluster # As k0s binary has everything it needs packaged into a single binary, it makes it super easy to spin up Kubernetes clusters. Pre-requisites # Download k0s binary from releases and push it to all the nodes you wish to connect to the cluster. That's it, really. Bootstrapping controller node # Create a configuration file if you wish to tune some of the settings. $ k0s server -c k0s.yaml That's it, really. k0s process will act as a \"supervisor\" for all the control plane components. In few seconds you'll have the control plane up-and-running. Naturally, to make k0s boot up the control plane when the node itself reboots you should really make the k0s process to be supervised by systemd or some other init system. Create join token # To be able to join workers into the cluster we need a token. The token embeds information with which we can enable mutual trust between the worker and controller(s) and allow the node to join the cluster as worker. To get a token run the following on one of the existing controller nodes: k0s token create --role = worker This will output a long token which we will use to join the worker. To enhance security, we can also set an expiration time on the tokens by using: k0s token create --role = worker --expiry = \"100h\" Joining worker(s) to cluster # To join the worker we need to run k0s in worker mode with the token from previous step: $ k0s worker \"long-join-token\" That's it, really. Naturally, to make k0s boot up the worker components when the node itself reboots you should really make the k0s process to be supervised by systemd or some other init system. Tokens # The tokens are actually base64 encoded kubeconfigs . Why: - well defined structure - can be used directly as bootstrap auth configs for kubelet - embeds CA info for mutual trust The actual bearer token embedded in the kubeconfig is a bootstrap token . For controller join token and for worker join token we use different usage attributes so we can make sure we can validate the token role on the controller side. Join controller node # To be able to join a new controller node into the cluster you must be using either etcd or some externalized data store (MySQL or Postgres) via kine. Also make sure the configurations match for the data storage on all controller nodes. To create a join token for the new controller, run the following on existing controller node: k0s token create --role = controller --expiry = 1h On the new controller, run: k0s server \"long-join-token\" Adding a Cluster User # To add a user to cluster, use the kubeconfig create command. This will output a kubeconfig for the user, which can be used for authentication. On the controller, run the following to generate a kubeconfig for a user: k0s kubeconfig create [ username ] Enabling Access to Cluster Resources # To allow the user access to the cluster, the user needs to be created with the system:masters group: clusterUser = \"testUser\" k0s kubeconfig create --groups \"system:masters\" $clusterUser > ~/.kube/config Create the proper roleBinding, to allow the user access to the resources: kubectl create clusterrolebinding $clusterUser -admin-binding --clusterrole = admin --user = $clusterUser Service and Log Setup # k0s install sub-command was created as a helper command to allow users to easily install k0s as a service. For more information, read here . Enabling Shell Completion # The k0s completion script for Bash, zsh, fish and powershell can be generated with the command k0s completion < shell > . Sourcing the completion script in your shell enables k0s autocompletion. Bash # echo 'source <(k0s completion bash)' >>~/.bashrc # To load completions for each session, execute once: $ k0s completion bash > /etc/bash_completion.d/k0s Zsh # If shell completion is not already enabled in your environment you will need to enable it. You can execute the following once: $ echo \"autoload -U compinit; compinit\" >> ~/.zshrc # To load completions for each session, execute once: $ k0s completion zsh > \" ${ fpath [1] } /_k0s\" You will need to start a new shell for this setup to take effect. Fish # $ k0s completion fish | source # To load completions for each session, execute once: $ k0s completion fish > ~/.config/fish/completions/k0s.fish","title":"Multi-Node Cluster"},{"location":"create-cluster/#creating-a-multi-node-cluster","text":"As k0s binary has everything it needs packaged into a single binary, it makes it super easy to spin up Kubernetes clusters.","title":"Creating a multi-node cluster"},{"location":"create-cluster/#pre-requisites","text":"Download k0s binary from releases and push it to all the nodes you wish to connect to the cluster. That's it, really.","title":"Pre-requisites"},{"location":"create-cluster/#bootstrapping-controller-node","text":"Create a configuration file if you wish to tune some of the settings. $ k0s server -c k0s.yaml That's it, really. k0s process will act as a \"supervisor\" for all the control plane components. In few seconds you'll have the control plane up-and-running. Naturally, to make k0s boot up the control plane when the node itself reboots you should really make the k0s process to be supervised by systemd or some other init system.","title":"Bootstrapping controller node"},{"location":"create-cluster/#create-join-token","text":"To be able to join workers into the cluster we need a token. The token embeds information with which we can enable mutual trust between the worker and controller(s) and allow the node to join the cluster as worker. To get a token run the following on one of the existing controller nodes: k0s token create --role = worker This will output a long token which we will use to join the worker. To enhance security, we can also set an expiration time on the tokens by using: k0s token create --role = worker --expiry = \"100h\"","title":"Create join token"},{"location":"create-cluster/#joining-workers-to-cluster","text":"To join the worker we need to run k0s in worker mode with the token from previous step: $ k0s worker \"long-join-token\" That's it, really. Naturally, to make k0s boot up the worker components when the node itself reboots you should really make the k0s process to be supervised by systemd or some other init system.","title":"Joining worker(s) to cluster"},{"location":"create-cluster/#tokens","text":"The tokens are actually base64 encoded kubeconfigs . Why: - well defined structure - can be used directly as bootstrap auth configs for kubelet - embeds CA info for mutual trust The actual bearer token embedded in the kubeconfig is a bootstrap token . For controller join token and for worker join token we use different usage attributes so we can make sure we can validate the token role on the controller side.","title":"Tokens"},{"location":"create-cluster/#join-controller-node","text":"To be able to join a new controller node into the cluster you must be using either etcd or some externalized data store (MySQL or Postgres) via kine. Also make sure the configurations match for the data storage on all controller nodes. To create a join token for the new controller, run the following on existing controller node: k0s token create --role = controller --expiry = 1h On the new controller, run: k0s server \"long-join-token\"","title":"Join controller node"},{"location":"create-cluster/#adding-a-cluster-user","text":"To add a user to cluster, use the kubeconfig create command. This will output a kubeconfig for the user, which can be used for authentication. On the controller, run the following to generate a kubeconfig for a user: k0s kubeconfig create [ username ]","title":"Adding a Cluster User"},{"location":"create-cluster/#enabling-access-to-cluster-resources","text":"To allow the user access to the cluster, the user needs to be created with the system:masters group: clusterUser = \"testUser\" k0s kubeconfig create --groups \"system:masters\" $clusterUser > ~/.kube/config Create the proper roleBinding, to allow the user access to the resources: kubectl create clusterrolebinding $clusterUser -admin-binding --clusterrole = admin --user = $clusterUser","title":"Enabling Access to Cluster Resources"},{"location":"create-cluster/#service-and-log-setup","text":"k0s install sub-command was created as a helper command to allow users to easily install k0s as a service. For more information, read here .","title":"Service and Log Setup"},{"location":"create-cluster/#enabling-shell-completion","text":"The k0s completion script for Bash, zsh, fish and powershell can be generated with the command k0s completion < shell > . Sourcing the completion script in your shell enables k0s autocompletion.","title":"Enabling Shell Completion"},{"location":"create-cluster/#bash","text":"echo 'source <(k0s completion bash)' >>~/.bashrc # To load completions for each session, execute once: $ k0s completion bash > /etc/bash_completion.d/k0s","title":"Bash"},{"location":"create-cluster/#zsh","text":"If shell completion is not already enabled in your environment you will need to enable it. You can execute the following once: $ echo \"autoload -U compinit; compinit\" >> ~/.zshrc # To load completions for each session, execute once: $ k0s completion zsh > \" ${ fpath [1] } /_k0s\" You will need to start a new shell for this setup to take effect.","title":"Zsh"},{"location":"create-cluster/#fish","text":"$ k0s completion fish | source # To load completions for each session, execute once: $ k0s completion fish > ~/.config/fish/completions/k0s.fish","title":"Fish"},{"location":"custom-cri-runtime/","text":"Custom CRI runtime # k0s supports users bringing their own CRI runtime (for example, docker). In which case, k0s will not start nor manage the runtime, and it is fully up to the user to configure it properly. To run a k0s worker with a custom CRI runtime use the option --cri-socket . It takes input in the form of <type>:<socket> where: type : Either remote or docker . Use docker for pure docker setup, remote for anything else. socket : Path to the socket, examples: unix:///var/run/docker.sock To run k0s with pre-existing docker setup run the worker with k0s worker --cri-socket docker:unix:///var/run/docker.sock <token> . When docker is used as a runtime, k0s will configure kubelet to create the dockershim socket at /var/run/dockershim.sock .","title":"Using a Custom CRI"},{"location":"custom-cri-runtime/#custom-cri-runtime","text":"k0s supports users bringing their own CRI runtime (for example, docker). In which case, k0s will not start nor manage the runtime, and it is fully up to the user to configure it properly. To run a k0s worker with a custom CRI runtime use the option --cri-socket . It takes input in the form of <type>:<socket> where: type : Either remote or docker . Use docker for pure docker setup, remote for anything else. socket : Path to the socket, examples: unix:///var/run/docker.sock To run k0s with pre-existing docker setup run the worker with k0s worker --cri-socket docker:unix:///var/run/docker.sock <token> . When docker is used as a runtime, k0s will configure kubelet to create the dockershim socket at /var/run/dockershim.sock .","title":"Custom CRI runtime"},{"location":"dual-stack/","text":"Dual-stack networking # To enable dual-stack networking use the following k0s.yaml as an example. This settings will set up bundled calico cni, enable feature gates for the Kubernetes components and set up kubernetes-controller-manager. spec: network: podCIDR: \"10.244.0.0/16\" serviceCIDR: \"10.96.0.0/12\" calico: mode: \"bird\" dualStack: enabled: true IPv6podCIDR: \"fd00::/108\" IPv6serviceCIDR: \"fd01::/108\" CNI settings # Calico settings # Calico doesn't support tunneling for the IPv6, so \"vxlan\" and \"ipip\" backend wouldn't work. If you need to have cross-pod connectivity, you need to use \"bird\" as a backend mode. In any other mode the pods would be able to reach only pods on the same node. External CNI # The k0s.yaml dualStack section will enable all the neccessary feature gates for the Kubernetes components but in case of using external CNI it must be set up with IPv6 support. Additional materials # https://kubernetes.io/docs/concepts/services-networking/dual-stack/ https://kubernetes.io/docs/tasks/network/validate-dual-stack/ https://www.projectcalico.org/dual-stack-operation-with-calico-on-kubernetes/ https://docs.projectcalico.org/networking/ipv6","title":"IPv4/IPv6 dual-stack networking"},{"location":"dual-stack/#dual-stack-networking","text":"To enable dual-stack networking use the following k0s.yaml as an example. This settings will set up bundled calico cni, enable feature gates for the Kubernetes components and set up kubernetes-controller-manager. spec: network: podCIDR: \"10.244.0.0/16\" serviceCIDR: \"10.96.0.0/12\" calico: mode: \"bird\" dualStack: enabled: true IPv6podCIDR: \"fd00::/108\" IPv6serviceCIDR: \"fd01::/108\"","title":"Dual-stack networking"},{"location":"dual-stack/#cni-settings","text":"","title":"CNI settings"},{"location":"dual-stack/#calico-settings","text":"Calico doesn't support tunneling for the IPv6, so \"vxlan\" and \"ipip\" backend wouldn't work. If you need to have cross-pod connectivity, you need to use \"bird\" as a backend mode. In any other mode the pods would be able to reach only pods on the same node.","title":"Calico settings"},{"location":"dual-stack/#external-cni","text":"The k0s.yaml dualStack section will enable all the neccessary feature gates for the Kubernetes components but in case of using external CNI it must be set up with IPv6 support.","title":"External CNI"},{"location":"dual-stack/#additional-materials","text":"https://kubernetes.io/docs/concepts/services-networking/dual-stack/ https://kubernetes.io/docs/tasks/network/validate-dual-stack/ https://www.projectcalico.org/dual-stack-operation-with-calico-on-kubernetes/ https://docs.projectcalico.org/networking/ipv6","title":"Additional materials"},{"location":"experimental-windows/","text":"Running k0s worker nodes in Windows # Experimental status # Windows support feature is under active development and MUST BE considered as experemential. Requirements # The cluster must have at least one worker node and control plane running on Linux. Windows can be used for running additional worker nodes. Build # make clean k0s.exe This should create k0s.exe with staged kubelet.exe and kube-proxy.exe Description # the k0s.exe supervises kubelet.exe and kube-proxy.exe During the first run calico install script created as C:\\bootstrap.ps1 The bootstrap script downloads the calico binaries, builds pause container and set ups vSwitch settings. Running # It is expected to have docker EE installed on the windows node (we need it during the initial calico set up) C:\\>k0s.exe worker --cri-socket=docker:tcp://127.0.0.1:2375 --cidr-range=<cidr_range> --cluster-dns=<clusterdns> --api-server=<k0s api> <token> Cluster control plane must be inited with proper config (see section below) Configuration # Strict-affinity # To run windows node we need to have strict affinity enabled. There is a configuration field spec.network.calico.withWindowsNodes , equals false by default. If set to the true, the additional calico related manifest /var/lib/k0s/manifests/calico/calico-IPAMConfig-ipamconfig.yaml would be created with the following values --- apiVersion: crd.projectcalico.org/v1 kind: IPAMConfig metadata: name: default spec: strictAffinity: true Another way is to use calicoctl manually: calicoctl ipam configure --strictaffinity=true Network connectivity in AWS # The network interface attached to your EC2 instance MUST have disabled \u201cChange Source/Dest. Check\u201d option. In AWS console option can be found on the Actions menu for a selected network interface. Hacks # We need to figure out proper way to pass cluster settings from controller plane to worker. While we don't have it, there are CLI arguments: - cidr-range - cluster-dns - api-server Some useful commands # Run pod with cmd.exe shell kubectl run win --image=hello-world:nanoserver --command=true -i --attach=true -- cmd.exe Manifest for pod with IIS web-server apiVersion: v1 kind: Pod metadata: name: iis spec: containers: - name: iis image: mcr.microsoft.com/windows/servercore/iis imagePullPolicy: IfNotPresent","title":"Windows (experimental)"},{"location":"experimental-windows/#running-k0s-worker-nodes-in-windows","text":"","title":"Running k0s worker nodes in Windows"},{"location":"experimental-windows/#experimental-status","text":"Windows support feature is under active development and MUST BE considered as experemential.","title":"Experimental status"},{"location":"experimental-windows/#requirements","text":"The cluster must have at least one worker node and control plane running on Linux. Windows can be used for running additional worker nodes.","title":"Requirements"},{"location":"experimental-windows/#build","text":"make clean k0s.exe This should create k0s.exe with staged kubelet.exe and kube-proxy.exe","title":"Build"},{"location":"experimental-windows/#description","text":"the k0s.exe supervises kubelet.exe and kube-proxy.exe During the first run calico install script created as C:\\bootstrap.ps1 The bootstrap script downloads the calico binaries, builds pause container and set ups vSwitch settings.","title":"Description"},{"location":"experimental-windows/#running","text":"It is expected to have docker EE installed on the windows node (we need it during the initial calico set up) C:\\>k0s.exe worker --cri-socket=docker:tcp://127.0.0.1:2375 --cidr-range=<cidr_range> --cluster-dns=<clusterdns> --api-server=<k0s api> <token> Cluster control plane must be inited with proper config (see section below)","title":"Running"},{"location":"experimental-windows/#configuration","text":"","title":"Configuration"},{"location":"experimental-windows/#strict-affinity","text":"To run windows node we need to have strict affinity enabled. There is a configuration field spec.network.calico.withWindowsNodes , equals false by default. If set to the true, the additional calico related manifest /var/lib/k0s/manifests/calico/calico-IPAMConfig-ipamconfig.yaml would be created with the following values --- apiVersion: crd.projectcalico.org/v1 kind: IPAMConfig metadata: name: default spec: strictAffinity: true Another way is to use calicoctl manually: calicoctl ipam configure --strictaffinity=true","title":"Strict-affinity"},{"location":"experimental-windows/#network-connectivity-in-aws","text":"The network interface attached to your EC2 instance MUST have disabled \u201cChange Source/Dest. Check\u201d option. In AWS console option can be found on the Actions menu for a selected network interface.","title":"Network connectivity in AWS"},{"location":"experimental-windows/#hacks","text":"We need to figure out proper way to pass cluster settings from controller plane to worker. While we don't have it, there are CLI arguments: - cidr-range - cluster-dns - api-server","title":"Hacks"},{"location":"experimental-windows/#some-useful-commands","text":"Run pod with cmd.exe shell kubectl run win --image=hello-world:nanoserver --command=true -i --attach=true -- cmd.exe Manifest for pod with IIS web-server apiVersion: v1 kind: Pod metadata: name: iis spec: containers: - name: iis image: mcr.microsoft.com/windows/servercore/iis imagePullPolicy: IfNotPresent","title":"Some useful commands"},{"location":"extensions/","text":"Cluster extensions # k0s allows users to use extensions to extend cluster functionality. At the moment the only supported type of extensions is helm based charts. The default configuration has no extensions. Helm based extensions # Configuration # Example. helm: repositories: - name: stable url: https://charts.helm.sh/stable - name: prometheus-community url: https://prometheus-community.github.io/helm-charts charts: - name: prometheus-stack chartname: prometheus-community/prometheus version: \"11.16.8\" values: | storageSpec: emptyDir: medium: Memory namespace: default By using the configuration above, the cluster would: - add stable and prometheus-community chart repositories - install the `prometheus-community/prometheus` chart of the specified version to the `default` namespace. The chart installation is implemented by using CRD `helm.k0sproject.io/Chart`. For every given helm extension the cluster creates a Chart CRD instance. The cluster has a controller which monitors for the Chart CRDs, supporting the following operations: - install - upgrade - delete For security reasons, the cluster operates only on Chart CRDs instantiated in the `kube-system` namespace, however, the target namespace could be any. #### CRD definition apiVersion: helm.k0sproject.io/v1beta1 kind: Chart metadata: creationTimestamp: \"2020-11-10T14:17:53Z\" generation: 2 labels: k0s.k0sproject.io/stack: helm name: k0s-addon-chart-test-addon namespace: kube-system resourceVersion: \"627\" selfLink: /apis/helm.k0sproject.io/v1beta1/namespaces/kube-system/charts/k0s-addon-chart-test-addon uid: ebe59ed4-1ff8-4d41-8e33-005b183651ed spec: chartName: prometheus-community/prometheus namespace: default values: | storageSpec: emptyDir: medium: Memory version: 11.16.8 status: appVersion: 2.21.0 namespace: default releaseName: prometheus-1605017878 revision: 2 updated: 2020-11-10 14:18:08.235656 +0000 UTC m=+41.871656901 version: 11.16.8 The Chart.spec defines the chart information. The Chart.status keeps the information about the last operation performed by the operator.","title":"Cluster extensions"},{"location":"extensions/#cluster-extensions","text":"k0s allows users to use extensions to extend cluster functionality. At the moment the only supported type of extensions is helm based charts. The default configuration has no extensions.","title":"Cluster extensions"},{"location":"extensions/#helm-based-extensions","text":"","title":"Helm based extensions"},{"location":"extensions/#configuration","text":"Example. helm: repositories: - name: stable url: https://charts.helm.sh/stable - name: prometheus-community url: https://prometheus-community.github.io/helm-charts charts: - name: prometheus-stack chartname: prometheus-community/prometheus version: \"11.16.8\" values: | storageSpec: emptyDir: medium: Memory namespace: default By using the configuration above, the cluster would: - add stable and prometheus-community chart repositories - install the `prometheus-community/prometheus` chart of the specified version to the `default` namespace. The chart installation is implemented by using CRD `helm.k0sproject.io/Chart`. For every given helm extension the cluster creates a Chart CRD instance. The cluster has a controller which monitors for the Chart CRDs, supporting the following operations: - install - upgrade - delete For security reasons, the cluster operates only on Chart CRDs instantiated in the `kube-system` namespace, however, the target namespace could be any. #### CRD definition apiVersion: helm.k0sproject.io/v1beta1 kind: Chart metadata: creationTimestamp: \"2020-11-10T14:17:53Z\" generation: 2 labels: k0s.k0sproject.io/stack: helm name: k0s-addon-chart-test-addon namespace: kube-system resourceVersion: \"627\" selfLink: /apis/helm.k0sproject.io/v1beta1/namespaces/kube-system/charts/k0s-addon-chart-test-addon uid: ebe59ed4-1ff8-4d41-8e33-005b183651ed spec: chartName: prometheus-community/prometheus namespace: default values: | storageSpec: emptyDir: medium: Memory version: 11.16.8 status: appVersion: 2.21.0 namespace: default releaseName: prometheus-1605017878 revision: 2 updated: 2020-11-10 14:18:08.235656 +0000 UTC m=+41.871656901 version: 11.16.8 The Chart.spec defines the chart information. The Chart.status keeps the information about the last operation performed by the operator.","title":"Configuration"},{"location":"install/","text":"Running k0s as a service # Sub-command k0s install allows users to easily install k0s as a service, and define its logging. Caveats # This command is strictly a helper command. It is not meant to provide a fully-automated solution, since you can run k0s in multiple, very different ways. It configures your service set-up as either a worker or a server, and will have different tasks, depending on the role you pick. Supported services: OpenRC & Systemd Server setup # This is the default mode of operation. When a server role is picked, the installer will do the following: Create user accounts for the different components (see https://github.com/k0sproject/k0s/blob/main/pkg/apis/v1beta1/system.go#L6) Create a service file (OpenRC/Systemd) and redirects logging to /var/log/k0s.log . If the --debug flag is used, it will also pass this flag along to the service file. Worker Setup # A worker cannot run with any other user, other than root , so no special users will be created. The service file will include the --token-file flag, with a value that needs to be manually changed. If the --debug flag is used, it will also pass this flag along to the service file. Single-node setup # Single-node configuration can be installed with 'k0s install server --enable-worker' command. Additional Documentation # see: k0s install","title":"Running k0s as a service"},{"location":"install/#running-k0s-as-a-service","text":"Sub-command k0s install allows users to easily install k0s as a service, and define its logging.","title":"Running k0s as a service"},{"location":"install/#caveats","text":"This command is strictly a helper command. It is not meant to provide a fully-automated solution, since you can run k0s in multiple, very different ways. It configures your service set-up as either a worker or a server, and will have different tasks, depending on the role you pick. Supported services: OpenRC & Systemd","title":"Caveats"},{"location":"install/#server-setup","text":"This is the default mode of operation. When a server role is picked, the installer will do the following: Create user accounts for the different components (see https://github.com/k0sproject/k0s/blob/main/pkg/apis/v1beta1/system.go#L6) Create a service file (OpenRC/Systemd) and redirects logging to /var/log/k0s.log . If the --debug flag is used, it will also pass this flag along to the service file.","title":"Server setup"},{"location":"install/#worker-setup","text":"A worker cannot run with any other user, other than root , so no special users will be created. The service file will include the --token-file flag, with a value that needs to be manually changed. If the --debug flag is used, it will also pass this flag along to the service file.","title":"Worker Setup"},{"location":"install/#single-node-setup","text":"Single-node configuration can be installed with 'k0s install server --enable-worker' command.","title":"Single-node setup"},{"location":"install/#additional-documentation","text":"see: k0s install","title":"Additional Documentation"},{"location":"k0s-in-docker/","text":"Running k0s in Docker # We publish a k0s container image with every release. By default, we run both controller and worker in the same container to provide an easy local testing \"cluster\". The containers are published both on Docker Hub and GitHub. The examples in this page show Docker Hub, because it's more simple to use. Using GitHub requires a separate authentication (not covered here). Alternative links: - docker.io/k0sproject/k0s:latest - docker.pkg.github.com/k0sproject/k0s/k0s: You can run your own k0s-in-docker easily with: docker run -d --name k0s --hostname k0s --privileged -v /var/lib/k0s -p 6443 :6443 docker.io/k0sproject/k0s:latest Just grab the kubeconfig file with docker exec k0s cat /var/lib/k0s/pki/admin.conf and paste e.g. into Lens . Running workers # If you want to attach multiple workers nodes into the cluster you can run separate containers for each worker. First, we need a join token for the worker: token = $( docker exec -t -i k0s k0s token create --role = worker ) Then join a new worker by running the container with: docker run -d --name k0s-worker1 --hostname k0s-worker1 --privileged -v /var/lib/k0s docker.io/k0sproject/k0s:latest k0s worker $token Repeat for as many workers you need, and have resources for. :) Docker Compose # You can also run k0s with Docker Compose: version : \"3.9\" services : k0s : container_name : k0s image : docker.io/k0sproject/k0s:latest command : k0s server --config=/etc/k0s/config.yaml --enable-worker hostname : k0s privileged : true volumes : - \"/var/lib/k0s\" tmpfs : - /run - /var/run ports : - \"6443:6443\" network_mode : \"bridge\" environment : K0S_CONFIG : |- apiVersion: k0s.k0sproject.io/v1beta1 kind: Cluster metadata: name: k0s # Any additional configuration goes here ... Known limitations # No custom Docker networks # Currently, we cannot run k0s nodes if the containers are configured to use custom networks e.g. with --net my-net . This is caused by the fact that Docker sets up a custom DNS service within the network and that messes up CoreDNS. We know that there are some workarounds possible, but they are bit hackish. And on the other hand, running k0s cluster(s) in bridge network should not cause issues.","title":"Docker"},{"location":"k0s-in-docker/#running-k0s-in-docker","text":"We publish a k0s container image with every release. By default, we run both controller and worker in the same container to provide an easy local testing \"cluster\". The containers are published both on Docker Hub and GitHub. The examples in this page show Docker Hub, because it's more simple to use. Using GitHub requires a separate authentication (not covered here). Alternative links: - docker.io/k0sproject/k0s:latest - docker.pkg.github.com/k0sproject/k0s/k0s: You can run your own k0s-in-docker easily with: docker run -d --name k0s --hostname k0s --privileged -v /var/lib/k0s -p 6443 :6443 docker.io/k0sproject/k0s:latest Just grab the kubeconfig file with docker exec k0s cat /var/lib/k0s/pki/admin.conf and paste e.g. into Lens .","title":"Running k0s in Docker"},{"location":"k0s-in-docker/#running-workers","text":"If you want to attach multiple workers nodes into the cluster you can run separate containers for each worker. First, we need a join token for the worker: token = $( docker exec -t -i k0s k0s token create --role = worker ) Then join a new worker by running the container with: docker run -d --name k0s-worker1 --hostname k0s-worker1 --privileged -v /var/lib/k0s docker.io/k0sproject/k0s:latest k0s worker $token Repeat for as many workers you need, and have resources for. :)","title":"Running workers"},{"location":"k0s-in-docker/#docker-compose","text":"You can also run k0s with Docker Compose: version : \"3.9\" services : k0s : container_name : k0s image : docker.io/k0sproject/k0s:latest command : k0s server --config=/etc/k0s/config.yaml --enable-worker hostname : k0s privileged : true volumes : - \"/var/lib/k0s\" tmpfs : - /run - /var/run ports : - \"6443:6443\" network_mode : \"bridge\" environment : K0S_CONFIG : |- apiVersion: k0s.k0sproject.io/v1beta1 kind: Cluster metadata: name: k0s # Any additional configuration goes here ...","title":"Docker Compose"},{"location":"k0s-in-docker/#known-limitations","text":"","title":"Known limitations"},{"location":"k0s-in-docker/#no-custom-docker-networks","text":"Currently, we cannot run k0s nodes if the containers are configured to use custom networks e.g. with --net my-net . This is caused by the fact that Docker sets up a custom DNS service within the network and that messes up CoreDNS. We know that there are some workarounds possible, but they are bit hackish. And on the other hand, running k0s cluster(s) in bridge network should not cause issues.","title":"No custom Docker networks"},{"location":"k0s-single-node/","text":"Creating a single-node cluster # These instructions outline a quick method for running a local k0s master and worker in a single node. NOTE: This method of running k0s is only recommended for dev, test or POC environments. Prepare dependencies # 1. Download the k0s binary # curl -sSLf https://get.k0s.sh | sh 2. Download the kubectl binary # sudo curl --output /usr/local/sbin/kubectl -L \"https://storage.googleapis.com/kubernetes-release/release/ $( curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt ) /bin/linux/amd64/kubectl\" 3. Make both binaries executable # sudo chmod +x /usr/local/sbin/kubectl sudo chmod +x /usr/bin/k0s Start k0s # 1. Create the k0s config directory # mkdir -p ${ HOME } /.k0s 2. Generate a default cluster configuration # k0s default-config | tee ${ HOME } /.k0s/k0s.yaml 3. Start k0s # sudo k0s server -c ${ HOME } /.k0s/k0s.yaml --enable-worker & Use kubectl to access k0s # 1. Save kubeconfig for user # sudo cat /var/lib/k0s/pki/admin.conf | tee ~/.k0s/kubeconfig 2. Set the KUBECONFIG environment variable # export KUBECONFIG = \" ${ HOME } /.k0s/kubeconfig\" 3. Monitor cluster startup # kubectl get pods --all-namespaces","title":"Single-Node Cluster"},{"location":"k0s-single-node/#creating-a-single-node-cluster","text":"These instructions outline a quick method for running a local k0s master and worker in a single node. NOTE: This method of running k0s is only recommended for dev, test or POC environments.","title":"Creating a single-node cluster"},{"location":"k0s-single-node/#prepare-dependencies","text":"","title":"Prepare dependencies"},{"location":"k0s-single-node/#start-k0s","text":"","title":"Start k0s"},{"location":"k0s-single-node/#use-kubectl-to-access-k0s","text":"","title":"Use kubectl to access k0s"},{"location":"manifests/","text":"Manifest deployer # k0s embeds a manifest deployer on controllers which provides an easy way to deploy manifests automatically. By default k0s reads all manifests under ${DATADIR}/manifests (default: /var/lib/k0s/manifests ) and ensures their state matches the cluster state. When you remove a manifest file, k0s will automatically prune all the resources associated with it. Each directory that is a direct descendant of ${DATADIR}/manifests is considered as its own \"stack\", but nested directories will be excluded from the stack mechanism. Note: k0s uses this mechanism for some of its internal in-cluster components and other resources. Make sure you only touch the manifests not managed by k0s. Future # We may in the future support nested directories, but those will not be considered stacks , but rather sub-resources of a parent stacks. Stacks are exclusively top-level.","title":"Deploying Manifests"},{"location":"manifests/#manifest-deployer","text":"k0s embeds a manifest deployer on controllers which provides an easy way to deploy manifests automatically. By default k0s reads all manifests under ${DATADIR}/manifests (default: /var/lib/k0s/manifests ) and ensures their state matches the cluster state. When you remove a manifest file, k0s will automatically prune all the resources associated with it. Each directory that is a direct descendant of ${DATADIR}/manifests is considered as its own \"stack\", but nested directories will be excluded from the stack mechanism. Note: k0s uses this mechanism for some of its internal in-cluster components and other resources. Make sure you only touch the manifests not managed by k0s.","title":"Manifest deployer"},{"location":"manifests/#future","text":"We may in the future support nested directories, but those will not be considered stacks , but rather sub-resources of a parent stacks. Stacks are exclusively top-level.","title":"Future"},{"location":"networking/","text":"Networking # In-cluster networking # k0s uses Calico as the default, built-in network provider. Calico is a container networking solution making use of layer 3 to route packets to pods. It supports for example pod specific network policies helping to secure kubernetes clusters in demanding use cases. Calico uses vxlan overlay network by default. Also ipip (IP-in-IP) is supported by configuration. When deploying k0s with the default settings, all pods on a node can communicate with all pods on all nodes. No configuration changes are needed to get started. It is possible for a user to opt-out of Calico and k0s managing the network. Users are able to utilize any network plugin following the CNI specification. By configuring custom as the network provider (in k0s.yaml) it is expected that the user sets up the networking. This can be achieved e.g. by pushing network provider manifests into /var/lib/k0s/manifests from where k0s controllers will pick them up and deploy into the cluster. More on the automatic manifest handling here . Controller(s) - Worker communication # As one of the goals of k0s is to allow deployment of totally isolated control plane we cannot rely on the fact that there is an IP route between controller nodes and the pod overlay network. To enable this communication path, which is mandated by conformance tests, we use Konnectivity service to proxy the traffic from API server (control plane) into the worker nodes. Possible firewalls should be configured with outbound access so that Konnectivity agents running on the worker nodes can establish the connection. This ensures that we can always fulfill all the Kubernetes API functionalities, but still operate the control plane in total isolation from the workers. Needed open ports & protocols # Protocol Port Service Direction Notes TCP 2380 etcd peers controller <-> controller TCP 6443 kube-apiserver Worker, CLI => controller authenticated kube API using kube TLS client certs, ServiceAccount tokens with RBAC UDP 4789 Calico worker <-> worker Calico VXLAN overlay TCP 10250 kubelet Master, Worker => Host * authenticated kubelet API for the master node kube-apiserver (and heapster / metrics-server addons) using TLS client certs TCP 9443 k0s-api controller <-> controller k0s controller join API, TLS with token auth TCP 8132,8133 konnectivity server worker <-> controller konnectivity is used as \"reverse\" tunnel between kube-apiserver and worker kubelets","title":"Networking"},{"location":"networking/#networking","text":"","title":"Networking"},{"location":"networking/#in-cluster-networking","text":"k0s uses Calico as the default, built-in network provider. Calico is a container networking solution making use of layer 3 to route packets to pods. It supports for example pod specific network policies helping to secure kubernetes clusters in demanding use cases. Calico uses vxlan overlay network by default. Also ipip (IP-in-IP) is supported by configuration. When deploying k0s with the default settings, all pods on a node can communicate with all pods on all nodes. No configuration changes are needed to get started. It is possible for a user to opt-out of Calico and k0s managing the network. Users are able to utilize any network plugin following the CNI specification. By configuring custom as the network provider (in k0s.yaml) it is expected that the user sets up the networking. This can be achieved e.g. by pushing network provider manifests into /var/lib/k0s/manifests from where k0s controllers will pick them up and deploy into the cluster. More on the automatic manifest handling here .","title":"In-cluster networking"},{"location":"networking/#controllers-worker-communication","text":"As one of the goals of k0s is to allow deployment of totally isolated control plane we cannot rely on the fact that there is an IP route between controller nodes and the pod overlay network. To enable this communication path, which is mandated by conformance tests, we use Konnectivity service to proxy the traffic from API server (control plane) into the worker nodes. Possible firewalls should be configured with outbound access so that Konnectivity agents running on the worker nodes can establish the connection. This ensures that we can always fulfill all the Kubernetes API functionalities, but still operate the control plane in total isolation from the workers.","title":"Controller(s) - Worker communication"},{"location":"networking/#needed-open-ports-protocols","text":"Protocol Port Service Direction Notes TCP 2380 etcd peers controller <-> controller TCP 6443 kube-apiserver Worker, CLI => controller authenticated kube API using kube TLS client certs, ServiceAccount tokens with RBAC UDP 4789 Calico worker <-> worker Calico VXLAN overlay TCP 10250 kubelet Master, Worker => Host * authenticated kubelet API for the master node kube-apiserver (and heapster / metrics-server addons) using TLS client certs TCP 9443 k0s-api controller <-> controller k0s controller join API, TLS with token auth TCP 8132,8133 konnectivity server worker <-> controller konnectivity is used as \"reverse\" tunnel between kube-apiserver and worker kubelets","title":"Needed open ports &amp; protocols"},{"location":"raspberry-pi4/","text":"Creating Raspberry Pi 4 Cluster # This is a highly opinionated example of deploying the K0s distribution of Kubernetes to a cluster comprised of Raspberry Pi 4 Computers with Ubuntu 20.04 LTS as the operating system. Prerequisites # The following tools should be installed on your local workstation to use this example: Kubectl v1.19.4 + Raspberry Pi Imager v1.5 + Walkthrough # In order to deploy K0s on your Raspberry Pi systems we'll follow these steps: Hardware & Operating System Setup Networking Configurations Node Configurations Deploying Kubernetes These steps require a fair amount of pre-requisite knowledge of Linux and assume a basic understanding of the Ubuntu Linux Distribution as well as Kubernetes . If you're feeling out of sorts, consider reading through the Kubernetes Basics Documentation for more context and some less complex exercises to get started with. Hardware & Operating System # Note that this example was developed with Raspberry Pi 4 Model B Computers with 8GB of RAM and 64GB SD Cards. You may have to do some manual editing of the example code and k0s configuration to get things working in your environment if you use lower spec machines. Downloading Ubuntu # When this example was developed the following image was used: Ubuntu Server 20.04.1 LTS RASPI 4 Image It's likely later versions and other versions will work but when in doubt consider using the above image for the nodes. Installing Ubuntu # The Raspberry Pi Foundation provides a convenient imaging tool for writing the Ubuntu image to your nodes' SD cards by selecting the image and the SD card you want to write it to. If all goes well it should be as simple as flashing the SD, inserting it into the Raspberry Pi system, and turning it on and Ubuntu will bootstrap itself. If you run into any trouble however, Ubuntu provides documentation on installing Ubuntu on Raspberry Pi Computers . Note that the default login credentials for the systems once cloud-init finishes bootstrapping the system will be user ubuntu with password ubuntu and you'll be required to change the password on first login. Networking # For this example it's assumed that you have all computers connected to eachother on the same subnet, but the rest is pretty much open ended. Make sure that you review the K0s required ports documentation to ensure that your network and firewall configurations will allow necessary traffic for the cluster. Review the Ubuntu Server Networking Configuration Documentation and ensure that all systems get a static IP address on the network, or that the network is providing a static DHCP lease for the nodes. OpenSSH # Ubuntu Server will deploy and enable OpenSSH by default, but make sure that for whichever user you're going to deploy the cluster with on the build system their SSH Key is copied to each node's root user , or you may have to do additional manual configurations to run the example. Effectively before you start, you should have it configured so that the current user can run: $ ssh root@ ${ HOST } Where ${HOST} is any node and the login will succeed with no further prompts. Setup Nodes # Each node (whether control plane or not) will need some additional setup to prepare for K0s deployment. CGroup Configuration # Ensure that the following packages are installed on each node: $ apt-get install cgroup-lite cgroup-tools cgroupfs-mount Additionally not all Ubuntu images are going to have the memory cgroup enabled in the Kernel by default, one simple way to enable this is by adding it to the Kernel command line. Open the file /boot/firmware/cmdline.txt which is responsible for managing the Kernel parameters, and ensure that the following parameters exist (add them if not): cgroup_enable = cpuset cgroup_enable = memory cgroup_memory = 1 Make sure you reboot each node to ensure the memory cgroup is loaded. Swap (Optional) # While this is technical optional if you don't have the 8GB RAM Raspberry PI for your nodes and instead have the 4GB it can be helpful to enable swap to ease some memory pressure. You can create a swapfile by running the following: fallocate -l 2G /swapfile chmod 0600 /swapfile mkswap /swapfile swapon -a To ensure that the usage of swap is not too agressive, make sure you set the sudo sysctl vm.swappiness=10 (the default is generally higher). Configure this in /etc/sysctl.d/* to be persistent. Lastly to ensure that your swap is mounted after reboots, make sure the following line exists in your /etc/fstab configuration: /swapfile none swap sw 0 0 Kernel Modules # Some important Kernel modules to keep track of are the overlay , nf_conntrack and br_netfilter modules, ensure those are loaded: $ modprobe overlay $ modprobe nf_conntrack $ modprobe br_netfilter Add each of these modules to your /etc/modules-load.d/modules.conf file as well to ensure they persist after reboot. Download K0s # Download a K0s release , for example: $ wget -O /tmp/k0s https://github.com/k0sproject/k0s/releases/download/v0.9.1/k0s-v0.9.1-arm64 $ chmod a+x /tmp/k0s $ sudo mv /tmp/k0s /usr/bin/k0s Now you'll be able to run k0s : $ k0s version v0.9.1 Deploying Kubernetes # Each node is now setup to handle being a control plane node or worker node. Control Plane Node # For this demonstration, we'll use a non-ha control plane with a single node. Systemd Service # Create a systemd service file for k0s : $ cat << EOF > /etc/systemd/system/k0s.service [Unit] Description=k0s - Kubernetes Control Plane & Worker ConditionFileIsExecutable=/usr/bin/k0s After=network.target [Service] EnvironmentFile=/etc/sysconfig/k0s ExecStart=/usr/bin/k0s server KillMode=process Restart=always RestartSec=120 StartLimitBurst=10 StartLimitInterval=5 [Install] WantedBy=multi-user.target EOF Enable and start the service: $ systemctl enable --now k0s Run systemctl status k0s to verify the service status. Worker Tokens # For each worker node that you expect to have, create a join token (and save this for later steps): $ k0s token create --role worker Worker # For any number of worker nodes which you created join tokens for we'll need to deploy a worker service and start it. Systemd Service # Create the join token for the worker: $ mkdir -p /var/lib/k0s/ $ echo ${ TOKEN_CONTENT } > /var/lib/k0s/join-token Where ${TOKEN_CONTENT} is one of the join tokens you created in the control plane setup. Then deploy the systemd service for the worker: $ cat << EOF > /etc/systemd/system/k0s.service [Unit] Description=k0s - Kubernetes Worker ConditionFileIsExecutable=/usr/bin/k0s After=network.target [Service] EnvironmentFile=-/etc/sysconfig/k0s ExecStart=/usr/bin/k0s worker --token-file /var/lib/k0s/join-token KillMode=process Restart=always RestartSec=120 StartLimitBurst=10 StartLimitInterval=5 [Install] WantedBy=multi-user.target EOF Enable and start the service: $ systemctl enable --now k0s Run systemctl status k0s to verify the service status. Connecting To Your Cluster # Now generate a kubeconfig for the cluster and start managing it with kubectl : ssh root@ ${ CONTROL_PLANE_NODE } k0s kubeconfig create --groups \"system:masters\" k0s > config.yaml export KUBECONFIG = $( pwd ) /config.yaml kubectl create clusterrolebinding k0s-admin-binding --clusterrole = admin --user = k0s Where ${CONTROL_PLANE_NODE} is the address of your control plane node. Now the cluster can be accessed and used: $ kubectl get nodes,deployments,pods -A NAME STATUS ROLES AGE VERSION node/k8s-4 Ready <none> 5m9s v1.20.1-k0s1 node/k8s-5 Ready <none> 5m v1.20.1-k0s1 node/k8s-6 Ready <none> 4m45s v1.20.1-k0s1 NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE kube-system deployment.apps/calico-kube-controllers 1 /1 1 1 12m kube-system deployment.apps/coredns 1 /1 1 1 12m NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/calico-kube-controllers-5f6546844f-rjdkz 1 /1 Running 0 12m kube-system pod/calico-node-j475n 1 /1 Running 0 5m9s kube-system pod/calico-node-lnfrf 1 /1 Running 0 4m45s kube-system pod/calico-node-pzp7x 1 /1 Running 0 5m kube-system pod/coredns-5c98d7d4d8-bg9pl 1 /1 Running 0 12m kube-system pod/konnectivity-agent-548hp 1 /1 Running 0 4m45s kube-system pod/konnectivity-agent-66cr8 1 /1 Running 0 4m49s kube-system pod/konnectivity-agent-lxt9z 1 /1 Running 0 4m58s kube-system pod/kube-proxy-ct6bg 1 /1 Running 0 5m kube-system pod/kube-proxy-hg8t2 1 /1 Running 0 4m45s kube-system pod/kube-proxy-vghs9 1 /1 Running 0 5m9s Enjoy!","title":"Creating Raspberry Pi 4 Cluster"},{"location":"raspberry-pi4/#creating-raspberry-pi-4-cluster","text":"This is a highly opinionated example of deploying the K0s distribution of Kubernetes to a cluster comprised of Raspberry Pi 4 Computers with Ubuntu 20.04 LTS as the operating system.","title":"Creating Raspberry Pi 4 Cluster"},{"location":"raspberry-pi4/#prerequisites","text":"The following tools should be installed on your local workstation to use this example: Kubectl v1.19.4 + Raspberry Pi Imager v1.5 +","title":"Prerequisites"},{"location":"raspberry-pi4/#walkthrough","text":"In order to deploy K0s on your Raspberry Pi systems we'll follow these steps: Hardware & Operating System Setup Networking Configurations Node Configurations Deploying Kubernetes These steps require a fair amount of pre-requisite knowledge of Linux and assume a basic understanding of the Ubuntu Linux Distribution as well as Kubernetes . If you're feeling out of sorts, consider reading through the Kubernetes Basics Documentation for more context and some less complex exercises to get started with.","title":"Walkthrough"},{"location":"raspberry-pi4/#hardware-operating-system","text":"Note that this example was developed with Raspberry Pi 4 Model B Computers with 8GB of RAM and 64GB SD Cards. You may have to do some manual editing of the example code and k0s configuration to get things working in your environment if you use lower spec machines.","title":"Hardware &amp; Operating System"},{"location":"raspberry-pi4/#downloading-ubuntu","text":"When this example was developed the following image was used: Ubuntu Server 20.04.1 LTS RASPI 4 Image It's likely later versions and other versions will work but when in doubt consider using the above image for the nodes.","title":"Downloading Ubuntu"},{"location":"raspberry-pi4/#installing-ubuntu","text":"The Raspberry Pi Foundation provides a convenient imaging tool for writing the Ubuntu image to your nodes' SD cards by selecting the image and the SD card you want to write it to. If all goes well it should be as simple as flashing the SD, inserting it into the Raspberry Pi system, and turning it on and Ubuntu will bootstrap itself. If you run into any trouble however, Ubuntu provides documentation on installing Ubuntu on Raspberry Pi Computers . Note that the default login credentials for the systems once cloud-init finishes bootstrapping the system will be user ubuntu with password ubuntu and you'll be required to change the password on first login.","title":"Installing Ubuntu"},{"location":"raspberry-pi4/#networking","text":"For this example it's assumed that you have all computers connected to eachother on the same subnet, but the rest is pretty much open ended. Make sure that you review the K0s required ports documentation to ensure that your network and firewall configurations will allow necessary traffic for the cluster. Review the Ubuntu Server Networking Configuration Documentation and ensure that all systems get a static IP address on the network, or that the network is providing a static DHCP lease for the nodes.","title":"Networking"},{"location":"raspberry-pi4/#openssh","text":"Ubuntu Server will deploy and enable OpenSSH by default, but make sure that for whichever user you're going to deploy the cluster with on the build system their SSH Key is copied to each node's root user , or you may have to do additional manual configurations to run the example. Effectively before you start, you should have it configured so that the current user can run: $ ssh root@ ${ HOST } Where ${HOST} is any node and the login will succeed with no further prompts.","title":"OpenSSH"},{"location":"raspberry-pi4/#setup-nodes","text":"Each node (whether control plane or not) will need some additional setup to prepare for K0s deployment.","title":"Setup Nodes"},{"location":"raspberry-pi4/#cgroup-configuration","text":"Ensure that the following packages are installed on each node: $ apt-get install cgroup-lite cgroup-tools cgroupfs-mount Additionally not all Ubuntu images are going to have the memory cgroup enabled in the Kernel by default, one simple way to enable this is by adding it to the Kernel command line. Open the file /boot/firmware/cmdline.txt which is responsible for managing the Kernel parameters, and ensure that the following parameters exist (add them if not): cgroup_enable = cpuset cgroup_enable = memory cgroup_memory = 1 Make sure you reboot each node to ensure the memory cgroup is loaded.","title":"CGroup Configuration"},{"location":"raspberry-pi4/#swap-optional","text":"While this is technical optional if you don't have the 8GB RAM Raspberry PI for your nodes and instead have the 4GB it can be helpful to enable swap to ease some memory pressure. You can create a swapfile by running the following: fallocate -l 2G /swapfile chmod 0600 /swapfile mkswap /swapfile swapon -a To ensure that the usage of swap is not too agressive, make sure you set the sudo sysctl vm.swappiness=10 (the default is generally higher). Configure this in /etc/sysctl.d/* to be persistent. Lastly to ensure that your swap is mounted after reboots, make sure the following line exists in your /etc/fstab configuration: /swapfile none swap sw 0 0","title":"Swap (Optional)"},{"location":"raspberry-pi4/#kernel-modules","text":"Some important Kernel modules to keep track of are the overlay , nf_conntrack and br_netfilter modules, ensure those are loaded: $ modprobe overlay $ modprobe nf_conntrack $ modprobe br_netfilter Add each of these modules to your /etc/modules-load.d/modules.conf file as well to ensure they persist after reboot.","title":"Kernel Modules"},{"location":"raspberry-pi4/#download-k0s","text":"Download a K0s release , for example: $ wget -O /tmp/k0s https://github.com/k0sproject/k0s/releases/download/v0.9.1/k0s-v0.9.1-arm64 $ chmod a+x /tmp/k0s $ sudo mv /tmp/k0s /usr/bin/k0s Now you'll be able to run k0s : $ k0s version v0.9.1","title":"Download K0s"},{"location":"raspberry-pi4/#deploying-kubernetes","text":"Each node is now setup to handle being a control plane node or worker node.","title":"Deploying Kubernetes"},{"location":"raspberry-pi4/#control-plane-node","text":"For this demonstration, we'll use a non-ha control plane with a single node.","title":"Control Plane Node"},{"location":"raspberry-pi4/#worker","text":"For any number of worker nodes which you created join tokens for we'll need to deploy a worker service and start it.","title":"Worker"},{"location":"raspberry-pi4/#connecting-to-your-cluster","text":"Now generate a kubeconfig for the cluster and start managing it with kubectl : ssh root@ ${ CONTROL_PLANE_NODE } k0s kubeconfig create --groups \"system:masters\" k0s > config.yaml export KUBECONFIG = $( pwd ) /config.yaml kubectl create clusterrolebinding k0s-admin-binding --clusterrole = admin --user = k0s Where ${CONTROL_PLANE_NODE} is the address of your control plane node. Now the cluster can be accessed and used: $ kubectl get nodes,deployments,pods -A NAME STATUS ROLES AGE VERSION node/k8s-4 Ready <none> 5m9s v1.20.1-k0s1 node/k8s-5 Ready <none> 5m v1.20.1-k0s1 node/k8s-6 Ready <none> 4m45s v1.20.1-k0s1 NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE kube-system deployment.apps/calico-kube-controllers 1 /1 1 1 12m kube-system deployment.apps/coredns 1 /1 1 1 12m NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/calico-kube-controllers-5f6546844f-rjdkz 1 /1 Running 0 12m kube-system pod/calico-node-j475n 1 /1 Running 0 5m9s kube-system pod/calico-node-lnfrf 1 /1 Running 0 4m45s kube-system pod/calico-node-pzp7x 1 /1 Running 0 5m kube-system pod/coredns-5c98d7d4d8-bg9pl 1 /1 Running 0 12m kube-system pod/konnectivity-agent-548hp 1 /1 Running 0 4m45s kube-system pod/konnectivity-agent-66cr8 1 /1 Running 0 4m49s kube-system pod/konnectivity-agent-lxt9z 1 /1 Running 0 4m58s kube-system pod/kube-proxy-ct6bg 1 /1 Running 0 5m kube-system pod/kube-proxy-hg8t2 1 /1 Running 0 4m45s kube-system pod/kube-proxy-vghs9 1 /1 Running 0 5m9s Enjoy!","title":"Connecting To Your Cluster"},{"location":"troubleshooting/","text":"Troubleshooting # There are few common cases we've seen where k0s fails to run properly. CoreDNS in crashloop # The most common case we've encountered so far has been CoreDNS getting into crashloop on the node(s). With kubectl you see something like this: $ kubectl get pod --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-5f6546844f-25px6 1 /1 Running 0 167m kube-system calico-node-fwjx5 1 /1 Running 0 164m kube-system calico-node-t4tx5 1 /1 Running 0 164m kube-system calico-node-whwsg 1 /1 Running 0 164m kube-system coredns-5c98d7d4d8-tfs4q 1 /1 Error 17 167m kube-system konnectivity-agent-9jkfd 1 /1 Running 0 164m kube-system konnectivity-agent-bvhdb 1 /1 Running 0 164m kube-system konnectivity-agent-r6mzj 1 /1 Running 0 164m kube-system kube-proxy-kr2r9 1 /1 Running 0 164m kube-system kube-proxy-tbljr 1 /1 Running 0 164m kube-system kube-proxy-xbw7p 1 /1 Running 0 164m kube-system metrics-server-7d4bcb75dd-pqkrs 1 /1 Running 0 167m When you check the logs, it'll show something like this: $ kubectl -n kube-system logs coredns-5c98d7d4d8-tfs4q plugin/loop: Loop (127.0.0.1:55953 -> :1053) detected for zone \".\", see https://coredns.io/plugins/loop#troubleshooting. Query: \"HINFO 4547991504243258144.3688648895315093531.\" This is most often caused by systemd-resolved stub (or something similar) running locally and CoreDNS detects a possible loop with DNS queries. The easiest but most crude way to workaround is to disable the systemd-resolved stub and revert the hosts /etc/resolv.conf to original Read more at CoreDNS troubleshooting docs . k0s server fails on ARM boxes # In the logs you probably see ETCD not starting up properly. Etcd is not fully supported on ARM architecture, thus you need to run k0s server and thus also etcd process with env ETCD_UNSUPPORTED_ARCH=arm64 . As Etcd is not fully supported on ARM architecture it also means that k0s controlplane with etcd itself is not fully supported on ARM either. Pods pending when using cloud providers # Once we enable cloud provider support on kubelet on worker nodes, kubelet will automatically add a taint node.cloudprovider.kubernetes.io/uninitialized for the node. This tain will prevent normal workloads to be scheduled on the node until the cloud provider controller actually runs second initialization on the node and removes the taint. This means that these nodes are not available for scheduling until the cloud provider controller is actually successfully running on the cluster. For troubleshooting your specific cloud provider see its documentation. k0s not working with read only /usr # By default k0s does not run on nodes where /usr is read only. This can be fixed by changing the default path for volumePluginDir in your k0s config. You will need to change to values, one for the kubelet itself, and one for Calico. Here is a snippet of an example config with the default values changed: spec : controllerManager : extraArgs : flex-volume-plugin-dir : \"/etc/kubernetes/kubelet-plugins/volume/exec\" network : calico : flexVolumeDriverPath : /etc/k0s/kubelet-plugins/volume/exec/nodeagent~uds workerProfiles : - name : coreos values : volumePluginDir : /etc/k0s/kubelet-plugins/volume/exec/ With this config you can start your server as usual. Any workers will need to be started with k0s worker --profile coreos [TOKEN] Profiling # We drop any debug related information and symbols from the compiled binary by utilzing -w -s linker flags. To keep those symbols use DEBUG env variable: $ DEBUG=true make k0s CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -ldflags=\" -X github.com/k0sproject/k0s/pkg/build.Version=v0.9.0-7-g97e5bac -X \\\"github.com/k0sproject/k0s/pkg/build.EulaNotice=\\\" -X github.com/k0sproject/k0s/pkg/telemetry.segmentToken=\" \\ -o k0s.code main.go Any value not equal to the \"false\" would work. To add custom linker flags use LDFLAGS variable. $ LD_FLAGS=\"--custom-flag=value\" make k0s CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -ldflags=\"--custom-flag=value -X github.com/k0sproject/k0s/pkg/build.Version=v0.9.0-7-g97e5bac -X \\\"github.com/k0sproject/k0s/pkg/build.EulaNotice=\\\" -X github.com/k0sproject/k0s/pkg/telemetry.segmentToken=\" \\ -o k0s.code main.go","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"There are few common cases we've seen where k0s fails to run properly.","title":"Troubleshooting"},{"location":"troubleshooting/#coredns-in-crashloop","text":"The most common case we've encountered so far has been CoreDNS getting into crashloop on the node(s). With kubectl you see something like this: $ kubectl get pod --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-5f6546844f-25px6 1 /1 Running 0 167m kube-system calico-node-fwjx5 1 /1 Running 0 164m kube-system calico-node-t4tx5 1 /1 Running 0 164m kube-system calico-node-whwsg 1 /1 Running 0 164m kube-system coredns-5c98d7d4d8-tfs4q 1 /1 Error 17 167m kube-system konnectivity-agent-9jkfd 1 /1 Running 0 164m kube-system konnectivity-agent-bvhdb 1 /1 Running 0 164m kube-system konnectivity-agent-r6mzj 1 /1 Running 0 164m kube-system kube-proxy-kr2r9 1 /1 Running 0 164m kube-system kube-proxy-tbljr 1 /1 Running 0 164m kube-system kube-proxy-xbw7p 1 /1 Running 0 164m kube-system metrics-server-7d4bcb75dd-pqkrs 1 /1 Running 0 167m When you check the logs, it'll show something like this: $ kubectl -n kube-system logs coredns-5c98d7d4d8-tfs4q plugin/loop: Loop (127.0.0.1:55953 -> :1053) detected for zone \".\", see https://coredns.io/plugins/loop#troubleshooting. Query: \"HINFO 4547991504243258144.3688648895315093531.\" This is most often caused by systemd-resolved stub (or something similar) running locally and CoreDNS detects a possible loop with DNS queries. The easiest but most crude way to workaround is to disable the systemd-resolved stub and revert the hosts /etc/resolv.conf to original Read more at CoreDNS troubleshooting docs .","title":"CoreDNS in crashloop"},{"location":"troubleshooting/#k0s-server-fails-on-arm-boxes","text":"In the logs you probably see ETCD not starting up properly. Etcd is not fully supported on ARM architecture, thus you need to run k0s server and thus also etcd process with env ETCD_UNSUPPORTED_ARCH=arm64 . As Etcd is not fully supported on ARM architecture it also means that k0s controlplane with etcd itself is not fully supported on ARM either.","title":"k0s server fails on ARM boxes"},{"location":"troubleshooting/#pods-pending-when-using-cloud-providers","text":"Once we enable cloud provider support on kubelet on worker nodes, kubelet will automatically add a taint node.cloudprovider.kubernetes.io/uninitialized for the node. This tain will prevent normal workloads to be scheduled on the node until the cloud provider controller actually runs second initialization on the node and removes the taint. This means that these nodes are not available for scheduling until the cloud provider controller is actually successfully running on the cluster. For troubleshooting your specific cloud provider see its documentation.","title":"Pods pending when using cloud providers"},{"location":"troubleshooting/#k0s-not-working-with-read-only-usr","text":"By default k0s does not run on nodes where /usr is read only. This can be fixed by changing the default path for volumePluginDir in your k0s config. You will need to change to values, one for the kubelet itself, and one for Calico. Here is a snippet of an example config with the default values changed: spec : controllerManager : extraArgs : flex-volume-plugin-dir : \"/etc/kubernetes/kubelet-plugins/volume/exec\" network : calico : flexVolumeDriverPath : /etc/k0s/kubelet-plugins/volume/exec/nodeagent~uds workerProfiles : - name : coreos values : volumePluginDir : /etc/k0s/kubelet-plugins/volume/exec/ With this config you can start your server as usual. Any workers will need to be started with k0s worker --profile coreos [TOKEN]","title":"k0s not working with read only /usr"},{"location":"troubleshooting/#profiling","text":"We drop any debug related information and symbols from the compiled binary by utilzing -w -s linker flags. To keep those symbols use DEBUG env variable: $ DEBUG=true make k0s CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -ldflags=\" -X github.com/k0sproject/k0s/pkg/build.Version=v0.9.0-7-g97e5bac -X \\\"github.com/k0sproject/k0s/pkg/build.EulaNotice=\\\" -X github.com/k0sproject/k0s/pkg/telemetry.segmentToken=\" \\ -o k0s.code main.go Any value not equal to the \"false\" would work. To add custom linker flags use LDFLAGS variable. $ LD_FLAGS=\"--custom-flag=value\" make k0s CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -ldflags=\"--custom-flag=value -X github.com/k0sproject/k0s/pkg/build.Version=v0.9.0-7-g97e5bac -X \\\"github.com/k0sproject/k0s/pkg/build.EulaNotice=\\\" -X github.com/k0sproject/k0s/pkg/telemetry.segmentToken=\" \\ -o k0s.code main.go","title":"Profiling"},{"location":"cli/","text":"k0s # k0s - Zero Friction Kubernetes Synopsis # k0s - The zero friction Kubernetes - https://k0sproject.io This software is built and distributed by Mirantis, Inc., and is subject to EULA https://k0sproject.io/licenses/eula Options # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -h, --help help for k0s -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) SEE ALSO # k0s api - Run the controller api k0s completion - Generate completion script k0s default-config - Output the default k0s configuration yaml to stdout k0s docs - Generate Markdown docs for the k0s binary k0s etcd - Manage etcd cluster k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo) k0s kubeconfig - Create a kubeconfig file for a specified user k0s server - Run server k0s status - Helper command for get general information about k0s k0s token - Manage join tokens k0s validate - Helper command for validating the config file k0s version - Print the k0s version k0s worker - Run worker","title":"Index"},{"location":"cli/#k0s","text":"k0s - Zero Friction Kubernetes","title":"k0s"},{"location":"cli/#synopsis","text":"k0s - The zero friction Kubernetes - https://k0sproject.io This software is built and distributed by Mirantis, Inc., and is subject to EULA https://k0sproject.io/licenses/eula","title":"Synopsis"},{"location":"cli/#options","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -h, --help help for k0s -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info])","title":"Options"},{"location":"cli/#see-also","text":"k0s api - Run the controller api k0s completion - Generate completion script k0s default-config - Output the default k0s configuration yaml to stdout k0s docs - Generate Markdown docs for the k0s binary k0s etcd - Manage etcd cluster k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo) k0s kubeconfig - Create a kubeconfig file for a specified user k0s server - Run server k0s status - Helper command for get general information about k0s k0s token - Manage join tokens k0s validate - Helper command for validating the config file k0s version - Print the k0s version k0s worker - Run worker","title":"SEE ALSO"},{"location":"cli/k0s/","text":"k0s # k0s - Zero Friction Kubernetes Synopsis # k0s - The zero friction Kubernetes - https://k0sproject.io This software is built and distributed by Mirantis, Inc., and is subject to EULA https://k0sproject.io/licenses/eula Options # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -h, --help help for k0s -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) SEE ALSO # k0s api - Run the controller api k0s completion - Generate completion script k0s default-config - Output the default k0s configuration yaml to stdout k0s docs - Generate Markdown docs for the k0s binary k0s etcd - Manage etcd cluster k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo) k0s kubeconfig - Create a kubeconfig file for a specified user k0s server - Run server k0s status - Helper command for get general information about k0s k0s token - Manage join tokens k0s validate - Helper command for validating the config file k0s version - Print the k0s version k0s worker - Run worker","title":"K0s"},{"location":"cli/k0s/#k0s","text":"k0s - Zero Friction Kubernetes","title":"k0s"},{"location":"cli/k0s/#synopsis","text":"k0s - The zero friction Kubernetes - https://k0sproject.io This software is built and distributed by Mirantis, Inc., and is subject to EULA https://k0sproject.io/licenses/eula","title":"Synopsis"},{"location":"cli/k0s/#options","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -h, --help help for k0s -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info])","title":"Options"},{"location":"cli/k0s/#see-also","text":"k0s api - Run the controller api k0s completion - Generate completion script k0s default-config - Output the default k0s configuration yaml to stdout k0s docs - Generate Markdown docs for the k0s binary k0s etcd - Manage etcd cluster k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo) k0s kubeconfig - Create a kubeconfig file for a specified user k0s server - Run server k0s status - Helper command for get general information about k0s k0s token - Manage join tokens k0s validate - Helper command for validating the config file k0s version - Print the k0s version k0s worker - Run worker","title":"SEE ALSO"},{"location":"cli/k0s_api/","text":"k0s api # Run the controller api k0s api [flags] Options # -h, --help help for api Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s api"},{"location":"cli/k0s_api/#k0s-api","text":"Run the controller api k0s api [flags]","title":"k0s api"},{"location":"cli/k0s_api/#options","text":"-h, --help help for api","title":"Options"},{"location":"cli/k0s_api/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info])","title":"Options inherited from parent commands"},{"location":"cli/k0s_api/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_completion/","text":"k0s completion # Generate completion script Synopsis # To load completions: Bash: $ source <(k0s completion bash) To load completions for each session, execute once: # $ k0s completion bash > /etc/bash_completion.d/k0s Zsh: If shell completion is not already enabled in your environment you will need # to enable it. You can execute the following once: # $ echo \"autoload -U compinit; compinit\" >> ~/.zshrc To load completions for each session, execute once: # $ k0s completion zsh > \"${fpath[1]}/_k0s\" You will need to start a new shell for this setup to take effect. # Fish: $ k0s completion fish | source To load completions for each session, execute once: # $ k0s completion fish > ~/.config/fish/completions/k0s.fish k0s completion [bash|zsh|fish|powershell] Options # -h, --help help for completion Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s completion"},{"location":"cli/k0s_completion/#k0s-completion","text":"Generate completion script","title":"k0s completion"},{"location":"cli/k0s_completion/#synopsis","text":"To load completions: Bash: $ source <(k0s completion bash)","title":"Synopsis"},{"location":"cli/k0s_completion/#to-load-completions-for-each-session-execute-once","text":"$ k0s completion bash > /etc/bash_completion.d/k0s Zsh:","title":"To load completions for each session, execute once:"},{"location":"cli/k0s_completion/#if-shell-completion-is-not-already-enabled-in-your-environment-you-will-need","text":"","title":"If shell completion is not already enabled in your environment you will need"},{"location":"cli/k0s_completion/#to-enable-it-you-can-execute-the-following-once","text":"$ echo \"autoload -U compinit; compinit\" >> ~/.zshrc","title":"to enable it.  You can execute the following once:"},{"location":"cli/k0s_completion/#to-load-completions-for-each-session-execute-once_1","text":"$ k0s completion zsh > \"${fpath[1]}/_k0s\"","title":"To load completions for each session, execute once:"},{"location":"cli/k0s_completion/#you-will-need-to-start-a-new-shell-for-this-setup-to-take-effect","text":"Fish: $ k0s completion fish | source","title":"You will need to start a new shell for this setup to take effect."},{"location":"cli/k0s_completion/#to-load-completions-for-each-session-execute-once_2","text":"$ k0s completion fish > ~/.config/fish/completions/k0s.fish k0s completion [bash|zsh|fish|powershell]","title":"To load completions for each session, execute once:"},{"location":"cli/k0s_completion/#options","text":"-h, --help help for completion","title":"Options"},{"location":"cli/k0s_completion/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info])","title":"Options inherited from parent commands"},{"location":"cli/k0s_completion/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_default-config/","text":"k0s default-config # Output the default k0s configuration yaml to stdout k0s default-config [flags] Options # -h, --help help for default-config Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s default config"},{"location":"cli/k0s_default-config/#k0s-default-config","text":"Output the default k0s configuration yaml to stdout k0s default-config [flags]","title":"k0s default-config"},{"location":"cli/k0s_default-config/#options","text":"-h, --help help for default-config","title":"Options"},{"location":"cli/k0s_default-config/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info])","title":"Options inherited from parent commands"},{"location":"cli/k0s_default-config/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_docs/","text":"k0s docs # Generate Markdown docs for the k0s binary k0s docs [flags] Options # -h, --help help for docs Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s docs"},{"location":"cli/k0s_docs/#k0s-docs","text":"Generate Markdown docs for the k0s binary k0s docs [flags]","title":"k0s docs"},{"location":"cli/k0s_docs/#options","text":"-h, --help help for docs","title":"Options"},{"location":"cli/k0s_docs/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info])","title":"Options inherited from parent commands"},{"location":"cli/k0s_docs/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_etcd/","text":"k0s etcd # Manage etcd cluster Options # -h, --help help for etcd Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes k0s etcd leave - Sign off a given etc node from etcd cluster k0s etcd member-list - Returns etcd cluster members list","title":"K0s etcd"},{"location":"cli/k0s_etcd/#k0s-etcd","text":"Manage etcd cluster","title":"k0s etcd"},{"location":"cli/k0s_etcd/#options","text":"-h, --help help for etcd","title":"Options"},{"location":"cli/k0s_etcd/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info])","title":"Options inherited from parent commands"},{"location":"cli/k0s_etcd/#see-also","text":"k0s - k0s - Zero Friction Kubernetes k0s etcd leave - Sign off a given etc node from etcd cluster k0s etcd member-list - Returns etcd cluster members list","title":"SEE ALSO"},{"location":"cli/k0s_etcd_leave/","text":"k0s etcd leave # Sign off a given etc node from etcd cluster k0s etcd leave [flags] Options # -h, --help help for leave --peer-address string etcd peer address Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) SEE ALSO # k0s etcd - Manage etcd cluster","title":"K0s etcd leave"},{"location":"cli/k0s_etcd_leave/#k0s-etcd-leave","text":"Sign off a given etc node from etcd cluster k0s etcd leave [flags]","title":"k0s etcd leave"},{"location":"cli/k0s_etcd_leave/#options","text":"-h, --help help for leave --peer-address string etcd peer address","title":"Options"},{"location":"cli/k0s_etcd_leave/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info])","title":"Options inherited from parent commands"},{"location":"cli/k0s_etcd_leave/#see-also","text":"k0s etcd - Manage etcd cluster","title":"SEE ALSO"},{"location":"cli/k0s_etcd_member-list/","text":"k0s etcd member-list # Returns etcd cluster members list k0s etcd member-list [flags] Options # -h, --help help for member-list Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) SEE ALSO # k0s etcd - Manage etcd cluster","title":"K0s etcd member list"},{"location":"cli/k0s_etcd_member-list/#k0s-etcd-member-list","text":"Returns etcd cluster members list k0s etcd member-list [flags]","title":"k0s etcd member-list"},{"location":"cli/k0s_etcd_member-list/#options","text":"-h, --help help for member-list","title":"Options"},{"location":"cli/k0s_etcd_member-list/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info])","title":"Options inherited from parent commands"},{"location":"cli/k0s_etcd_member-list/#see-also","text":"k0s etcd - Manage etcd cluster","title":"SEE ALSO"},{"location":"cli/k0s_install/","text":"k0s install # Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo) Options # -h, --help help for install Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes k0s install server - Helper command for setting up k0s as server node on a brand-new system. Must be run as root (or with sudo) k0s install worker - Helper command for setting up k0s as a worker node on a brand-new system. Must be run as root (or with sudo)","title":"K0s install"},{"location":"cli/k0s_install/#k0s-install","text":"Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo)","title":"k0s install"},{"location":"cli/k0s_install/#options","text":"-h, --help help for install","title":"Options"},{"location":"cli/k0s_install/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info])","title":"Options inherited from parent commands"},{"location":"cli/k0s_install/#see-also","text":"k0s - k0s - Zero Friction Kubernetes k0s install server - Helper command for setting up k0s as server node on a brand-new system. Must be run as root (or with sudo) k0s install worker - Helper command for setting up k0s as a worker node on a brand-new system. Must be run as root (or with sudo)","title":"SEE ALSO"},{"location":"cli/k0s_install_server/","text":"k0s install server # Helper command for setting up k0s as server node on a brand-new system. Must be run as root (or with sudo) k0s install server [flags] Examples # All default values of server command will be passed to the service stub unless overriden. With server subcommand you can setup a single node cluster by running: k0s install server --enable-worker Options # --cri-socket string contrainer runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket] --enable-worker enable worker (default false) -h, --help help for server --profile string worker profile to use on the node (default \"default\") --token-file string Path to the file containing join-token. Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) SEE ALSO # k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo)","title":"K0s install server"},{"location":"cli/k0s_install_server/#k0s-install-server","text":"Helper command for setting up k0s as server node on a brand-new system. Must be run as root (or with sudo) k0s install server [flags]","title":"k0s install server"},{"location":"cli/k0s_install_server/#examples","text":"All default values of server command will be passed to the service stub unless overriden. With server subcommand you can setup a single node cluster by running: k0s install server --enable-worker","title":"Examples"},{"location":"cli/k0s_install_server/#options","text":"--cri-socket string contrainer runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket] --enable-worker enable worker (default false) -h, --help help for server --profile string worker profile to use on the node (default \"default\") --token-file string Path to the file containing join-token.","title":"Options"},{"location":"cli/k0s_install_server/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info])","title":"Options inherited from parent commands"},{"location":"cli/k0s_install_server/#see-also","text":"k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo)","title":"SEE ALSO"},{"location":"cli/k0s_install_worker/","text":"k0s install worker # Helper command for setting up k0s as a worker node on a brand-new system. Must be run as root (or with sudo) k0s install worker [flags] Examples # Worker subcommand allows you to pass in all available worker parameters. All default values of worker command will be passed to the service stub unless overriden. Windows flags like \"--api-server\", \"--cidr-range\" and \"--cluster-dns\" will be ignored since install command doesn't yet support Windows services Options # --api-server string HACK: api-server for the windows worker node --cidr-range string HACK: cidr range for the windows worker node (default \"10.96.0.0/12\") --cluster-dns string HACK: cluster dns for the windows worker node (default \"10.96.0.10\") --cri-socket string contrainer runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket] --enable-cloud-provider Whether or not to enable cloud provider support in kubelet -h, --help help for worker --profile string worker profile to use on the node (default \"default\") --token-file string Path to the file containing token. Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) SEE ALSO # k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo)","title":"K0s install worker"},{"location":"cli/k0s_install_worker/#k0s-install-worker","text":"Helper command for setting up k0s as a worker node on a brand-new system. Must be run as root (or with sudo) k0s install worker [flags]","title":"k0s install worker"},{"location":"cli/k0s_install_worker/#examples","text":"Worker subcommand allows you to pass in all available worker parameters. All default values of worker command will be passed to the service stub unless overriden. Windows flags like \"--api-server\", \"--cidr-range\" and \"--cluster-dns\" will be ignored since install command doesn't yet support Windows services","title":"Examples"},{"location":"cli/k0s_install_worker/#options","text":"--api-server string HACK: api-server for the windows worker node --cidr-range string HACK: cidr range for the windows worker node (default \"10.96.0.0/12\") --cluster-dns string HACK: cluster dns for the windows worker node (default \"10.96.0.10\") --cri-socket string contrainer runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket] --enable-cloud-provider Whether or not to enable cloud provider support in kubelet -h, --help help for worker --profile string worker profile to use on the node (default \"default\") --token-file string Path to the file containing token.","title":"Options"},{"location":"cli/k0s_install_worker/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info])","title":"Options inherited from parent commands"},{"location":"cli/k0s_install_worker/#see-also","text":"k0s install - Helper command for setting up k0s on a brand-new system. Must be run as root (or with sudo)","title":"SEE ALSO"},{"location":"cli/k0s_kubeconfig/","text":"k0s kubeconfig # Create a kubeconfig file for a specified user k0s kubeconfig [command] [flags] Options # -h, --help help for kubeconfig Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes k0s kubeconfig admin - Display Admin's Kubeconfig file k0s kubeconfig create - Create a kubeconfig for a user","title":"K0s kubeconfig"},{"location":"cli/k0s_kubeconfig/#k0s-kubeconfig","text":"Create a kubeconfig file for a specified user k0s kubeconfig [command] [flags]","title":"k0s kubeconfig"},{"location":"cli/k0s_kubeconfig/#options","text":"-h, --help help for kubeconfig","title":"Options"},{"location":"cli/k0s_kubeconfig/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info])","title":"Options inherited from parent commands"},{"location":"cli/k0s_kubeconfig/#see-also","text":"k0s - k0s - Zero Friction Kubernetes k0s kubeconfig admin - Display Admin's Kubeconfig file k0s kubeconfig create - Create a kubeconfig for a user","title":"SEE ALSO"},{"location":"cli/k0s_kubeconfig_admin/","text":"k0s kubeconfig admin # Display Admin's Kubeconfig file Synopsis # Print kubeconfig for the Admin user to stdout k0s kubeconfig admin [command] [flags] Examples # $ k0s kubeconfig admin > ~/.kube/config $ export KUBECONFIG=~/.kube/config $ kubectl get nodes Options # -h, --help help for admin Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) SEE ALSO # k0s kubeconfig - Create a kubeconfig file for a specified user","title":"K0s kubeconfig admin"},{"location":"cli/k0s_kubeconfig_admin/#k0s-kubeconfig-admin","text":"Display Admin's Kubeconfig file","title":"k0s kubeconfig admin"},{"location":"cli/k0s_kubeconfig_admin/#synopsis","text":"Print kubeconfig for the Admin user to stdout k0s kubeconfig admin [command] [flags]","title":"Synopsis"},{"location":"cli/k0s_kubeconfig_admin/#examples","text":"$ k0s kubeconfig admin > ~/.kube/config $ export KUBECONFIG=~/.kube/config $ kubectl get nodes","title":"Examples"},{"location":"cli/k0s_kubeconfig_admin/#options","text":"-h, --help help for admin","title":"Options"},{"location":"cli/k0s_kubeconfig_admin/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info])","title":"Options inherited from parent commands"},{"location":"cli/k0s_kubeconfig_admin/#see-also","text":"k0s kubeconfig - Create a kubeconfig file for a specified user","title":"SEE ALSO"},{"location":"cli/k0s_kubeconfig_create/","text":"k0s kubeconfig create # Create a kubeconfig for a user Synopsis # Create a kubeconfig with a signed certificate and public key for a given user (and optionally user groups) Note: A certificate once signed cannot be revoked for a particular user k0s kubeconfig create [username] [flags] Examples # Command to create a kubeconfig for a user: CLI argument: $ k0s kubeconfig create [username] optionally add groups: $ k0s kubeconfig create [username] --groups [groups] Options # --groups string Specify groups -h, --help help for create Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) SEE ALSO # k0s kubeconfig - Create a kubeconfig file for a specified user","title":"K0s kubeconfig create"},{"location":"cli/k0s_kubeconfig_create/#k0s-kubeconfig-create","text":"Create a kubeconfig for a user","title":"k0s kubeconfig create"},{"location":"cli/k0s_kubeconfig_create/#synopsis","text":"Create a kubeconfig with a signed certificate and public key for a given user (and optionally user groups) Note: A certificate once signed cannot be revoked for a particular user k0s kubeconfig create [username] [flags]","title":"Synopsis"},{"location":"cli/k0s_kubeconfig_create/#examples","text":"Command to create a kubeconfig for a user: CLI argument: $ k0s kubeconfig create [username] optionally add groups: $ k0s kubeconfig create [username] --groups [groups]","title":"Examples"},{"location":"cli/k0s_kubeconfig_create/#options","text":"--groups string Specify groups -h, --help help for create","title":"Options"},{"location":"cli/k0s_kubeconfig_create/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info])","title":"Options inherited from parent commands"},{"location":"cli/k0s_kubeconfig_create/#see-also","text":"k0s kubeconfig - Create a kubeconfig file for a specified user","title":"SEE ALSO"},{"location":"cli/k0s_server/","text":"k0s server # Run server k0s server [join-token] [flags] Examples # Command to associate master nodes: CLI argument: $ k0s server [join-token] or CLI flag: $ k0s server --token-file [path_to_file] Note: Token can be passed either as a CLI argument or as a flag Options # --cri-socket string contrainer runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket] --enable-worker enable worker (default false) -h, --help help for server --profile string worker profile to use on the node (default \"default\") --token-file string Path to the file containing join-token. Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s server"},{"location":"cli/k0s_server/#k0s-server","text":"Run server k0s server [join-token] [flags]","title":"k0s server"},{"location":"cli/k0s_server/#examples","text":"Command to associate master nodes: CLI argument: $ k0s server [join-token] or CLI flag: $ k0s server --token-file [path_to_file] Note: Token can be passed either as a CLI argument or as a flag","title":"Examples"},{"location":"cli/k0s_server/#options","text":"--cri-socket string contrainer runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket] --enable-worker enable worker (default false) -h, --help help for server --profile string worker profile to use on the node (default \"default\") --token-file string Path to the file containing join-token.","title":"Options"},{"location":"cli/k0s_server/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info])","title":"Options inherited from parent commands"},{"location":"cli/k0s_server/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_status/","text":"k0s status # Helper command for get general information about k0s k0s status [flags] Examples # The command will return information about system init, PID, k0s role, kubeconfig and similar. Options # -h, --help help for status -o, --out string sets type of out put to json or yaml Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s status"},{"location":"cli/k0s_status/#k0s-status","text":"Helper command for get general information about k0s k0s status [flags]","title":"k0s status"},{"location":"cli/k0s_status/#examples","text":"The command will return information about system init, PID, k0s role, kubeconfig and similar.","title":"Examples"},{"location":"cli/k0s_status/#options","text":"-h, --help help for status -o, --out string sets type of out put to json or yaml","title":"Options"},{"location":"cli/k0s_status/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info])","title":"Options inherited from parent commands"},{"location":"cli/k0s_status/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_token/","text":"k0s token # Manage join tokens k0s token [flags] Options # -h, --help help for token --kubeconfig string path to kubeconfig file [$KUBECONFIG] Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes k0s token create - Create join token","title":"K0s token"},{"location":"cli/k0s_token/#k0s-token","text":"Manage join tokens k0s token [flags]","title":"k0s token"},{"location":"cli/k0s_token/#options","text":"-h, --help help for token --kubeconfig string path to kubeconfig file [$KUBECONFIG]","title":"Options"},{"location":"cli/k0s_token/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info])","title":"Options inherited from parent commands"},{"location":"cli/k0s_token/#see-also","text":"k0s - k0s - Zero Friction Kubernetes k0s token create - Create join token","title":"SEE ALSO"},{"location":"cli/k0s_token_create/","text":"k0s token create # Create join token k0s token create [flags] Options # --expiry string set duration time for token (default \"0\") -h, --help help for create --role string Either worker or controller (default \"worker\") --wait wait forever (default false) Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) SEE ALSO # k0s token - Manage join tokens","title":"K0s token create"},{"location":"cli/k0s_token_create/#k0s-token-create","text":"Create join token k0s token create [flags]","title":"k0s token create"},{"location":"cli/k0s_token_create/#options","text":"--expiry string set duration time for token (default \"0\") -h, --help help for create --role string Either worker or controller (default \"worker\") --wait wait forever (default false)","title":"Options"},{"location":"cli/k0s_token_create/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info])","title":"Options inherited from parent commands"},{"location":"cli/k0s_token_create/#see-also","text":"k0s token - Manage join tokens","title":"SEE ALSO"},{"location":"cli/k0s_validate/","text":"k0s validate # Helper command for validating the config file Options # -h, --help help for validate Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes k0s validate config - Helper command for validating the config file","title":"K0s validate"},{"location":"cli/k0s_validate/#k0s-validate","text":"Helper command for validating the config file","title":"k0s validate"},{"location":"cli/k0s_validate/#options","text":"-h, --help help for validate","title":"Options"},{"location":"cli/k0s_validate/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info])","title":"Options inherited from parent commands"},{"location":"cli/k0s_validate/#see-also","text":"k0s - k0s - Zero Friction Kubernetes k0s validate config - Helper command for validating the config file","title":"SEE ALSO"},{"location":"cli/k0s_validate_config/","text":"k0s validate config # Helper command for validating the config file Synopsis # Example: k0s validate config --config path_to_config.yaml k0s validate config [flags] Options # -h, --help help for config Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) SEE ALSO # k0s validate - Helper command for validating the config file","title":"K0s validate config"},{"location":"cli/k0s_validate_config/#k0s-validate-config","text":"Helper command for validating the config file","title":"k0s validate config"},{"location":"cli/k0s_validate_config/#synopsis","text":"Example: k0s validate config --config path_to_config.yaml k0s validate config [flags]","title":"Synopsis"},{"location":"cli/k0s_validate_config/#options","text":"-h, --help help for config","title":"Options"},{"location":"cli/k0s_validate_config/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info])","title":"Options inherited from parent commands"},{"location":"cli/k0s_validate_config/#see-also","text":"k0s validate - Helper command for validating the config file","title":"SEE ALSO"},{"location":"cli/k0s_version/","text":"k0s version # Print the k0s version k0s version [flags] Options # -h, --help help for version Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s version"},{"location":"cli/k0s_version/#k0s-version","text":"Print the k0s version k0s version [flags]","title":"k0s version"},{"location":"cli/k0s_version/#options","text":"-h, --help help for version","title":"Options"},{"location":"cli/k0s_version/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info])","title":"Options inherited from parent commands"},{"location":"cli/k0s_version/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_worker/","text":"k0s worker # Run worker k0s worker [join-token] [flags] Examples # Command to add worker node to the master node: CLI argument: $ k0s worker [token] or CLI flag: $ k0s worker --token-file [path_to_file] Note: Token can be passed either as a CLI argument or as a flag Options # --api-server string HACK: api-server for the windows worker node --cidr-range string HACK: cidr range for the windows worker node (default \"10.96.0.0/12\") --cluster-dns string HACK: cluster dns for the windows worker node (default \"10.96.0.10\") --cri-socket string contrainer runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket] --enable-cloud-provider Whether or not to enable cloud provider support in kubelet -h, --help help for worker --profile string worker profile to use on the node (default \"default\") --token-file string Path to the file containing token. Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes","title":"K0s worker"},{"location":"cli/k0s_worker/#k0s-worker","text":"Run worker k0s worker [join-token] [flags]","title":"k0s worker"},{"location":"cli/k0s_worker/#examples","text":"Command to add worker node to the master node: CLI argument: $ k0s worker [token] or CLI flag: $ k0s worker --token-file [path_to_file] Note: Token can be passed either as a CLI argument or as a flag","title":"Examples"},{"location":"cli/k0s_worker/#options","text":"--api-server string HACK: api-server for the windows worker node --cidr-range string HACK: cidr range for the windows worker node (default \"10.96.0.0/12\") --cluster-dns string HACK: cluster dns for the windows worker node (default \"10.96.0.10\") --cri-socket string contrainer runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket] --enable-cloud-provider Whether or not to enable cloud provider support in kubelet -h, --help help for worker --profile string worker profile to use on the node (default \"default\") --token-file string Path to the file containing token.","title":"Options"},{"location":"cli/k0s_worker/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) --debugListenOn string Http listenOn for debug pprof handler (default \":6060\") -l, --logging stringToString Logging Levels for the different components (default [containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1,kube-proxy=1,etcd=info])","title":"Options inherited from parent commands"},{"location":"cli/k0s_worker/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"contributors/CODE_OF_CONDUCT/","text":"k0s Community Code of Conduct # k0s follows the CNCF Code of Conduct .","title":"k0s Community Code of Conduct"},{"location":"contributors/CODE_OF_CONDUCT/#k0s-community-code-of-conduct","text":"k0s follows the CNCF Code of Conduct .","title":"k0s Community Code of Conduct"},{"location":"contributors/github_workflow/","text":"Github Workflow # Fork The Project Adding the Forked Remote Create & Rebase Your Feature Branch Commit & Push Open a Pull Request Get a code review Squash commits Push Your Final Changes This guide assumes you have already cloned the upstream repo to your system via git clone, or via go get github.com/k0sproject/k0s . Fork The Project # Go to http://github.com/k0sproject/k0s On the top, right-hand side, click on \"fork\" and select your username for the fork destination. Adding the Forked Remote # export GITHUB_USER={ your github's username } cd $WORKDIR/k0s git remote add $GITHUB_USER git@github.com:${GITHUB_USER}/k0s.git # Prevent push to Upstream git remote set-url --push origin no_push # Set your fork remote as a default push target git push --set-upstream $GITHUB_USER main Your remotes should look something like this: \u279c git remote -v origin https://github.com/k0sproject/k0s (fetch) origin no_push (push) my_fork git@github.com:{ github_username }/k0s.git (fetch) my_fork git@github.com:{ github_username }/k0s.git (push) Create & Rebase Your Feature Branch # Create a feature branch and switch to it: git checkout -b my_feature_branch Rebase your branch: git fetch origin git rebase origin/main Current branch my_feature_branch is up to date. Please don't use git pull instead of the above fetch / rebase . git pull does a merge, which leaves merge commits. These make the commit history messy and violate the principle that commits ought to be individually understandable and useful. Commit & Push # Commit and sign your changes: git commit --signoff The commit message should have a short title as first line, an empty line and then a longer description that explains why the change was made, unless it is obvious. You can go back and edit/build/test some more, then commit --amend in a few cycles. When ready, push your changes to your fork's repository: git push --set-upstream my_fork my_feature_branch Open a Pull Request # Github Docs Get a code review # Once your pull request has been opened it will be assigned to one or more reviewers, and will go through a series of smoke tests. Commit changes made in response to review comments should be added to the same branch on your fork. Very small PRs are easy to review. Very large PRs are very difficult to review. Squashing Commits # Commits on your branch should represent meaningful milestones or units of work. Small commits that contain typo fixes, rebases, review feedbacks, etc should be squashed. To do that, it's best to perform an interactive rebase : Example # Rebase your feature branch against upstream main branch: git rebase -i origin/main If your PR has 3 commits, output would be similar to this: pick f7f3f6d Changed some code pick 310154e fixed some typos pick a5f4a0d made some review changes # Rebase 710f0f8..a5f4a0d onto 710f0f8 # # Commands: # p, pick <commit> = use commit # r, reword <commit> = use commit, but edit the commit message # e, edit <commit> = use commit, but stop for amending # s, squash <commit> = use commit, but meld into previous commit # f, fixup <commit> = like \"squash\", but discard this commit's log message # x, exec <command> = run command (the rest of the line) using shell # b, break = stop here (continue rebase later with 'git rebase --continue') # d, drop <commit> = remove commit # l, label <label> = label current HEAD with a name # t, reset <label> = reset HEAD to a label # m, merge [-C <commit> | -c <commit>] <label> [# <oneline>] # . create a merge commit using the original merge commit's # . message (or the oneline, if no original merge commit was # . specified). Use -c <commit> to reword the commit message. # # These lines can be re-ordered; they are executed from top to bottom. # # However, if you remove everything, the rebase will be aborted. # # Note that empty commits are commented out Use a command line text editor to change the word pick to f of fixup for the commits you want to squash, then save your changes and continue the rebase: Per the output above, you can see that: fixup <commit> = like \"squash\", but discard this commit's log message Which means that when rebased, the commit message \"fixed some typos\" will be removed, and squashed with the parent commit. Push Your Final Changes # Once done, you can push the final commits to your branch: git push --force You can run multiple iteration of rebase / push -f , if needed.","title":"Workflow"},{"location":"contributors/github_workflow/#github-workflow","text":"Fork The Project Adding the Forked Remote Create & Rebase Your Feature Branch Commit & Push Open a Pull Request Get a code review Squash commits Push Your Final Changes This guide assumes you have already cloned the upstream repo to your system via git clone, or via go get github.com/k0sproject/k0s .","title":"Github Workflow"},{"location":"contributors/github_workflow/#fork-the-project","text":"Go to http://github.com/k0sproject/k0s On the top, right-hand side, click on \"fork\" and select your username for the fork destination.","title":"Fork The Project"},{"location":"contributors/github_workflow/#adding-the-forked-remote","text":"export GITHUB_USER={ your github's username } cd $WORKDIR/k0s git remote add $GITHUB_USER git@github.com:${GITHUB_USER}/k0s.git # Prevent push to Upstream git remote set-url --push origin no_push # Set your fork remote as a default push target git push --set-upstream $GITHUB_USER main Your remotes should look something like this: \u279c git remote -v origin https://github.com/k0sproject/k0s (fetch) origin no_push (push) my_fork git@github.com:{ github_username }/k0s.git (fetch) my_fork git@github.com:{ github_username }/k0s.git (push)","title":"Adding the Forked Remote"},{"location":"contributors/github_workflow/#create-rebase-your-feature-branch","text":"Create a feature branch and switch to it: git checkout -b my_feature_branch Rebase your branch: git fetch origin git rebase origin/main Current branch my_feature_branch is up to date. Please don't use git pull instead of the above fetch / rebase . git pull does a merge, which leaves merge commits. These make the commit history messy and violate the principle that commits ought to be individually understandable and useful.","title":"Create &amp; Rebase Your Feature Branch"},{"location":"contributors/github_workflow/#commit-push","text":"Commit and sign your changes: git commit --signoff The commit message should have a short title as first line, an empty line and then a longer description that explains why the change was made, unless it is obvious. You can go back and edit/build/test some more, then commit --amend in a few cycles. When ready, push your changes to your fork's repository: git push --set-upstream my_fork my_feature_branch","title":"Commit &amp; Push"},{"location":"contributors/github_workflow/#open-a-pull-request","text":"Github Docs","title":"Open a Pull Request"},{"location":"contributors/github_workflow/#get-a-code-review","text":"Once your pull request has been opened it will be assigned to one or more reviewers, and will go through a series of smoke tests. Commit changes made in response to review comments should be added to the same branch on your fork. Very small PRs are easy to review. Very large PRs are very difficult to review.","title":"Get a code review"},{"location":"contributors/github_workflow/#squashing-commits","text":"Commits on your branch should represent meaningful milestones or units of work. Small commits that contain typo fixes, rebases, review feedbacks, etc should be squashed. To do that, it's best to perform an interactive rebase :","title":"Squashing Commits"},{"location":"contributors/github_workflow/#push-your-final-changes","text":"Once done, you can push the final commits to your branch: git push --force You can run multiple iteration of rebase / push -f , if needed.","title":"Push Your Final Changes"},{"location":"contributors/overview/","text":"Contributing to k0s # Thank you for taking the time to make a contribution to k0s. The following document is a set of guidelines and instructions for contributing to k0s. When contributing to this repository, please consider first discussing the change you wish to make by opening an issue. Code of Conduct # Our code of conduct can be found in the link below. Please follow it in all your interactions with the project. Code Of Conduct Github Workflow # We Use Github Flow , so all code changes are tracked via Pull Requests. A detailed guide on the recommended workflow can be found below: Github Workflow Code Testing # All submitted PRs go through a set of tests and reviews. You can run most of these tests before a PR is submitted. In fact, we recommend it, because it will save on many possible review iterations and automated tests. The testing guidelines can be found here: Contributor's Guide to Testing License # By contributing, you agree that your contributions will be licensed as followed: All content residing under the \"docs/\" directory of this repository is licensed under \"Creative Commons Attribution Share Alike 4.0 International\" (CC-BY-SA-4.0). See docs/LICENCE for details. Content outside of the above mentioned directories or restrictions above is available under the \"Apache License 2.0\". Community # Some of you might have noticed we have official community blog hosted on Medium . If you are not yet following us, we'd like to invite you to do so now! Make sure to follow us on Twitter as well \ud83d\ude0a We have also agreed to share Slack with Lens IDE team. They are close friends of the k0s crew, and many of the Kubernetes users are using Lens anyway, so it felt very natural for us to share their Slack. You\u2019ll find us on dedicated k0s channel. Please join the k0s Slack channel to hear the latest news, discussions and provide your feedback!","title":"Overview"},{"location":"contributors/overview/#contributing-to-k0s","text":"Thank you for taking the time to make a contribution to k0s. The following document is a set of guidelines and instructions for contributing to k0s. When contributing to this repository, please consider first discussing the change you wish to make by opening an issue.","title":"Contributing to k0s"},{"location":"contributors/overview/#code-of-conduct","text":"Our code of conduct can be found in the link below. Please follow it in all your interactions with the project. Code Of Conduct","title":"Code of Conduct"},{"location":"contributors/overview/#github-workflow","text":"We Use Github Flow , so all code changes are tracked via Pull Requests. A detailed guide on the recommended workflow can be found below: Github Workflow","title":"Github Workflow"},{"location":"contributors/overview/#code-testing","text":"All submitted PRs go through a set of tests and reviews. You can run most of these tests before a PR is submitted. In fact, we recommend it, because it will save on many possible review iterations and automated tests. The testing guidelines can be found here: Contributor's Guide to Testing","title":"Code Testing"},{"location":"contributors/overview/#license","text":"By contributing, you agree that your contributions will be licensed as followed: All content residing under the \"docs/\" directory of this repository is licensed under \"Creative Commons Attribution Share Alike 4.0 International\" (CC-BY-SA-4.0). See docs/LICENCE for details. Content outside of the above mentioned directories or restrictions above is available under the \"Apache License 2.0\".","title":"License"},{"location":"contributors/overview/#community","text":"Some of you might have noticed we have official community blog hosted on Medium . If you are not yet following us, we'd like to invite you to do so now! Make sure to follow us on Twitter as well \ud83d\ude0a We have also agreed to share Slack with Lens IDE team. They are close friends of the k0s crew, and many of the Kubernetes users are using Lens anyway, so it felt very natural for us to share their Slack. You\u2019ll find us on dedicated k0s channel. Please join the k0s Slack channel to hear the latest news, discussions and provide your feedback!","title":"Community"},{"location":"contributors/testing/","text":"Testing Your Code # k0s uses github actions to run automated tests on any PR, before merging. However, a PR will not be reviewed before all tests are green, so to save time and prevent your PR from going stale, it is best to test it before submitting the PR. Run Local Verifications # Please run the following style and formatting commands and fix/check-in any changes: 1. Linting # We use golangci-lint for style verification. In the repository's root directory, simply run: make lint 2. Go fmt # go fmt ./... 3. Pre-submit Flight Checks # In the repository root directory, make sure that: make build runs successfully. make check-basic runs successfully. make check-unit has no errors. make check-hacontrolplane runs successfully. Please note that this last test is prone to \"flakiness\", so it might fail on occasion. If it fails constantly, take a deeper look at your code to find the source of the problem. If you find that all tests passed, you may open a pull request upstream. Opening A Pull Request # Draft Mode # You may open a pull request in draft mode . All automated tests will still run against the PR, but the PR will not be assigned for review. Once a PR is ready for review, transition it from Draft mode, and code owners will be notified. Conformance Testing # Once a PR has been reviewed and all other tests have passed, a code owner will run a full end-to-end conformance test against the PR. This is usually the last step before merging. Pre-Requisites for PR Merge # In order for a PR to be merged, the following conditions should exist: 1. The PR has passed all the automated tests (style, build & conformance tests). 2. PR commits have been signed with the --signoff option. 3. PR was reviewed and approved by a code owner. 4. PR is rebased against upstream's main branch.","title":"Testing"},{"location":"contributors/testing/#testing-your-code","text":"k0s uses github actions to run automated tests on any PR, before merging. However, a PR will not be reviewed before all tests are green, so to save time and prevent your PR from going stale, it is best to test it before submitting the PR.","title":"Testing Your Code"},{"location":"contributors/testing/#run-local-verifications","text":"Please run the following style and formatting commands and fix/check-in any changes:","title":"Run Local Verifications"},{"location":"contributors/testing/#opening-a-pull-request","text":"","title":"Opening A Pull Request"},{"location":"examples/ambassador-ingress/","text":"Installing the Ambassador Gateway on k0s # In this tutorial, you'll learn how to run k0s under Docker and configure it with the Ambassador API Gateway and a MetalLB service loadbalancer . We'll also deploy a sample service and expose it with an Ambassador mapping resource. Utilizing the extensible bootstrapping functionality with Helm, it's as simple as adding the right extensions to the k0s.yaml file when configuring your cluster. Running k0s under docker # If you're not on a platform natively supported by k0s, running under docker is a viable option (see k0s in Docker ). Since we're going to create a custom configuration file we'll need to map that into the k0s container - and of course we'll need to expose the ports required by Ambassador for outside access. Start by running k0s under docker : docker run -d --name k0s --hostname k0s --privileged -v /var/lib/k0s -p 6443 :6443 docker.io/k0sproject/k0s:latest Once running, export the default k0s configuration file using docker exec k0s k0s default-config > k0s.yaml and export the cluster config so you can access it with kubectl: docker exec k0s cat /var/lib/k0s/pki/admin.conf > k0s-cluster.conf export KUBECONFIG = $KUBECONFIG :<absolute path to k0s-cluster.conf> (somewhat brute-force but gets the job done) Configuring k0s.yaml # Open the file in your favorite code editor and add the following extensions at the bottom: extensions : helm : repositories : - name : datawire url : https://www.getambassador.io - name : bitnami url : https://charts.bitnami.com/bitnami charts : - name : ambassador chartname : datawire/ambassador version : \"6.5.13\" namespace : ambassador values : |2 service: externalIPs: - 172.17.0.2 - name : metallb chartname : bitnami/metallb version : \"1.0.1\" namespace : default values : |2 configInline: address-pools: - name: generic-cluster-pool protocol: layer2 addresses: - 172.17.0.2 (you might need to replace the 172.17.0.2 IP with your local IP which you can find higher up in the generated file under spec.api.address) As you can see it adds both Ambassador and Metallb (required for LoadBalancers) with corresponding repositories and (minimal) configurations. This example only uses your local network - providing a range of IPs for MetalLB that are addressable on your LAN is suggested if you want to access these services from anywhere on your network. Now stop/remove your k0s container with docker stop k0s and docker rm k0s , then start it again with additional ports and the above config file mapped into it: docker run --name k0s --hostname k0s --privileged -v /var/lib/k0s -v <path to k0s.yaml file>:/k0s.yaml -p 6443 :6443 -p 80 :80 -p 443 :443 -p 8080 :8080 docker.io/k0sproject/k0s:latest Let it start, and eventually (this can take some time) you'll be able to list the Ambassador Services: kubectl get services -n ambassador NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE ambassador-1611224811 LoadBalancer 10 .99.84.151 172 .17.0.2 80 :30327/TCP,443:30355/TCP 2m11s ambassador-1611224811-admin ClusterIP 10 .96.79.130 <none> 8877 /TCP 2m11s ambassador-1611224811-redis ClusterIP 10 .110.33.229 <none> 6379 /TCP 2m11s Install the Ambassador edgectl tool and run the login command: edgectl login --namespace = ambassador localhost This will open your browser and take you to the Ambassador Console - all ready to go. Deploy / map a service # Let's deploy and map the Swagger Petstore service; create a petstore.yaml file with the following content. --- apiVersion : v1 kind : Service metadata : name : petstore namespace : ambassador spec : ports : - name : http port : 80 targetPort : 8080 selector : app : petstore --- apiVersion : apps/v1 kind : Deployment metadata : name : petstore namespace : ambassador spec : replicas : 1 selector : matchLabels : app : petstore strategy : type : RollingUpdate template : metadata : labels : app : petstore spec : containers : - name : petstore-backend image : docker.io/swaggerapi/petstore3:unstable ports : - name : http containerPort : 8080 --- apiVersion : getambassador.io/v2 kind : Mapping metadata : name : petstore namespace : ambassador spec : prefix : /petstore/ service : petstore Once you've created this, apply it: kubectl apply -f petstore.yaml service/petstore created deployment.apps/petstore created mapping.getambassador.io/petstore created and you should be able to curl the service: curl -k 'https://localhost/petstore/api/v3/pet/findByStatus?status=available' [{ \"id\" :1, \"category\" : { \"id\" :2, \"name\" : \"Cats\" } , \"name\" : \"Cat 1\" , \"photoUrls\" : [ \"url1\" , \"url2\" ] , \"tags\" : [{ \"id\" :1, \"name\" : \"tag1\" } , { \"id\" :2, \"name\" : \"tag2\" }] , \"status\" : \"available\" } , { \"id\" :2, \"category\" : { \"id\" :2, \"name\" : \"Cats\" } , \"name\" : \"Cat 2\" , \"photoUrls\" : [ \"url1\" , \"url2\" ] , \"tags\" : [{ \"id\" :1, \"name\" : \"tag2\" } , { \"id\" :2, \"name\" : \"tag3\" }] , \"status\" : \"available\" } , { \"id\" :4, \"category\" : { \"id\" :1, \"name\" : \"Dogs\" } , \"name\" : \"Dog 1\" , \"photoUrls\" : [ \"url1\" , \"url2\" ] , \"tags\" : [{ \"id\" :1, \"name\" : \"tag1\" } , { \"id\" :2, \"name\" : \"tag2\" }] , \"status\" : \"available\" } , { \"id\" :7, \"category\" : { \"id\" :4, \"name\" : \"Lions\" } , \"name\" : \"Lion 1\" , \"photoUrls\" : [ \"url1\" , \"url2\" ] , \"tags\" : [{ \"id\" :1, \"name\" : \"tag1\" } , { \"id\" :2, \"name\" : \"tag2\" }] , \"status\" : \"available\" } , { \"id\" :8, \"category\" : { \"id\" :4, \"name\" : \"Lions\" } , \"name\" : \"Lion 2\" , \"photoUrls\" : [ \"url1\" , \"url2\" ] , \"tags\" : [{ \"id\" :1, \"name\" : \"tag2\" } , { \"id\" :2, \"name\" : \"tag3\" }] , \"status\" : \"available\" } , { \"id\" :9, \"category\" : { \"id\" :4, \"name\" : \"Lions\" } , \"name\" : \"Lion 3\" , \"photoUrls\" : [ \"url1\" , \"url2\" ] , \"tags\" : [{ \"id\" :1, \"name\" : \"tag3\" } , { \"id\" :2, \"name\" : \"tag4\" }] , \"status\" : \"available\" } , { \"id\" :10, \"category\" : { \"id\" :3, \"name\" : \"Rabbits\" } , \"name\" : \"Rabbit 1\" , \"photoUrls\" : [ \"url1\" , \"url2\" ] , \"tags\" : [{ \"id\" :1, \"name\" : \"tag3\" } , { \"id\" :2, \"name\" : \"tag4\" }] , \"status\" : \"available\" }] or you can open https://localhost/petstore/ in your browser and change the URL of the specification to https://localhost/petstore/api/v3/openapi.json (since we mapped it to the /petstore prefix). If you navigate to the Mappings part of the Ambassador Console (opened above) you will see the corresponding PetStore mapping as configured. Summary # This should get you all set with running Ambassador under k0s. If you're not running under Docker just skip the docker-related steps above - but make sure that you have updated the k0s configuration in the same way as above. If you're stuck with or have any questions about Ambassador please try the Ambassador Slack to get help.","title":"Installing the Ambassador Gateway"},{"location":"examples/ambassador-ingress/#installing-the-ambassador-gateway-on-k0s","text":"In this tutorial, you'll learn how to run k0s under Docker and configure it with the Ambassador API Gateway and a MetalLB service loadbalancer . We'll also deploy a sample service and expose it with an Ambassador mapping resource. Utilizing the extensible bootstrapping functionality with Helm, it's as simple as adding the right extensions to the k0s.yaml file when configuring your cluster.","title":"Installing the Ambassador Gateway on k0s"},{"location":"examples/ambassador-ingress/#running-k0s-under-docker","text":"If you're not on a platform natively supported by k0s, running under docker is a viable option (see k0s in Docker ). Since we're going to create a custom configuration file we'll need to map that into the k0s container - and of course we'll need to expose the ports required by Ambassador for outside access. Start by running k0s under docker : docker run -d --name k0s --hostname k0s --privileged -v /var/lib/k0s -p 6443 :6443 docker.io/k0sproject/k0s:latest Once running, export the default k0s configuration file using docker exec k0s k0s default-config > k0s.yaml and export the cluster config so you can access it with kubectl: docker exec k0s cat /var/lib/k0s/pki/admin.conf > k0s-cluster.conf export KUBECONFIG = $KUBECONFIG :<absolute path to k0s-cluster.conf> (somewhat brute-force but gets the job done)","title":"Running k0s under docker"},{"location":"examples/ambassador-ingress/#configuring-k0syaml","text":"Open the file in your favorite code editor and add the following extensions at the bottom: extensions : helm : repositories : - name : datawire url : https://www.getambassador.io - name : bitnami url : https://charts.bitnami.com/bitnami charts : - name : ambassador chartname : datawire/ambassador version : \"6.5.13\" namespace : ambassador values : |2 service: externalIPs: - 172.17.0.2 - name : metallb chartname : bitnami/metallb version : \"1.0.1\" namespace : default values : |2 configInline: address-pools: - name: generic-cluster-pool protocol: layer2 addresses: - 172.17.0.2 (you might need to replace the 172.17.0.2 IP with your local IP which you can find higher up in the generated file under spec.api.address) As you can see it adds both Ambassador and Metallb (required for LoadBalancers) with corresponding repositories and (minimal) configurations. This example only uses your local network - providing a range of IPs for MetalLB that are addressable on your LAN is suggested if you want to access these services from anywhere on your network. Now stop/remove your k0s container with docker stop k0s and docker rm k0s , then start it again with additional ports and the above config file mapped into it: docker run --name k0s --hostname k0s --privileged -v /var/lib/k0s -v <path to k0s.yaml file>:/k0s.yaml -p 6443 :6443 -p 80 :80 -p 443 :443 -p 8080 :8080 docker.io/k0sproject/k0s:latest Let it start, and eventually (this can take some time) you'll be able to list the Ambassador Services: kubectl get services -n ambassador NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE ambassador-1611224811 LoadBalancer 10 .99.84.151 172 .17.0.2 80 :30327/TCP,443:30355/TCP 2m11s ambassador-1611224811-admin ClusterIP 10 .96.79.130 <none> 8877 /TCP 2m11s ambassador-1611224811-redis ClusterIP 10 .110.33.229 <none> 6379 /TCP 2m11s Install the Ambassador edgectl tool and run the login command: edgectl login --namespace = ambassador localhost This will open your browser and take you to the Ambassador Console - all ready to go.","title":"Configuring k0s.yaml"},{"location":"examples/ambassador-ingress/#deploy-map-a-service","text":"Let's deploy and map the Swagger Petstore service; create a petstore.yaml file with the following content. --- apiVersion : v1 kind : Service metadata : name : petstore namespace : ambassador spec : ports : - name : http port : 80 targetPort : 8080 selector : app : petstore --- apiVersion : apps/v1 kind : Deployment metadata : name : petstore namespace : ambassador spec : replicas : 1 selector : matchLabels : app : petstore strategy : type : RollingUpdate template : metadata : labels : app : petstore spec : containers : - name : petstore-backend image : docker.io/swaggerapi/petstore3:unstable ports : - name : http containerPort : 8080 --- apiVersion : getambassador.io/v2 kind : Mapping metadata : name : petstore namespace : ambassador spec : prefix : /petstore/ service : petstore Once you've created this, apply it: kubectl apply -f petstore.yaml service/petstore created deployment.apps/petstore created mapping.getambassador.io/petstore created and you should be able to curl the service: curl -k 'https://localhost/petstore/api/v3/pet/findByStatus?status=available' [{ \"id\" :1, \"category\" : { \"id\" :2, \"name\" : \"Cats\" } , \"name\" : \"Cat 1\" , \"photoUrls\" : [ \"url1\" , \"url2\" ] , \"tags\" : [{ \"id\" :1, \"name\" : \"tag1\" } , { \"id\" :2, \"name\" : \"tag2\" }] , \"status\" : \"available\" } , { \"id\" :2, \"category\" : { \"id\" :2, \"name\" : \"Cats\" } , \"name\" : \"Cat 2\" , \"photoUrls\" : [ \"url1\" , \"url2\" ] , \"tags\" : [{ \"id\" :1, \"name\" : \"tag2\" } , { \"id\" :2, \"name\" : \"tag3\" }] , \"status\" : \"available\" } , { \"id\" :4, \"category\" : { \"id\" :1, \"name\" : \"Dogs\" } , \"name\" : \"Dog 1\" , \"photoUrls\" : [ \"url1\" , \"url2\" ] , \"tags\" : [{ \"id\" :1, \"name\" : \"tag1\" } , { \"id\" :2, \"name\" : \"tag2\" }] , \"status\" : \"available\" } , { \"id\" :7, \"category\" : { \"id\" :4, \"name\" : \"Lions\" } , \"name\" : \"Lion 1\" , \"photoUrls\" : [ \"url1\" , \"url2\" ] , \"tags\" : [{ \"id\" :1, \"name\" : \"tag1\" } , { \"id\" :2, \"name\" : \"tag2\" }] , \"status\" : \"available\" } , { \"id\" :8, \"category\" : { \"id\" :4, \"name\" : \"Lions\" } , \"name\" : \"Lion 2\" , \"photoUrls\" : [ \"url1\" , \"url2\" ] , \"tags\" : [{ \"id\" :1, \"name\" : \"tag2\" } , { \"id\" :2, \"name\" : \"tag3\" }] , \"status\" : \"available\" } , { \"id\" :9, \"category\" : { \"id\" :4, \"name\" : \"Lions\" } , \"name\" : \"Lion 3\" , \"photoUrls\" : [ \"url1\" , \"url2\" ] , \"tags\" : [{ \"id\" :1, \"name\" : \"tag3\" } , { \"id\" :2, \"name\" : \"tag4\" }] , \"status\" : \"available\" } , { \"id\" :10, \"category\" : { \"id\" :3, \"name\" : \"Rabbits\" } , \"name\" : \"Rabbit 1\" , \"photoUrls\" : [ \"url1\" , \"url2\" ] , \"tags\" : [{ \"id\" :1, \"name\" : \"tag3\" } , { \"id\" :2, \"name\" : \"tag4\" }] , \"status\" : \"available\" }] or you can open https://localhost/petstore/ in your browser and change the URL of the specification to https://localhost/petstore/api/v3/openapi.json (since we mapped it to the /petstore prefix). If you navigate to the Mappings part of the Ambassador Console (opened above) you will see the corresponding PetStore mapping as configured.","title":"Deploy / map a service"},{"location":"examples/ambassador-ingress/#summary","text":"This should get you all set with running Ambassador under k0s. If you're not running under Docker just skip the docker-related steps above - but make sure that you have updated the k0s configuration in the same way as above. If you're stuck with or have any questions about Ambassador please try the Ambassador Slack to get help.","title":"Summary"},{"location":"examples/ansible-playbook/","text":"Creating a cluster with Ansible Playbook # Using Ansible and the k0s-ansible playbook, you can install a multi-node Kubernetes Cluster in a couple of minutes. Ansible is a popular infrastructure as code tool which helps you automate tasks to achieve the desired state in a system. This guide shows how you can install k0s on local virtual machines. In this guide, the following tools are used: multipass , a lightweight VM manager that uses KVM on Linux, Hyper-V on Windows, and hypervisor.framework on macOS ( installation guide ). ansible , a popular infrastructure as code tool ( installation guide ). and of course kubectl on your local machine ( Installation guide ). Before following this tutorial, you should have a general understanding of Ansible. A great way to start is the official Ansible User Guide . Please note: k0s users created k0s-ansible. Please send your feedback, bug reports, and pull requests to github.com/movd/k0s-ansible . Without further ado, let's jump right in. Download k0s-ansible # On your local machine clone the k0s-ansible repository: $ git clone https://github.com/movd/k0s-ansible.git $ cd k0s-ansible Create virtual machines # For this tutorial, multipass was used. However, there is no interdependence. This playbook should also work with VMs created in alternative ways or Raspberry Pis. Next, create a couple of virtual machines. For the automation to work, each instance must have passwordless SSH access. To achieve this, we provision each instance with a cloud-init manifest that imports your current users' public SSH key and into a user k0s . For your convenience, a bash script is included that does just that: ./tools/multipass_create_instances.sh 7 \u25c0\ufe0f this creates 7 virtual machines $ ./tools/multipass_create_instances.sh 7 Create cloud-init to import ssh key... [1/7] Creating instance k0s-1 with multipass... Launched: k0s-1 [2/7] Creating instance k0s-2 with multipass... Launched: k0s-2 [3/7] Creating instance k0s-3 with multipass... Launched: k0s-3 [4/7] Creating instance k0s-4 with multipass... Launched: k0s-4 [5/7] Creating instance k0s-5 with multipass... Launched: k0s-5 [6/7] Creating instance k0s-6 with multipass... Launched: k0s-6 [7/7] Creating instance k0s-7 with multipass... Launched: k0s-7 Name State IPv4 Image k0s-1 Running 192.168.64.32 Ubuntu 20.04 LTS k0s-2 Running 192.168.64.33 Ubuntu 20.04 LTS k0s-3 Running 192.168.64.56 Ubuntu 20.04 LTS k0s-4 Running 192.168.64.57 Ubuntu 20.04 LTS k0s-5 Running 192.168.64.58 Ubuntu 20.04 LTS k0s-6 Running 192.168.64.60 Ubuntu 20.04 LTS k0s-7 Running 192.168.64.61 Ubuntu 20.04 LTS Create Ansible inventory # After that, we create our inventory directory by copying the sample: $ cp -rfp inventory/sample inventory/multipass Now we need to create our inventory. The before built virtual machines need to be assigned to the different host groups required by the playbook's logic. initial_controller = must contain a single node that creates the worker and server tokens needed by the other nodes. controller = can contain nodes that, together with the host from initial_controller form a highly available isolated control plane. worker = must contain at least one node so that we can deploy Kubernetes objects. We could fill inventory/multipass/inventory.yml by hand with the metadata provided by multipass list, but since we are lazy and want to automate as much as possible, we can use the included Python script multipass_generate_inventory.py : To automatically fill our inventory run: $ ./tools/multipass_generate_inventory.py Designate first three instances as control plane Created Ansible Inventory at: /Users/dev/k0s-ansible/tools/inventory.yml $ cp tools/inventory.yml inventory/multipass/inventory.yml Now inventory/multipass/inventory.yml should look like this (Of course, your IP addresses might differ): --- all : children : initial_controller : hosts : k0s-1 : controller : hosts : k0s-2 : k0s-3 : worker : hosts : k0s-4 : k0s-5 : k0s-6 : k0s-7 : hosts : k0s-1 : ansible_host : 192.168.64.32 k0s-2 : ansible_host : 192.168.64.33 k0s-3 : ansible_host : 192.168.64.56 k0s-4 : ansible_host : 192.168.64.57 k0s-5 : ansible_host : 192.168.64.58 k0s-6 : ansible_host : 192.168.64.60 k0s-7 : ansible_host : 192.168.64.61 vars : ansible_user : k0s Test the connection to the virtual machines # To test the connection to your hosts just run: $ ansible -i inventory/multipass/inventory.yml -m ping k0s-4 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" } ... If all is green and successful, you can proceed. Provision the cluster with Ansible # Finally, we can start provisioning the cluster. Applying the playbook, k0s will get downloaded and set up on all nodes, tokens will get exchanged, and a kubeconfig will get dumped to your local deployment environment. $ ansible-playbook site.yml -i inventory/multipass/inventory.yml ... TASK [k0s/initial_controller : print kubeconfig command] ******************************************************* Tuesday 22 December 2020 17:43:20 +0100 (0:00:00.257) 0:00:41.287 ****** ok: [k0s-1] => { \"msg\": \"To use Cluster: export KUBECONFIG=/Users/dev/k0s-ansible/inventory/multipass/artifacts/k0s-kubeconfig.yml\" } ... PLAY RECAP ***************************************************************************************************** k0s-1 : ok=21 changed=11 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 k0s-2 : ok=10 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 k0s-3 : ok=10 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 k0s-4 : ok=9 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 k0s-5 : ok=9 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 k0s-6 : ok=9 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 k0s-7 : ok=9 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 Tuesday 22 December 2020 17:43:36 +0100 (0:00:01.204) 0:00:57.478 ****** =============================================================================== prereq : Install apt packages -------------------------------------------------------------------------- 22.70s k0s/controller : Wait for k8s apiserver ----------------------------------------------------------------- 4.30s k0s/initial_controller : Create worker join token ------------------------------------------------------- 3.38s k0s/initial_controller : Wait for k8s apiserver --------------------------------------------------------- 3.36s download : Download k0s binary k0s-v0.9.0-rc1-amd64 ----------------------------------------------------- 3.11s Gathering Facts ----------------------------------------------------------------------------------------- 2.85s Gathering Facts ----------------------------------------------------------------------------------------- 1.95s prereq : Create k0s Directories ------------------------------------------------------------------------- 1.53s k0s/worker : Enable and check k0s service --------------------------------------------------------------- 1.20s prereq : Write the k0s config file ---------------------------------------------------------------------- 1.09s k0s/initial_controller : Enable and check k0s service --------------------------------------------------- 0.94s k0s/controller : Enable and check k0s service ----------------------------------------------------------- 0.73s Gathering Facts ----------------------------------------------------------------------------------------- 0.71s Gathering Facts ----------------------------------------------------------------------------------------- 0.66s Gathering Facts ----------------------------------------------------------------------------------------- 0.64s k0s/worker : Write the k0s token file on worker --------------------------------------------------------- 0.64s k0s/worker : Copy k0s service file ---------------------------------------------------------------------- 0.53s k0s/controller : Write the k0s token file on controller ------------------------------------------------- 0.41s k0s/controller : Copy k0s service file ------------------------------------------------------------------ 0.40s k0s/initial_controller : Copy k0s service file ---------------------------------------------------------- 0.36s Use the cluster with kubectl # While the playbook ran, a kubeconfig got copied to your local machine. You can use it to get simple access to your new Kubernetes cluster: $ export KUBECONFIG=/Users/dev/k0s-ansible/inventory/multipass/artifacts/k0s-kubeconfig.yml $ kubectl cluster-info Kubernetes control plane is running at https://192.168.64.32:6443 CoreDNS is running at https://192.168.64.32:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://192.168.64.32:6443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k0s-4 Ready <none> 21s v1.20.1-k0s1 192.168.64.57 <none> Ubuntu 20.04.1 LTS 5.4.0-54-generic containerd://1.4.3 k0s-5 Ready <none> 21s v1.20.1-k0s1 192.168.64.58 <none> Ubuntu 20.04.1 LTS 5.4.0-54-generic containerd://1.4.3 k0s-6 NotReady <none> 21s v1.20.1-k0s1 192.168.64.60 <none> Ubuntu 20.04.1 LTS 5.4.0-54-generic containerd://1.4.3 k0s-7 NotReady <none> 21s v1.20.1-k0s1 192.168.64.61 <none> Ubuntu 20.04.1 LTS 5.4.0-54-generic containerd://1.4.3 \u2b06\ufe0f Of course, the first three control plane nodes won't show up here because the control plane is fully isolated. You can check on the distributed etcd cluster by running this ad-hoc command (or ssh'ing directly into a controller node): $ ansible k0s-1 -a \"k0s etcd member-list -c /etc/k0s/k0s.yaml\" -i inventory/multipass/inventory.yml | tail -1 | jq { \"level\": \"info\", \"members\": { \"k0s-1\": \"https://192.168.64.32:2380\", \"k0s-2\": \"https://192.168.64.33:2380\", \"k0s-3\": \"https://192.168.64.56:2380\" }, \"msg\": \"done\", \"time\": \"2020-12-23T00:21:22+01:00\" } After a while, all worker nodes become Ready . Your cluster is now waiting to get used. We can test by creating a simple nginx deployment. $ kubectl create deployment nginx --image=gcr.io/google-containers/nginx --replicas=5 deployment.apps/nginx created $ kubectl expose deployment nginx --target-port=80 --port=8100 service/nginx exposed $ kubectl run hello-k0s --image=quay.io/prometheus/busybox --rm -it --restart=Never --command -- wget -qO- nginx:8100 <!DOCTYPE html> <html> <head> <title>Welcome to nginx on Debian!</title> ... pod \"hello-k0s\" deleted","title":"Creating a cluster with Ansible Playbook"},{"location":"examples/ansible-playbook/#creating-a-cluster-with-ansible-playbook","text":"Using Ansible and the k0s-ansible playbook, you can install a multi-node Kubernetes Cluster in a couple of minutes. Ansible is a popular infrastructure as code tool which helps you automate tasks to achieve the desired state in a system. This guide shows how you can install k0s on local virtual machines. In this guide, the following tools are used: multipass , a lightweight VM manager that uses KVM on Linux, Hyper-V on Windows, and hypervisor.framework on macOS ( installation guide ). ansible , a popular infrastructure as code tool ( installation guide ). and of course kubectl on your local machine ( Installation guide ). Before following this tutorial, you should have a general understanding of Ansible. A great way to start is the official Ansible User Guide . Please note: k0s users created k0s-ansible. Please send your feedback, bug reports, and pull requests to github.com/movd/k0s-ansible . Without further ado, let's jump right in.","title":"Creating a cluster with Ansible Playbook"},{"location":"examples/ansible-playbook/#download-k0s-ansible","text":"On your local machine clone the k0s-ansible repository: $ git clone https://github.com/movd/k0s-ansible.git $ cd k0s-ansible","title":"Download k0s-ansible"},{"location":"examples/ansible-playbook/#create-virtual-machines","text":"For this tutorial, multipass was used. However, there is no interdependence. This playbook should also work with VMs created in alternative ways or Raspberry Pis. Next, create a couple of virtual machines. For the automation to work, each instance must have passwordless SSH access. To achieve this, we provision each instance with a cloud-init manifest that imports your current users' public SSH key and into a user k0s . For your convenience, a bash script is included that does just that: ./tools/multipass_create_instances.sh 7 \u25c0\ufe0f this creates 7 virtual machines $ ./tools/multipass_create_instances.sh 7 Create cloud-init to import ssh key... [1/7] Creating instance k0s-1 with multipass... Launched: k0s-1 [2/7] Creating instance k0s-2 with multipass... Launched: k0s-2 [3/7] Creating instance k0s-3 with multipass... Launched: k0s-3 [4/7] Creating instance k0s-4 with multipass... Launched: k0s-4 [5/7] Creating instance k0s-5 with multipass... Launched: k0s-5 [6/7] Creating instance k0s-6 with multipass... Launched: k0s-6 [7/7] Creating instance k0s-7 with multipass... Launched: k0s-7 Name State IPv4 Image k0s-1 Running 192.168.64.32 Ubuntu 20.04 LTS k0s-2 Running 192.168.64.33 Ubuntu 20.04 LTS k0s-3 Running 192.168.64.56 Ubuntu 20.04 LTS k0s-4 Running 192.168.64.57 Ubuntu 20.04 LTS k0s-5 Running 192.168.64.58 Ubuntu 20.04 LTS k0s-6 Running 192.168.64.60 Ubuntu 20.04 LTS k0s-7 Running 192.168.64.61 Ubuntu 20.04 LTS","title":"Create virtual machines"},{"location":"examples/ansible-playbook/#create-ansible-inventory","text":"After that, we create our inventory directory by copying the sample: $ cp -rfp inventory/sample inventory/multipass Now we need to create our inventory. The before built virtual machines need to be assigned to the different host groups required by the playbook's logic. initial_controller = must contain a single node that creates the worker and server tokens needed by the other nodes. controller = can contain nodes that, together with the host from initial_controller form a highly available isolated control plane. worker = must contain at least one node so that we can deploy Kubernetes objects. We could fill inventory/multipass/inventory.yml by hand with the metadata provided by multipass list, but since we are lazy and want to automate as much as possible, we can use the included Python script multipass_generate_inventory.py : To automatically fill our inventory run: $ ./tools/multipass_generate_inventory.py Designate first three instances as control plane Created Ansible Inventory at: /Users/dev/k0s-ansible/tools/inventory.yml $ cp tools/inventory.yml inventory/multipass/inventory.yml Now inventory/multipass/inventory.yml should look like this (Of course, your IP addresses might differ): --- all : children : initial_controller : hosts : k0s-1 : controller : hosts : k0s-2 : k0s-3 : worker : hosts : k0s-4 : k0s-5 : k0s-6 : k0s-7 : hosts : k0s-1 : ansible_host : 192.168.64.32 k0s-2 : ansible_host : 192.168.64.33 k0s-3 : ansible_host : 192.168.64.56 k0s-4 : ansible_host : 192.168.64.57 k0s-5 : ansible_host : 192.168.64.58 k0s-6 : ansible_host : 192.168.64.60 k0s-7 : ansible_host : 192.168.64.61 vars : ansible_user : k0s","title":"Create Ansible inventory"},{"location":"examples/ansible-playbook/#test-the-connection-to-the-virtual-machines","text":"To test the connection to your hosts just run: $ ansible -i inventory/multipass/inventory.yml -m ping k0s-4 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" } ... If all is green and successful, you can proceed.","title":"Test the connection to the virtual machines"},{"location":"examples/ansible-playbook/#provision-the-cluster-with-ansible","text":"Finally, we can start provisioning the cluster. Applying the playbook, k0s will get downloaded and set up on all nodes, tokens will get exchanged, and a kubeconfig will get dumped to your local deployment environment. $ ansible-playbook site.yml -i inventory/multipass/inventory.yml ... TASK [k0s/initial_controller : print kubeconfig command] ******************************************************* Tuesday 22 December 2020 17:43:20 +0100 (0:00:00.257) 0:00:41.287 ****** ok: [k0s-1] => { \"msg\": \"To use Cluster: export KUBECONFIG=/Users/dev/k0s-ansible/inventory/multipass/artifacts/k0s-kubeconfig.yml\" } ... PLAY RECAP ***************************************************************************************************** k0s-1 : ok=21 changed=11 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 k0s-2 : ok=10 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 k0s-3 : ok=10 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 k0s-4 : ok=9 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 k0s-5 : ok=9 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 k0s-6 : ok=9 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 k0s-7 : ok=9 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 Tuesday 22 December 2020 17:43:36 +0100 (0:00:01.204) 0:00:57.478 ****** =============================================================================== prereq : Install apt packages -------------------------------------------------------------------------- 22.70s k0s/controller : Wait for k8s apiserver ----------------------------------------------------------------- 4.30s k0s/initial_controller : Create worker join token ------------------------------------------------------- 3.38s k0s/initial_controller : Wait for k8s apiserver --------------------------------------------------------- 3.36s download : Download k0s binary k0s-v0.9.0-rc1-amd64 ----------------------------------------------------- 3.11s Gathering Facts ----------------------------------------------------------------------------------------- 2.85s Gathering Facts ----------------------------------------------------------------------------------------- 1.95s prereq : Create k0s Directories ------------------------------------------------------------------------- 1.53s k0s/worker : Enable and check k0s service --------------------------------------------------------------- 1.20s prereq : Write the k0s config file ---------------------------------------------------------------------- 1.09s k0s/initial_controller : Enable and check k0s service --------------------------------------------------- 0.94s k0s/controller : Enable and check k0s service ----------------------------------------------------------- 0.73s Gathering Facts ----------------------------------------------------------------------------------------- 0.71s Gathering Facts ----------------------------------------------------------------------------------------- 0.66s Gathering Facts ----------------------------------------------------------------------------------------- 0.64s k0s/worker : Write the k0s token file on worker --------------------------------------------------------- 0.64s k0s/worker : Copy k0s service file ---------------------------------------------------------------------- 0.53s k0s/controller : Write the k0s token file on controller ------------------------------------------------- 0.41s k0s/controller : Copy k0s service file ------------------------------------------------------------------ 0.40s k0s/initial_controller : Copy k0s service file ---------------------------------------------------------- 0.36s","title":"Provision the cluster with Ansible"},{"location":"examples/ansible-playbook/#use-the-cluster-with-kubectl","text":"While the playbook ran, a kubeconfig got copied to your local machine. You can use it to get simple access to your new Kubernetes cluster: $ export KUBECONFIG=/Users/dev/k0s-ansible/inventory/multipass/artifacts/k0s-kubeconfig.yml $ kubectl cluster-info Kubernetes control plane is running at https://192.168.64.32:6443 CoreDNS is running at https://192.168.64.32:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://192.168.64.32:6443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k0s-4 Ready <none> 21s v1.20.1-k0s1 192.168.64.57 <none> Ubuntu 20.04.1 LTS 5.4.0-54-generic containerd://1.4.3 k0s-5 Ready <none> 21s v1.20.1-k0s1 192.168.64.58 <none> Ubuntu 20.04.1 LTS 5.4.0-54-generic containerd://1.4.3 k0s-6 NotReady <none> 21s v1.20.1-k0s1 192.168.64.60 <none> Ubuntu 20.04.1 LTS 5.4.0-54-generic containerd://1.4.3 k0s-7 NotReady <none> 21s v1.20.1-k0s1 192.168.64.61 <none> Ubuntu 20.04.1 LTS 5.4.0-54-generic containerd://1.4.3 \u2b06\ufe0f Of course, the first three control plane nodes won't show up here because the control plane is fully isolated. You can check on the distributed etcd cluster by running this ad-hoc command (or ssh'ing directly into a controller node): $ ansible k0s-1 -a \"k0s etcd member-list -c /etc/k0s/k0s.yaml\" -i inventory/multipass/inventory.yml | tail -1 | jq { \"level\": \"info\", \"members\": { \"k0s-1\": \"https://192.168.64.32:2380\", \"k0s-2\": \"https://192.168.64.33:2380\", \"k0s-3\": \"https://192.168.64.56:2380\" }, \"msg\": \"done\", \"time\": \"2020-12-23T00:21:22+01:00\" } After a while, all worker nodes become Ready . Your cluster is now waiting to get used. We can test by creating a simple nginx deployment. $ kubectl create deployment nginx --image=gcr.io/google-containers/nginx --replicas=5 deployment.apps/nginx created $ kubectl expose deployment nginx --target-port=80 --port=8100 service/nginx exposed $ kubectl run hello-k0s --image=quay.io/prometheus/busybox --rm -it --restart=Never --command -- wget -qO- nginx:8100 <!DOCTYPE html> <html> <head> <title>Welcome to nginx on Debian!</title> ... pod \"hello-k0s\" deleted","title":"Use the cluster with kubectl"},{"location":"examples/traefik-ingress/","text":"Installing the Traefik Ingress Controller on k0s # In this tutorial, you'll learn how to configure k0s with the Traefik ingress controller , a MetalLB service loadbalancer , and deploy the Traefik Dashboard along with a service example. Utilizing the extensible bootstrapping functionality with Helm, it's as simple as adding the right extensions to the k0s.yaml file when configuring your cluster. Configuring k0s.yaml # Modify your k0s.yaml file to include the Traefik and MetalLB helm charts as extensions, and these will install during the cluster's bootstrap. Note: You may want to have a small range of IP addresses that are addressable on your network, preferably outside the assignment pool allocated by your DHCP server. Providing an addressable range should allow you to access your LoadBalancer and Ingress services from anywhere on your local network. However, any valid IP range should work locally on your machine. extensions : helm : repositories : - name : traefik url : https://helm.traefik.io/traefik - name : bitnami url : https://charts.bitnami.com/bitnami charts : - name : traefik chartname : traefik/traefik version : \"9.11.0\" namespace : default - name : metallb chartname : bitnami/metallb version : \"1.0.1\" namespace : default values : |2 configInline: address-pools: - name: generic-cluster-pool protocol: layer2 addresses: - 192.168.0.5-192.168.0.10 Providing a range of IPs for MetalLB that are addressable on your LAN is suggested if you want to access LoadBalancer and Ingress services from anywhere on your local network. Retrieving the Load Balancer IP # Once you've started your cluster, you should confirm the deployment of Traefik and MetalLB. Executing a kubectl get all should include a response with the metallb and traefik resources, along with a service loadbalancer that has an EXTERNAL-IP assigned to it. See the example below: root@k0s-host \u279c kubectl get all NAME READY STATUS RESTARTS AGE pod/metallb-1607085578-controller-864c9757f6-bpx6r 1 /1 Running 0 81s pod/metallb-1607085578-speaker-245c2 1 /1 Running 0 60s pod/traefik-1607085579-77bbc57699-b2f2t 1 /1 Running 0 81s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 96s service/traefik-1607085579 LoadBalancer 10 .105.119.102 192 .168.0.5 80 :32153/TCP,443:30791/TCP 84s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/metallb-1607085578-speaker 1 1 1 1 1 kubernetes.io/os = linux 87s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/metallb-1607085578-controller 1 /1 1 1 87s deployment.apps/traefik-1607085579 1 /1 1 1 84s NAME DESIRED CURRENT READY AGE replicaset.apps/metallb-1607085578-controller-864c9757f6 1 1 1 81s replicaset.apps/traefik-1607085579-77bbc57699 1 1 1 81s Take note of the EXTERNAL-IP given to the service/traefik-n LoadBalancer. In this example, 192.168.0.5 has been assigned and can be used to access services via the Ingress proxy: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/traefik-1607085579 LoadBalancer 10 .105.119.102 192 .168.0.5 80 :32153/TCP,443:30791/TCP 84s # Receiving a 404 response here is normal, as you've not configured any Ingress resources to respond yet root@k0s-host \u279c curl http://192.168.0.5 404 page not found Deploy and access the Traefik Dashboard # Now that you have an available and addressable load balancer on your cluster, you can quickly deploy the Traefik dashboard and access it from anywhere on your local network (provided that you configured MetalLB with an addressable range). Create the Traefik Dashboard IngressRoute in a YAML file: apiVersion : traefik.containo.us/v1alpha1 kind : IngressRoute metadata : name : dashboard spec : entryPoints : - web routes : - match : PathPrefix(`/dashboard`) || PathPrefix(`/api`) kind : Rule services : - name : api@internal kind : TraefikService Next, deploy the resource: root@k0s-host \u279c kubectl apply -f traefik-dashboard.yaml ingressroute.traefik.containo.us/dashboard created Once deployed, you should be able to access the dashboard using the EXTERNAL-IP that you noted above by visiting http://192.168.0.5 in your browser: Now, create a simple whoami Deployment, Service, and Ingress manifest: apiVersion : apps/v1 kind : Deployment metadata : name : whoami-deployment spec : replicas : 1 selector : matchLabels : app : whoami template : metadata : labels : app : whoami spec : containers : - name : whoami-container image : containous/whoami --- apiVersion : v1 kind : Service metadata : name : whoami-service spec : ports : - name : http targetPort : 80 port : 80 selector : app : whoami --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : whoami-ingress spec : rules : - http : paths : - path : /whoami pathType : Exact backend : service : name : whoami-service port : number : 80 Once you've created this, apply and test it: # apply the manifests root@k0s-host \u279c kubectl apply -f whoami.yaml deployment.apps/whoami-deployment created service/whoami-service created ingress.networking.k8s.io/whoami-ingress created # test the ingress and service root@k0s-host \u279c curl http://192.168.0.5/whoami Hostname: whoami-deployment-85bfbd48f-7l77c IP: 127 .0.0.1 IP: ::1 IP: 10 .244.214.198 IP: fe80::b049:f8ff:fe77:3e64 RemoteAddr: 10 .244.214.196:34858 GET /whoami HTTP/1.1 Host: 192 .168.0.5 User-Agent: curl/7.68.0 Accept: */* Accept-Encoding: gzip X-Forwarded-For: 192 .168.0.82 X-Forwarded-Host: 192 .168.0.5 X-Forwarded-Port: 80 X-Forwarded-Proto: http X-Forwarded-Server: traefik-1607085579-77bbc57699-b2f2t X-Real-Ip: 192 .168.0.82 Summary # From here, it's possible to use 3rd party tools, such as ngrok , to go further and expose your LoadBalancer to the world. Doing so then enables dynamic certificate provisioning through Let's Encrypt utilizing either cert-manager or Traefik's own built-in ACME provider . This guide should have given you a general idea of getting started with Ingress on k0s and exposing your applications and services quickly.","title":"Installing the Traefik Ingress Controller"},{"location":"examples/traefik-ingress/#installing-the-traefik-ingress-controller-on-k0s","text":"In this tutorial, you'll learn how to configure k0s with the Traefik ingress controller , a MetalLB service loadbalancer , and deploy the Traefik Dashboard along with a service example. Utilizing the extensible bootstrapping functionality with Helm, it's as simple as adding the right extensions to the k0s.yaml file when configuring your cluster.","title":"Installing the Traefik Ingress Controller on k0s"},{"location":"examples/traefik-ingress/#configuring-k0syaml","text":"Modify your k0s.yaml file to include the Traefik and MetalLB helm charts as extensions, and these will install during the cluster's bootstrap. Note: You may want to have a small range of IP addresses that are addressable on your network, preferably outside the assignment pool allocated by your DHCP server. Providing an addressable range should allow you to access your LoadBalancer and Ingress services from anywhere on your local network. However, any valid IP range should work locally on your machine. extensions : helm : repositories : - name : traefik url : https://helm.traefik.io/traefik - name : bitnami url : https://charts.bitnami.com/bitnami charts : - name : traefik chartname : traefik/traefik version : \"9.11.0\" namespace : default - name : metallb chartname : bitnami/metallb version : \"1.0.1\" namespace : default values : |2 configInline: address-pools: - name: generic-cluster-pool protocol: layer2 addresses: - 192.168.0.5-192.168.0.10 Providing a range of IPs for MetalLB that are addressable on your LAN is suggested if you want to access LoadBalancer and Ingress services from anywhere on your local network.","title":"Configuring k0s.yaml"},{"location":"examples/traefik-ingress/#retrieving-the-load-balancer-ip","text":"Once you've started your cluster, you should confirm the deployment of Traefik and MetalLB. Executing a kubectl get all should include a response with the metallb and traefik resources, along with a service loadbalancer that has an EXTERNAL-IP assigned to it. See the example below: root@k0s-host \u279c kubectl get all NAME READY STATUS RESTARTS AGE pod/metallb-1607085578-controller-864c9757f6-bpx6r 1 /1 Running 0 81s pod/metallb-1607085578-speaker-245c2 1 /1 Running 0 60s pod/traefik-1607085579-77bbc57699-b2f2t 1 /1 Running 0 81s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 96s service/traefik-1607085579 LoadBalancer 10 .105.119.102 192 .168.0.5 80 :32153/TCP,443:30791/TCP 84s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/metallb-1607085578-speaker 1 1 1 1 1 kubernetes.io/os = linux 87s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/metallb-1607085578-controller 1 /1 1 1 87s deployment.apps/traefik-1607085579 1 /1 1 1 84s NAME DESIRED CURRENT READY AGE replicaset.apps/metallb-1607085578-controller-864c9757f6 1 1 1 81s replicaset.apps/traefik-1607085579-77bbc57699 1 1 1 81s Take note of the EXTERNAL-IP given to the service/traefik-n LoadBalancer. In this example, 192.168.0.5 has been assigned and can be used to access services via the Ingress proxy: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/traefik-1607085579 LoadBalancer 10 .105.119.102 192 .168.0.5 80 :32153/TCP,443:30791/TCP 84s # Receiving a 404 response here is normal, as you've not configured any Ingress resources to respond yet root@k0s-host \u279c curl http://192.168.0.5 404 page not found","title":"Retrieving the Load Balancer IP"},{"location":"examples/traefik-ingress/#deploy-and-access-the-traefik-dashboard","text":"Now that you have an available and addressable load balancer on your cluster, you can quickly deploy the Traefik dashboard and access it from anywhere on your local network (provided that you configured MetalLB with an addressable range). Create the Traefik Dashboard IngressRoute in a YAML file: apiVersion : traefik.containo.us/v1alpha1 kind : IngressRoute metadata : name : dashboard spec : entryPoints : - web routes : - match : PathPrefix(`/dashboard`) || PathPrefix(`/api`) kind : Rule services : - name : api@internal kind : TraefikService Next, deploy the resource: root@k0s-host \u279c kubectl apply -f traefik-dashboard.yaml ingressroute.traefik.containo.us/dashboard created Once deployed, you should be able to access the dashboard using the EXTERNAL-IP that you noted above by visiting http://192.168.0.5 in your browser: Now, create a simple whoami Deployment, Service, and Ingress manifest: apiVersion : apps/v1 kind : Deployment metadata : name : whoami-deployment spec : replicas : 1 selector : matchLabels : app : whoami template : metadata : labels : app : whoami spec : containers : - name : whoami-container image : containous/whoami --- apiVersion : v1 kind : Service metadata : name : whoami-service spec : ports : - name : http targetPort : 80 port : 80 selector : app : whoami --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : whoami-ingress spec : rules : - http : paths : - path : /whoami pathType : Exact backend : service : name : whoami-service port : number : 80 Once you've created this, apply and test it: # apply the manifests root@k0s-host \u279c kubectl apply -f whoami.yaml deployment.apps/whoami-deployment created service/whoami-service created ingress.networking.k8s.io/whoami-ingress created # test the ingress and service root@k0s-host \u279c curl http://192.168.0.5/whoami Hostname: whoami-deployment-85bfbd48f-7l77c IP: 127 .0.0.1 IP: ::1 IP: 10 .244.214.198 IP: fe80::b049:f8ff:fe77:3e64 RemoteAddr: 10 .244.214.196:34858 GET /whoami HTTP/1.1 Host: 192 .168.0.5 User-Agent: curl/7.68.0 Accept: */* Accept-Encoding: gzip X-Forwarded-For: 192 .168.0.82 X-Forwarded-Host: 192 .168.0.5 X-Forwarded-Port: 80 X-Forwarded-Proto: http X-Forwarded-Server: traefik-1607085579-77bbc57699-b2f2t X-Real-Ip: 192 .168.0.82","title":"Deploy and access the Traefik Dashboard"},{"location":"examples/traefik-ingress/#summary","text":"From here, it's possible to use 3rd party tools, such as ngrok , to go further and expose your LoadBalancer to the world. Doing so then enables dynamic certificate provisioning through Let's Encrypt utilizing either cert-manager or Traefik's own built-in ACME provider . This guide should have given you a general idea of getting started with Ingress on k0s and exposing your applications and services quickly.","title":"Summary"},{"location":"internal/host-dependencies/","text":"Host Dependencies # The goal of k0s is to only depend on kernel functionality. However, the kubelet currently has several host dependencies. Some of these may need to be bundled, but we should prefer upstream contributions to remove these dependencies. List of hard dependencies # find -- PR by @ncopa to resolve this: https://github.com/kubernetes/kubernetes/pull/95189 du -- PR by @ncopa to resolve this: https://github.com/kubernetes/kubernetes/pull/95178 -- note that du dependency remains, but using POSIX-compliant argument nice iptables -- as documented in https://github.com/k0sproject/k0s/issues/176 it is unclear whether iptables is needed. It appears to come from the portmap plugin, but the most robust solution may be to simply bundle iptables with k0s.","title":"Host Dependencies"},{"location":"internal/host-dependencies/#host-dependencies","text":"The goal of k0s is to only depend on kernel functionality. However, the kubelet currently has several host dependencies. Some of these may need to be bundled, but we should prefer upstream contributions to remove these dependencies.","title":"Host Dependencies"},{"location":"internal/host-dependencies/#list-of-hard-dependencies","text":"find -- PR by @ncopa to resolve this: https://github.com/kubernetes/kubernetes/pull/95189 du -- PR by @ncopa to resolve this: https://github.com/kubernetes/kubernetes/pull/95178 -- note that du dependency remains, but using POSIX-compliant argument nice iptables -- as documented in https://github.com/k0sproject/k0s/issues/176 it is unclear whether iptables is needed. It appears to come from the portmap plugin, but the most robust solution may be to simply bundle iptables with k0s.","title":"List of hard dependencies"},{"location":"internal/publishing_docs_using_mkdocs/","text":"Publishing Docs # We use mkdocs and mike for publishing docs to docs.k0sproject.io . This guide will provide a simple how-to on how to configure and deploy newly added docs to our website. Requirements # Install mike: https://github.com/jimporter/mike#installation Adding A New link to the Navigation # All docs must live under the docs directory (I.E., changes to the main README.md are not reflected in the website). Add a new link under nav in the main mkdocs.yml file: ``` nav: Overview: README.md Creating A Cluster: Quick Start Guide: create-cluster.md Run in Docker: k0s-in-docker.md Single node set-up: k0s-single-node.md Configuration Reference: Architecture: architecture.md Networking: networking.md Configuration Options: configuration.md Configuring Containerd: containerd_config.md Using A Custom CRI: custom-cri-runtime.md Using Cloud Providers: cloud-providers.md Running k0s with Traefik: examples/traefik-ingress.md Running k0s as a service: install.md k0s CLI Help Pages: cli/k0s.md Deploying Manifests: manifests.md FAQ: FAQ.md Troubleshooting: troubleshooting.md Contributing: Overview: contributors/overview.md Workflow: contributors/github_workflow.md Testing: contributors/testing.md ``` Once your changes are pushed to main , the \"Publish Docs\" jos will start running: https://github.com/k0sproject/k0s/actions?query=workflow%3A%22Publish+docs+via+GitHub+Pages%22 You should see the deployment outcome in the gh-pages deployment page: https://github.com/k0sproject/k0s/deployments/activity_log?environment=github-pages Testing docs locally # We've got a dockerized setup for easily testing docs in local environment. Simply run docker-compose up in the docs root folder. The docs will be available on localhost:80 . Note If you have something already running locally on port 80 you need to change the mapped port on the docker-compose.yml file.","title":"Documentation"},{"location":"internal/publishing_docs_using_mkdocs/#publishing-docs","text":"We use mkdocs and mike for publishing docs to docs.k0sproject.io . This guide will provide a simple how-to on how to configure and deploy newly added docs to our website.","title":"Publishing Docs"},{"location":"internal/publishing_docs_using_mkdocs/#requirements","text":"Install mike: https://github.com/jimporter/mike#installation","title":"Requirements"},{"location":"internal/publishing_docs_using_mkdocs/#adding-a-new-link-to-the-navigation","text":"All docs must live under the docs directory (I.E., changes to the main README.md are not reflected in the website). Add a new link under nav in the main mkdocs.yml file: ``` nav: Overview: README.md Creating A Cluster: Quick Start Guide: create-cluster.md Run in Docker: k0s-in-docker.md Single node set-up: k0s-single-node.md Configuration Reference: Architecture: architecture.md Networking: networking.md Configuration Options: configuration.md Configuring Containerd: containerd_config.md Using A Custom CRI: custom-cri-runtime.md Using Cloud Providers: cloud-providers.md Running k0s with Traefik: examples/traefik-ingress.md Running k0s as a service: install.md k0s CLI Help Pages: cli/k0s.md Deploying Manifests: manifests.md FAQ: FAQ.md Troubleshooting: troubleshooting.md Contributing: Overview: contributors/overview.md Workflow: contributors/github_workflow.md Testing: contributors/testing.md ``` Once your changes are pushed to main , the \"Publish Docs\" jos will start running: https://github.com/k0sproject/k0s/actions?query=workflow%3A%22Publish+docs+via+GitHub+Pages%22 You should see the deployment outcome in the gh-pages deployment page: https://github.com/k0sproject/k0s/deployments/activity_log?environment=github-pages","title":"Adding A New link to the Navigation"},{"location":"internal/publishing_docs_using_mkdocs/#testing-docs-locally","text":"We've got a dockerized setup for easily testing docs in local environment. Simply run docker-compose up in the docs root folder. The docs will be available on localhost:80 . Note If you have something already running locally on port 80 you need to change the mapped port on the docker-compose.yml file.","title":"Testing docs locally"},{"location":"internal/upgrading-calico/","text":"Upgrading Calico # k0s bundles Kubernetes manifests for Calico. The manifests are retrieved from the official Calico docs . As fetching and modifying the entire multi-thousand line file is error-prone, you may follow these steps to upgrade Calico to the latest version: run ./get-calico.sh check the git diff to see if it looks sensible re-apply our manual adjustments (documented below) run make bindata-manifests compile, pray, and test commit and create a PR Manual Adjustments # Note: All manual adjustments should be fairly obvious from the git diff. This section attempts to provide a sanity checklist to go through and make sure we still have those changes applied. The code blocks in this section are our modifications , not the calico originals. static/manifests/calico/DaemonSet/calico-node.yaml : variable-based support for both vxlan and ipip (search for ipip to find): {{- if eq .Mode \"ipip\" }} # Enable IPIP - name: CALICO_IPV4POOL_IPIP value: {{ .Overlay }} # Enable or Disable VXLAN on the default IP pool. - name: CALICO_IPV4POOL_VXLAN value: \"Never\" {{- else if eq .Mode \"vxlan\" }} # Disable IPIP - name: CALICO_IPV4POOL_IPIP value: \"Never\" # Enable VXLAN on the default IP pool. - name: CALICO_IPV4POOL_VXLAN value: {{ .Overlay }} - name: FELIX_VXLANPORT value: \"{{ .VxlanPort }}\" - name: FELIX_VXLANVNI value: \"{{ .VxlanVNI }}\" {{- end }} iptables auto detect: # Auto detect the iptables backend - name: FELIX_IPTABLESBACKEND value: \"auto\" variable-based WireGuard support: {{- if .EnableWireguard }} - name: FELIX_WIREGUARDENABLED value: \"true\" {{- end }} variable-based cluster CIDR: - name: CALICO_IPV4POOL_CIDR value: \"{{ .ClusterCIDR }}\" custom backend and MTU # calico-config.yaml calico_backend: \"{{ .Mode }}\" veth_mtu: \"{{ .MTU }}\" remove bgp from CLUSTER_TYPE - name: CLUSTER_TYPE value: \"k8s\" disable BIRD checks on liveness and readiness as we don't support BGP by removing -bird-ready and -bird-live from the readiness and liveness probes respectively Container image names # Instead of hardcoded image names and versions use placeholders to support configuration level settings. Following placeholders are used: CalicoCNIImage for calico/cni CalicoFlexVolumeImage for calico/pod2daemon-flexvol CalicoNodeImage for calico/node CalicoKubeControllersImage for calico/kube-controllers Example: # calico-node.yaml image: {{ .CalicoCNIImage }}","title":"Upgrading Calico"},{"location":"internal/upgrading-calico/#upgrading-calico","text":"k0s bundles Kubernetes manifests for Calico. The manifests are retrieved from the official Calico docs . As fetching and modifying the entire multi-thousand line file is error-prone, you may follow these steps to upgrade Calico to the latest version: run ./get-calico.sh check the git diff to see if it looks sensible re-apply our manual adjustments (documented below) run make bindata-manifests compile, pray, and test commit and create a PR","title":"Upgrading Calico"},{"location":"internal/upgrading-calico/#manual-adjustments","text":"Note: All manual adjustments should be fairly obvious from the git diff. This section attempts to provide a sanity checklist to go through and make sure we still have those changes applied. The code blocks in this section are our modifications , not the calico originals. static/manifests/calico/DaemonSet/calico-node.yaml : variable-based support for both vxlan and ipip (search for ipip to find): {{- if eq .Mode \"ipip\" }} # Enable IPIP - name: CALICO_IPV4POOL_IPIP value: {{ .Overlay }} # Enable or Disable VXLAN on the default IP pool. - name: CALICO_IPV4POOL_VXLAN value: \"Never\" {{- else if eq .Mode \"vxlan\" }} # Disable IPIP - name: CALICO_IPV4POOL_IPIP value: \"Never\" # Enable VXLAN on the default IP pool. - name: CALICO_IPV4POOL_VXLAN value: {{ .Overlay }} - name: FELIX_VXLANPORT value: \"{{ .VxlanPort }}\" - name: FELIX_VXLANVNI value: \"{{ .VxlanVNI }}\" {{- end }} iptables auto detect: # Auto detect the iptables backend - name: FELIX_IPTABLESBACKEND value: \"auto\" variable-based WireGuard support: {{- if .EnableWireguard }} - name: FELIX_WIREGUARDENABLED value: \"true\" {{- end }} variable-based cluster CIDR: - name: CALICO_IPV4POOL_CIDR value: \"{{ .ClusterCIDR }}\" custom backend and MTU # calico-config.yaml calico_backend: \"{{ .Mode }}\" veth_mtu: \"{{ .MTU }}\" remove bgp from CLUSTER_TYPE - name: CLUSTER_TYPE value: \"k8s\" disable BIRD checks on liveness and readiness as we don't support BGP by removing -bird-ready and -bird-live from the readiness and liveness probes respectively","title":"Manual Adjustments"},{"location":"internal/upgrading-calico/#container-image-names","text":"Instead of hardcoded image names and versions use placeholders to support configuration level settings. Following placeholders are used: CalicoCNIImage for calico/cni CalicoFlexVolumeImage for calico/pod2daemon-flexvol CalicoNodeImage for calico/node CalicoKubeControllersImage for calico/kube-controllers Example: # calico-node.yaml image: {{ .CalicoCNIImage }}","title":"Container image names"}]}